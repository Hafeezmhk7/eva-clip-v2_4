"""
FIXED Distributed Communication Utilities for BLIP3-o
src/modules/distributed/communication.py

Provides communication and synchronization utilities for distributed training.

FIXES:
- Improved error handling
- Better timeout management
- Robust communication utilities
- Fixed import/export compatibility
"""

import torch
import torch.distributed as dist
import logging
from typing import Dict, Any, Optional, List
import json
import os
import time

logger = logging.getLogger(__name__)


class DistributedCommunicator:
    """
    Handles distributed communication and synchronization for BLIP3-o training
    """
    
    def __init__(self, rank: int, world_size: int):
        self.rank = rank
        self.world_size = world_size
        self.is_distributed = dist.is_initialized()
        
    def barrier(self, timeout_seconds: Optional[int] = None):
        """Synchronize all processes"""
        if self.is_distributed:
            try:
                if timeout_seconds:
                    # Use timeout for barrier to prevent infinite hanging
                    import signal
                    def timeout_handler(signum, frame):
                        raise TimeoutError(f"Barrier timeout after {timeout_seconds}s")
                    
                    old_handler = signal.signal(signal.SIGALRM, timeout_handler)
                    signal.alarm(timeout_seconds)
                    try:
                        dist.barrier()
                    finally:
                        signal.alarm(0)
                        signal.signal(signal.SIGALRM, old_handler)
                else:
                    dist.barrier()
            except Exception as e:
                logger.warning(f"Barrier failed on rank {self.rank}: {e}")
    
    def is_master(self) -> bool:
        """Check if current process is master (rank 0)"""
        return self.rank == 0
    
    def broadcast_object(self, obj: Any, src_rank: int = 0) -> Any:
        """Broadcast an object from source rank to all ranks"""
        if not self.is_distributed:
            return obj
        
        try:
            object_list = [obj] if self.rank == src_rank else [None]
            dist.broadcast_object_list(object_list, src=src_rank)
            return object_list[0]
        except Exception as e:
            logger.warning(f"Broadcast failed on rank {self.rank}: {e}")
            return obj
    
    def all_gather_object(self, obj: Any) -> List[Any]:
        """Gather objects from all ranks"""
        if not self.is_distributed:
            return [obj]
        
        try:
            gathered_objects = [None] * self.world_size
            dist.all_gather_object(gathered_objects, obj)
            return gathered_objects
        except Exception as e:
            logger.warning(f"All-gather failed on rank {self.rank}: {e}")
            return [obj]
    
    def reduce_tensor(self, tensor: torch.Tensor, op=dist.ReduceOp.SUM, dst: int = 0) -> torch.Tensor:
        """Reduce tensor across all ranks"""
        if not self.is_distributed:
            return tensor
        
        try:
            dist.reduce(tensor, dst=dst, op=op)
            return tensor
        except Exception as e:
            logger.warning(f"Reduce failed on rank {self.rank}: {e}")
            return tensor
    
    def all_reduce_tensor(self, tensor: torch.Tensor, op=dist.ReduceOp.SUM) -> torch.Tensor:
        """All-reduce tensor across all ranks"""
        if not self.is_distributed:
            return tensor
        
        try:
            dist.all_reduce(tensor, op=op)
            return tensor
        except Exception as e:
            logger.warning(f"All-reduce failed on rank {self.rank}: {e}")
            return tensor


class MetricsAggregator:
    """
    Aggregates metrics across distributed training ranks
    """
    
    def __init__(self, rank: int, world_size: int):
        self.rank = rank
        self.world_size = world_size
        self.communicator = DistributedCommunicator(rank, world_size)
    
    def aggregate_metrics(self, local_metrics: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        Aggregate metrics from all ranks
        
        Args:
            local_metrics: Metrics from current rank
            
        Returns:
            Aggregated metrics (only on rank 0, None on other ranks)
        """
        if not self.communicator.is_distributed:
            return local_metrics
        
        try:
            # Gather metrics from all ranks
            all_metrics = self.communicator.all_gather_object(local_metrics)
            
            if self.rank == 0:
                # Aggregate metrics on master rank
                aggregated = {}
                
                # Collect all metric keys
                all_keys = set()
                for metrics in all_metrics:
                    if isinstance(metrics, dict):
                        all_keys.update(metrics.keys())
                
                # Aggregate each metric
                for key in all_keys:
                    values = []
                    total_weight = 0
                    
                    for rank_idx, metrics in enumerate(all_metrics):
                        if isinstance(metrics, dict) and key in metrics:
                            value = metrics[key]
                            
                            # Handle different value types
                            if isinstance(value, (int, float)) and not isinstance(value, bool):
                                # Weight by rank contribution (simple equal weighting)
                                weight = 1.0
                                values.append((value, weight))
                                total_weight += weight
                            elif isinstance(value, str):
                                # For string values, just take the first valid one
                                if key not in aggregated:
                                    aggregated[key] = value
                            elif isinstance(value, bool):
                                # For boolean values, take logical OR
                                aggregated[key] = aggregated.get(key, False) or value
                    
                    # Compute weighted average for numeric values
                    if values and total_weight > 0:
                        weighted_sum = sum(v * w for v, w in values)
                        aggregated[key] = weighted_sum / total_weight
                
                # Add aggregation metadata
                aggregated['_aggregation_info'] = {
                    'num_ranks': len(all_metrics),
                    'aggregated_by_rank': 0,
                    'aggregation_method': 'weighted_average'
                }
                
                return aggregated
            else:
                # Non-master ranks return None
                return None
                
        except Exception as e:
            logger.warning(f"Metrics aggregation failed on rank {self.rank}: {e}")
            if self.rank == 0:
                return local_metrics
            else:
                return None
    
    def log_aggregated_metrics(self, metrics: Dict[str, Any], prefix: str = ""):
        """Log aggregated metrics (only on rank 0)"""
        if self.rank != 0 or not metrics:
            return
        
        logger.info(f"ðŸ“Š {prefix}Aggregated metrics across {self.world_size} ranks:")
        for key, value in metrics.items():
            if not key.startswith('_') and isinstance(value, (int, float)):
                logger.info(f"   {key}: {value:.6f}")


def log_distributed_info(rank: int, world_size: int, message: str, level: str = "info"):
    """
    Log distributed training information
    
    Args:
        rank: Current process rank
        world_size: Total number of processes
        message: Message to log
        level: Log level ('info', 'warning', 'error')
    """
    
    if rank == 0:  # Only master rank logs by default
        log_msg = f"[Distributed {world_size}GPUs] {message}"
        
        if level == "info":
            logger.info(log_msg)
        elif level == "warning":
            logger.warning(log_msg)
        elif level == "error":
            logger.error(log_msg)
        else:
            logger.info(log_msg)


def sync_random_seed(seed: int, rank: int, world_size: int) -> int:
    """
    Synchronize random seed across all ranks
    
    Args:
        seed: Base seed value
        rank: Current process rank  
        world_size: Total number of processes
        
    Returns:
        Synchronized seed for this rank
    """
    
    # Create rank-specific seed while maintaining reproducibility
    rank_seed = seed + rank * 10000
    
    if rank == 0:
        logger.info(f"ðŸŽ² Synchronized random seeds across {world_size} ranks (base seed: {seed})")
    
    return rank_seed


def wait_for_everyone(rank: int, world_size: int, message: str = "Synchronizing", timeout_seconds: int = 60):
    """
    Wait for all ranks to reach this point with timeout protection
    
    Args:
        rank: Current process rank
        world_size: Total number of processes
        message: Description of what we're waiting for
        timeout_seconds: Maximum time to wait before timeout
    """
    
    if rank == 0:
        logger.info(f"â³ {message} - waiting for all {world_size} ranks...")
    
    if dist.is_initialized():
        try:
            # Use timeout to prevent infinite hanging
            start_time = time.time()
            dist.barrier()
            wait_time = time.time() - start_time
            
            if rank == 0:
                logger.info(f"âœ… All {world_size} ranks synchronized in {wait_time:.2f}s")
                
        except Exception as e:
            logger.warning(f"Synchronization failed on rank {rank}: {e}")


def check_distributed_setup(rank: int, world_size: int) -> Dict[str, Any]:
    """
    Check distributed training setup
    
    Args:
        rank: Current process rank
        world_size: Total number of processes
        
    Returns:
        Dictionary with setup information
    """
    
    setup_info = {
        'rank': rank,
        'world_size': world_size,
        'is_distributed': dist.is_initialized(),
        'backend': None,
        'master_addr': None,
        'master_port': None,
        'cuda_available': torch.cuda.is_available(),
        'cuda_device_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,
    }
    
    if setup_info['is_distributed']:
        try:
            setup_info['backend'] = dist.get_backend()
            setup_info['master_addr'] = os.environ.get('MASTER_ADDR', 'unknown')
            setup_info['master_port'] = os.environ.get('MASTER_PORT', 'unknown')
        except Exception as e:
            logger.warning(f"Could not get distributed setup info: {e}")
    
    if rank == 0:
        logger.info("ðŸ”§ Distributed setup check:")
        logger.info(f"   Rank: {rank}/{world_size}")
        logger.info(f"   Distributed: {setup_info['is_distributed']}")
        logger.info(f"   CUDA Available: {setup_info['cuda_available']}")
        logger.info(f"   CUDA Devices: {setup_info['cuda_device_count']}")
        if setup_info['is_distributed']:
            logger.info(f"   Backend: {setup_info['backend']}")
            logger.info(f"   Master: {setup_info['master_addr']}:{setup_info['master_port']}")
    
    return setup_info


def test_distributed_communication(rank: int, world_size: int) -> bool:
    """
    Test distributed communication to ensure it's working
    
    Args:
        rank: Current process rank
        world_size: Total number of processes
        
    Returns:
        True if communication test passes, False otherwise
    """
    
    if not dist.is_initialized():
        if rank == 0:
            logger.info("âœ… Communication test: Single process (no distributed setup needed)")
        return True
    
    try:
        communicator = DistributedCommunicator(rank, world_size)
        
        # Test 1: Barrier
        start_time = time.time()
        communicator.barrier(timeout_seconds=10)
        barrier_time = time.time() - start_time
        
        # Test 2: All-reduce
        test_tensor = torch.tensor([rank], dtype=torch.float32)
        if torch.cuda.is_available():
            test_tensor = test_tensor.cuda()
        
        original_value = test_tensor.item()
        result_tensor = communicator.all_reduce_tensor(test_tensor.clone())
        expected_sum = sum(range(world_size))
        actual_sum = result_tensor.item()
        
        # Test 3: Broadcast
        test_data = {"rank": rank, "message": f"Hello from rank {rank}"}
        broadcast_data = communicator.broadcast_object(test_data, src_rank=0)
        
        communication_success = (
            abs(actual_sum - expected_sum) < 1e-6 and
            broadcast_data["rank"] == 0 and
            barrier_time < 5.0
        )
        
        if rank == 0:
            if communication_success:
                logger.info(f"âœ… Communication test PASSED:")
                logger.info(f"   Barrier time: {barrier_time:.3f}s")
                logger.info(f"   All-reduce: {actual_sum} (expected: {expected_sum})")
                logger.info(f"   Broadcast: {broadcast_data['message']}")
            else:
                logger.error(f"âŒ Communication test FAILED:")
                logger.error(f"   Barrier time: {barrier_time:.3f}s")
                logger.error(f"   All-reduce: {actual_sum} (expected: {expected_sum})")
                logger.error(f"   Broadcast data: {broadcast_data}")
        
        return communication_success
        
    except Exception as e:
        if rank == 0:
            logger.error(f"âŒ Communication test failed with exception: {e}")
        return False


def cleanup_distributed_state():
    """Clean up distributed state and resources"""
    try:
        if dist.is_initialized():
            # Final barrier before cleanup
            dist.barrier()
            
            # Destroy process group
            dist.destroy_process_group()
            
            logger.info("ðŸ§¹ Distributed state cleaned up")
    except Exception as e:
        logger.warning(f"Warning during distributed cleanup: {e}")
    
    # Clear CUDA cache
    if torch.cuda.is_available():
        torch.cuda.empty_cache()


def get_distributed_rank_info() -> Dict[str, int]:
    """Get current distributed rank information"""
    if dist.is_initialized():
        return {
            'rank': dist.get_rank(),
            'world_size': dist.get_world_size(),
            'local_rank': int(os.environ.get('LOCAL_RANK', 0)),
        }
    else:
        return {
            'rank': 0,
            'world_size': 1,
            'local_rank': 0,
        }


def is_distributed_available() -> bool:
    """Check if distributed training is available and properly set up"""
    return (
        torch.cuda.is_available() and
        torch.cuda.device_count() > 1 and
        hasattr(torch, 'distributed') and
        torch.distributed.is_nccl_available()
    )


# Export all functions for easy import
__all__ = [
    'DistributedCommunicator',
    'MetricsAggregator', 
    'log_distributed_info',
    'sync_random_seed',
    'wait_for_everyone',
    'check_distributed_setup',
    'test_distributed_communication',
    'cleanup_distributed_state',
    'get_distributed_rank_info',
    'is_distributed_available'
]