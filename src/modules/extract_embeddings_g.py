#!/usr/bin/env python3
"""
MEMORY-OPTIMIZED BLIP3-o Embedding Extraction with Multi-GPU Support
src/modules/extract_embeddings_g.py

MEMORY OPTIMIZATION FIXES:
1. Enhanced memory monitoring and cleanup
2. Adaptive batch processing based on available memory
3. Better model loading with memory efficiency
4. OOM detection and recovery mechanisms
5. Progressive memory management throughout processing
"""

import sys
import os
import torch
import torch.nn.functional as F
from transformers import CLIPProcessor, CLIPModel, AutoModel, CLIPImageProcessor
import pickle
from tqdm import tqdm
import numpy as np
from pathlib import Path
import gc
import psutil
import time
import glob
import json
import shutil
import argparse

def setup_paths():
    """Setup paths for project structure"""
    script_dir = Path(__file__).parent
    project_root = script_dir.parent.parent
    
    # Add import paths
    sys.path.insert(0, str(project_root))
    sys.path.insert(0, str(project_root / "src"))
    sys.path.insert(0, str(project_root / "src" / "data_hand"))
    sys.path.insert(0, str(project_root / "src" / "utils"))
    
    return project_root

def setup_temp_manager():
    """Setup temp manager for structured directory management."""
    try:
        from src.modules.utils.temp_manager import setup_snellius_environment
        manager = setup_snellius_environment("blip3o_workspace")
        return manager
    except ImportError:
        print("‚ö†Ô∏è  Temp manager not available, using fallback directories")
        return None

def get_memory_usage():
    """Get current memory usage in GB"""
    try:
        process = psutil.Process(os.getpid())
        memory_mb = process.memory_info().rss / 1024 / 1024
        return memory_mb / 1024
    except:
        return 0.0

def get_gpu_memory_info(device_id: int = None) -> dict:
    """Get GPU memory information"""
    try:
        if device_id is not None:
            torch.cuda.set_device(device_id)
        else:
            device_id = torch.cuda.current_device()
            
        total_memory = torch.cuda.get_device_properties(device_id).total_memory / 1e9
        allocated = torch.cuda.memory_allocated(device_id) / 1e9
        cached = torch.cuda.memory_reserved(device_id) / 1e9
        free = total_memory - cached
        
        return {
            'device_id': device_id,
            'total_gb': total_memory,
            'allocated_gb': allocated,
            'cached_gb': cached,
            'free_gb': free,
            'utilization_pct': (cached / total_memory) * 100
        }
        
    except Exception as e:
        return {'error': str(e), 'device_id': device_id}

def cleanup_memory():
    """Enhanced memory cleanup with monitoring"""
    initial_memory = get_memory_usage()
    initial_gpu_memory = get_gpu_memory_info() if torch.cuda.is_available() else {}
    
    # Python garbage collection
    collected = gc.collect()
    
    # PyTorch CUDA cleanup
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    
    # Force another garbage collection
    gc.collect()
    
    final_memory = get_memory_usage()
    final_gpu_memory = get_gpu_memory_info() if torch.cuda.is_available() else {}
    
    # Calculate cleanup effectiveness
    system_freed = initial_memory - final_memory
    gpu_freed = (initial_gpu_memory.get('cached_gb', 0) - 
                final_gpu_memory.get('cached_gb', 0)) if torch.cuda.is_available() else 0
    
    return {
        'objects_collected': collected,
        'system_memory_freed_gb': system_freed,
        'gpu_memory_freed_gb': gpu_freed,
        'final_gpu_free_gb': final_gpu_memory.get('free_gb', 0)
    }

def adaptive_batch_size_selection(device, initial_batch_size: int = 16, min_free_memory_gb: float = 5.0) -> int:
    """Adaptively select batch size based on available GPU memory"""
    try:
        if not torch.cuda.is_available():
            return min(initial_batch_size, 8)  # Conservative for CPU
        
        memory_info = get_gpu_memory_info(device.index if hasattr(device, 'index') else None)
        free_memory_gb = memory_info.get('free_gb', 0)
        
        print(f"GPU memory: {free_memory_gb:.1f} GB free")
        
        # Adaptive batch size selection based on available memory
        if free_memory_gb > 60:  # Abundant memory (H100)
            recommended_batch_size = min(initial_batch_size, 24)
        elif free_memory_gb > 40:  # Good memory availability
            recommended_batch_size = min(initial_batch_size, 16)
        elif free_memory_gb > 20:  # Moderate memory
            recommended_batch_size = min(initial_batch_size, 12)
        elif free_memory_gb > 10:  # Limited memory
            recommended_batch_size = min(initial_batch_size, 8)
        elif free_memory_gb > min_free_memory_gb:  # Minimal memory
            recommended_batch_size = min(initial_batch_size, 4)
        else:  # Critical memory situation
            recommended_batch_size = 2
            print(f"‚ö†Ô∏è Critical memory situation: {free_memory_gb:.1f} GB free, using batch_size=2")
        
        if recommended_batch_size != initial_batch_size:
            print(f"üìä Adjusted batch size from {initial_batch_size} to {recommended_batch_size} based on memory")
        
        return recommended_batch_size
        
    except Exception as e:
        print(f"‚ö†Ô∏è Could not determine adaptive batch size: {e}")
        return min(initial_batch_size, 8)  # Conservative fallback

def load_models(device):
    """Load CLIP and EVA-CLIP models with memory optimization"""
    print("üì¶ Loading models with memory optimization...")
    
    # Get initial memory state
    initial_memory = get_gpu_memory_info(device.index if hasattr(device, 'index') else None)
    print(f"   Initial GPU memory: {initial_memory.get('free_gb', 0):.1f} GB free")
    
    # Load CLIP ViT-L/14 with memory optimization
    print("   Loading CLIP ViT-L/14...")
    clip_processor = CLIPProcessor.from_pretrained(
        "openai/clip-vit-large-patch14",
        cache_dir=os.environ.get('TRANSFORMERS_CACHE')
    )
    
    clip_model = CLIPModel.from_pretrained(
        "openai/clip-vit-large-patch14",
        torch_dtype=torch.float16,
        device_map=None,  # Manual device placement
        cache_dir=os.environ.get('TRANSFORMERS_CACHE')
    ).to(device)
    clip_model.eval()
    
    # Memory cleanup after CLIP loading
    cleanup_result = cleanup_memory()
    print(f"   After CLIP: {cleanup_result['final_gpu_free_gb']:.1f} GB free (freed {cleanup_result['gpu_memory_freed_gb']:.1f} GB)")
    
    # Load EVA-CLIP-8B with memory optimization
    print("   Loading EVA-CLIP-8B...")
    eva_model = AutoModel.from_pretrained(
        "BAAI/EVA-CLIP-8B", 
        trust_remote_code=True,
        torch_dtype=torch.float16,
        device_map=None,  # Manual device placement
        cache_dir=os.environ.get('TRANSFORMERS_CACHE')
    ).to(device)
    
    eva_processor = CLIPImageProcessor.from_pretrained(
        "openai/clip-vit-large-patch14",
        cache_dir=os.environ.get('TRANSFORMERS_CACHE')
    )
    eva_model.eval()
    
    # Final memory cleanup
    cleanup_result = cleanup_memory()
    final_memory = get_gpu_memory_info(device.index if hasattr(device, 'index') else None)
    
    print("‚úÖ Models loaded successfully with memory optimization")
    print(f"üíæ Final GPU memory: {final_memory.get('free_gb', 0):.1f} GB free")
    print(f"üíæ Total memory used by models: {initial_memory.get('free_gb', 0) - final_memory.get('free_gb', 0):.1f} GB")
    
    return clip_processor, clip_model, eva_processor, eva_model

def extract_clip_features_with_cls(images, processor, model, device, include_cls=True):
    """
    Extract CLIP ViT-L/14 features with CLS token + patches (memory optimized)
    """
    features = []
    
    for img in images:
        try:
            inputs = processor(images=img, return_tensors="pt")
            inputs = {k: v.to(device).half() if v.dtype == torch.float32 else v.to(device) 
                     for k, v in inputs.items()}
            
            with torch.no_grad():
                vision_outputs = model.vision_model(
                    pixel_values=inputs['pixel_values'],
                    output_hidden_states=True,
                    return_dict=True
                )
                
                # Get all hidden states (CLS + patches)
                if include_cls:
                    all_embeddings = vision_outputs.last_hidden_state  # [1, 257, 1024]
                    expected_tokens = 257
                else:
                    all_embeddings = vision_outputs.last_hidden_state[:, 1:, :]  # [1, 256, 1024]
                    expected_tokens = 256
                
                batch_size, num_tokens, hidden_dim = all_embeddings.shape
                
                # Validate dimensions
                assert hidden_dim == 1024, f"Expected CLIP 1024-dim, got {hidden_dim}"
                assert num_tokens == expected_tokens, f"Expected {expected_tokens} tokens, got {num_tokens}"
                
                # Convert to float32 and move to CPU immediately to save GPU memory
                features.append(all_embeddings.squeeze().cpu().float())
                
                # Clear GPU memory immediately
                del vision_outputs, all_embeddings
                
        except Exception as e:
            print(f"‚ö†Ô∏è Error extracting CLIP features for image: {e}")
            # Create a zero tensor as fallback
            expected_tokens = 257 if include_cls else 256
            fallback_tensor = torch.zeros(expected_tokens, 1024)
            features.append(fallback_tensor)
    
    return torch.stack(features)

def extract_eva_features_with_cls(images, processor, model, device, include_cls=True):
    """
    Extract EVA-CLIP-8B features with CLS token + patches (memory optimized)
    """
    features = []
    
    for img in images:
        try:
            inputs = processor(images=img, return_tensors="pt")
            pixel_values = inputs['pixel_values'].to(device).half()
            
            with torch.no_grad():
                vision_outputs = model.vision_model(
                    pixel_values=pixel_values,
                    output_hidden_states=True,
                    return_dict=True
                )
                
                # Get all hidden states (CLS + patches)
                if include_cls:
                    all_embeddings = vision_outputs.last_hidden_state  # [1, 257, hidden_dim]
                    expected_tokens = 257
                else:
                    all_embeddings = vision_outputs.last_hidden_state[:, 1:, :]  # [1, 256, hidden_dim]
                    expected_tokens = 256
                
                batch_size, num_tokens, hidden_dim = all_embeddings.shape
                
                # Validate dimensions
                assert num_tokens == expected_tokens, f"Expected {expected_tokens} tokens, got {num_tokens}"
                
                # Convert to float32 and move to CPU immediately to save GPU memory
                features.append(all_embeddings.squeeze().cpu().float())
                
                # Clear GPU memory immediately
                del vision_outputs, all_embeddings, pixel_values
                
        except Exception as e:
            print(f"‚ö†Ô∏è Error extracting EVA features for image: {e}")
            # Create a zero tensor as fallback (need to determine EVA hidden_dim)
            expected_tokens = 257 if include_cls else 256
            # EVA-CLIP-8B typically has 4096 dim
            fallback_tensor = torch.zeros(expected_tokens, 4096)
            features.append(fallback_tensor)
    
    return torch.stack(features)

def find_data_files(temp_manager, max_shards=None):
    """Find downloaded tar files using temp manager."""
    if temp_manager:
        datasets_dir = temp_manager.get_datasets_dir()
        print(f"üîç Searching for dataset shards in: {datasets_dir}")
    else:
        # Fallback to old method
        print("üîç Searching for dataset shards (fallback method)...")
        if "TMPDIR" in os.environ:
            datasets_dir = Path(os.environ["TMPDIR"]) / "blip3o_data"
        elif "SCRATCH_SHARED" in os.environ:
            user = os.environ.get("USER", "user")
            datasets_dir = Path(os.environ["SCRATCH_SHARED"]) / user / "blip3o_data"
        else:
            datasets_dir = Path(__file__).parent.parent.parent / "data"
    
    # Look for tar files
    tar_files = list(datasets_dir.glob("*.tar"))
    if tar_files:
        tar_files.sort()  # Sort numerically
        tar_files = [str(f) for f in tar_files]
        
        # Limit number of shards if specified
        if max_shards is not None:
            tar_files = tar_files[:max_shards]
            print(f"üìä Limited to {max_shards} shards")
        
        print(f"   ‚úÖ Found {len(tar_files)} tar files")
        
        # Validate files exist and show details
        valid_files = []
        total_size_gb = 0
        
        print(f"\nüìä Validating found files...")
        for tar_file in tar_files:
            tar_path = Path(tar_file)
            if tar_path.exists():
                size_gb = tar_path.stat().st_size / (1024**3)
                total_size_gb += size_gb
                valid_files.append(tar_file)
                print(f"   ‚úÖ {tar_path.name}: {size_gb:.2f} GB")
            else:
                print(f"   ‚ùå Missing: {tar_file}")
        
        print(f"\nüéØ Using {len(valid_files)} tar files for extraction")
        print(f"üìä Total dataset size: {total_size_gb:.2f} GB")
        
        # Estimate samples
        estimated_samples = int(total_size_gb * 400000 / 1.0)  # Rough estimate
        print(f"üìä Estimated samples: ~{estimated_samples:,}")
        
        return valid_files
    
    raise FileNotFoundError(
        f"No TAR files found in {datasets_dir}!\n"
        "Please download dataset shards first:\n"
        "  python src/data_hand/download_data.py --shards 0 1 2 3 4 5 6 7 8 9\n"
    )

def check_webdataset_version():
    """Check WebDataset version and capabilities"""
    try:
        import webdataset as wds
        version = getattr(wds, '__version__', 'unknown')
        
        # Check for pipe method
        has_pipe = hasattr(wds.WebDataset([]), 'pipe')
        
        # Check for split_by_node
        has_split_by_node = hasattr(wds, 'split_by_node')
        
        print(f"üì¶ WebDataset version: {version}")
        print(f"   Has .pipe() method: {'‚úÖ' if has_pipe else '‚ùå'}")
        print(f"   Has split_by_node: {'‚úÖ' if has_split_by_node else '‚ùå'}")
        
        return {
            'version': version,
            'has_pipe': has_pipe,
            'has_split_by_node': has_split_by_node
        }
    except ImportError:
        print("‚ùå WebDataset not available")
        return None

def create_distributed_webdataset_v2(tar_file_path: str, world_size: int = 1, rank: int = 0):
    """
    FIXED: Create WebDataset with version compatibility and memory optimization
    """
    print(f"üîß Creating WebDataset (world_size={world_size}, rank={rank})")
    
    # Check WebDataset capabilities
    wds_info = check_webdataset_version()
    if not wds_info:
        print("‚ùå WebDataset not available, using fallback")
        return None
    
    try:
        import webdataset as wds
        from PIL import Image
        import io
        
        def decode_sample(sample):
            """Decode a sample from WebDataset with memory optimization"""
            try:
                # Get image
                for ext in ['jpg', 'jpeg', 'png', 'webp']:
                    if ext in sample:
                        image_data = sample[ext]
                        break
                else:
                    return None
                
                # Load image and immediately convert to RGB to save memory
                image = Image.open(io.BytesIO(image_data)).convert('RGB')
                
                # Get caption
                caption = ""
                for caption_key in ['txt', 'caption', 'text']:
                    if caption_key in sample:
                        caption_data = sample[caption_key]
                        if isinstance(caption_data, bytes):
                            caption = caption_data.decode('utf-8').strip()
                        else:
                            caption = str(caption_data).strip()
                        break
                
                key = sample.get('__key__', 'unknown')
                
                # Clear sample data to free memory
                sample.clear()
                
                return {
                    'image': image,
                    'caption': caption,
                    'key': key,
                }
            except Exception as e:
                return None
        
        # APPROACH 1: Try modern WebDataset with .pipe() method
        if wds_info['has_pipe'] and wds_info['has_split_by_node'] and world_size > 1:
            print("   Attempting modern WebDataset with .pipe() method...")
            try:
                dataset = (
                    wds.WebDataset([tar_file_path], empty_check=False, shardshuffle=False)
                    .pipe(wds.split_by_node, group_size=None)
                    .pipe(wds.split_by_worker)
                    .map(decode_sample)
                    .select(lambda x: x is not None)
                )
                print("   ‚úÖ Modern WebDataset created successfully")
                return dataset
            except Exception as e:
                print(f"   ‚ùå Modern WebDataset failed: {e}")
        
        # APPROACH 2: Try WebDataset with manual distributed logic
        if world_size > 1:
            print("   Attempting WebDataset with manual distributed processing...")
            try:
                # Create dataset with manual rank-based filtering
                class RankFilteredDataset:
                    def __init__(self, base_dataset, rank, world_size):
                        self.base_dataset = base_dataset
                        self.rank = rank
                        self.world_size = world_size
                        self.counter = 0
                    
                    def __iter__(self):
                        self.counter = 0
                        for item in self.base_dataset:
                            if self.counter % self.world_size == self.rank:
                                yield item
                            self.counter += 1
                
                base_dataset = (
                    wds.WebDataset([tar_file_path], empty_check=False, shardshuffle=False)
                    .map(decode_sample)
                    .select(lambda x: x is not None)
                )
                
                dataset = RankFilteredDataset(base_dataset, rank, world_size)
                print("   ‚úÖ Manual distributed WebDataset created successfully")
                return dataset
            except Exception as e:
                print(f"   ‚ùå Manual distributed WebDataset failed: {e}")
        
        # APPROACH 3: Simple WebDataset (fallback)
        print("   Using simple WebDataset (single-GPU mode or fallback)...")
        try:
            dataset = (
                wds.WebDataset([tar_file_path], empty_check=False, shardshuffle=False)
                .map(decode_sample)
                .select(lambda x: x is not None)
            )
            print("   ‚úÖ Simple WebDataset created successfully")
            return dataset
        except Exception as e:
            print(f"   ‚ùå Simple WebDataset failed: {e}")
        
        print("‚ùå All WebDataset approaches failed")
        return None
        
    except ImportError as e:
        print(f"‚ùå WebDataset import failed: {e}")
        return None
    except Exception as e:
        print(f"‚ùå Unexpected error creating WebDataset: {e}")
        return None

def create_fallback_tar_processor(tar_file_path: str, world_size: int = 1, rank: int = 0):
    """
    Fallback TAR processor using Python tarfile when WebDataset fails (memory optimized)
    """
    print(f"üîÑ Creating fallback TAR processor for {Path(tar_file_path).name}")
    
    try:
        import tarfile
        from PIL import Image
        import io
        
        class FallbackTarDataset:
            def __init__(self, tar_path, rank=0, world_size=1):
                self.tar_path = tar_path
                self.rank = rank
                self.world_size = world_size
                
            def __iter__(self):
                try:
                    with tarfile.open(self.tar_path, 'r') as tar:
                        members = tar.getmembers()
                        
                        # Filter members for this rank if distributed
                        if self.world_size > 1:
                            members = [m for i, m in enumerate(members) if i % self.world_size == self.rank]
                        
                        for member in members:
                            if member.isfile():
                                try:
                                    # Extract the file
                                    file_obj = tar.extractfile(member)
                                    if file_obj is None:
                                        continue
                                    
                                    # Try to decode as image
                                    if any(ext in member.name.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                                        try:
                                            # Read and immediately convert image to save memory
                                            image_data = file_obj.read()
                                            image = Image.open(io.BytesIO(image_data)).convert('RGB')
                                            
                                            # Clear image data to free memory
                                            del image_data
                                            
                                            # Create a simple caption (or try to find associated text file)
                                            key = Path(member.name).stem
                                            caption = f"Image {key}"
                                            
                                            yield {
                                                'image': image,
                                                'caption': caption,
                                                'key': key,
                                            }
                                        except Exception as e:
                                            continue
                                    
                                except Exception as e:
                                    continue
                except Exception as e:
                    print(f"‚ùå Error processing TAR file: {e}")
                    return
        
        dataset = FallbackTarDataset(tar_file_path, rank, world_size)
        print("   ‚úÖ Fallback TAR processor created successfully")
        return dataset
        
    except Exception as e:
        print(f"‚ùå Fallback TAR processor failed: {e}")
        return None

def process_single_tar(
    tar_file_path: str,
    shard_idx: int,
    clip_processor, clip_model, eva_processor, eva_model,
    device: torch.device,
    output_dir: Path,
    working_dir: Path,
    batch_size: int = 16,
    include_cls: bool = True,
    target_tokens: int = 257,
    world_size: int = 1,
    rank: int = 0,
    max_retries: int = 3
) -> dict:
    """
    MEMORY-OPTIMIZED: Process a single TAR file with enhanced memory management
    """
    
    print(f"\nüîÑ Processing shard {shard_idx}: {Path(tar_file_path).name}")
    print(f"   Mode: {'CLS+Patches' if include_cls else 'Patches only'} ({target_tokens} tokens)")
    if world_size > 1:
        print(f"   Distributed: rank {rank}/{world_size}")
    
    # Get initial memory state
    initial_memory = get_gpu_memory_info(device.index if hasattr(device, 'index') else None)
    print(f"   Initial GPU memory: {initial_memory.get('free_gb', 0):.1f} GB free")
    
    # Adaptive batch size selection
    adaptive_batch_size = adaptive_batch_size_selection(device, batch_size)
    if adaptive_batch_size != batch_size:
        print(f"   Adjusted batch size from {batch_size} to {adaptive_batch_size}")
        batch_size = adaptive_batch_size
    
    # Expected output file path
    mode_suffix = "cls_patch" if include_cls else "patch_only"
    shard_filename = f"embeddings_shard_{shard_idx:05d}_{mode_suffix}.pkl"
    shard_path = output_dir / shard_filename
    
    # Check if this shard already exists
    if shard_path.exists():
        print(f"   ‚úÖ Shard {shard_idx} already exists: {shard_path}")
        file_size_mb = shard_path.stat().st_size / (1024 * 1024)
        
        try:
            with open(shard_path, 'rb') as f:
                existing_data = pickle.load(f)
            sample_count = len(existing_data.get('captions', []))
            
            return {
                'shard_idx': shard_idx,
                'total_samples': sample_count,
                'file_size_mb': file_size_mb,
                'processing_time': 0.0,
                'output_path': str(shard_path),
                'success': True,
                'skipped': True,
                'mode': mode_suffix,
                'tokens': target_tokens
            }
        except:
            print(f"   ‚ö†Ô∏è  Could not read existing file, will reprocess...")
            shard_path.unlink()
    
    # Try processing with retries and memory optimization
    for attempt in range(max_retries):
        try:
            print(f"   üîÑ Processing attempt {attempt + 1}/{max_retries} (batch_size={batch_size})")
            
            # Pre-processing memory cleanup
            cleanup_result = cleanup_memory()
            print(f"   Memory cleanup: freed {cleanup_result['gpu_memory_freed_gb']:.1f} GB GPU memory")
            
            # APPROACH 1: Try WebDataset (fixed version)
            dataset = create_distributed_webdataset_v2(tar_file_path, world_size, rank)
            
            # APPROACH 2: Fallback to direct TAR processing
            if dataset is None:
                print("   üîÑ WebDataset failed, trying fallback TAR processor...")
                dataset = create_fallback_tar_processor(tar_file_path, world_size, rank)
            
            if dataset is None:
                print(f"   ‚ùå All dataset creation methods failed")
                if attempt == max_retries - 1:
                    return {
                        'shard_idx': shard_idx,
                        'total_samples': 0,
                        'success': False,
                        'error': 'Failed to create any dataset processor'
                    }
                continue
            
            # Create dataloader with memory-optimized settings
            def simple_collate(batch):
                valid_batch = [item for item in batch if item is not None]
                if not valid_batch:
                    return None
                    
                images = [item['image'] for item in valid_batch]
                captions = [item['caption'] for item in valid_batch]
                keys = [item['key'] for item in valid_batch]
                return {
                    'image': images,
                    'caption': captions,
                    'key': keys
                }
            
            from torch.utils.data import DataLoader
            dataloader = DataLoader(
                dataset, 
                batch_size=batch_size, 
                collate_fn=simple_collate,
                num_workers=0,  # Use 0 to avoid multiprocessing overhead
                drop_last=False,
                pin_memory=False  # Disable pin_memory to save memory
            )
            
            print(f"   ‚úÖ Dataloader created successfully with batch_size={batch_size}")
            
            # Storage for this shard's embeddings
            shard_clip_embeddings = []
            shard_eva_embeddings = []
            shard_captions = []
            shard_keys = []
            
            total_samples = 0
            start_time = time.time()
            batch_count = 0
            error_count = 0
            oom_count = 0
            
            print(f"   üìä Processing batches with memory monitoring...")
            
            # Process all batches in this TAR file
            try:
                for batch_idx, batch in enumerate(tqdm(dataloader, desc=f"Shard {shard_idx}", unit="batch")):
                    if batch is None:
                        continue
                        
                    batch_count += 1
                    
                    # Memory check before processing batch
                    pre_batch_memory = get_gpu_memory_info(device.index if hasattr(device, 'index') else None)
                    
                    try:
                        images = batch['image']
                        captions = batch['caption']
                        keys = batch['key']
                        
                        if not images:  # Skip empty batches
                            continue
                        
                        # Extract features with memory monitoring
                        try:
                            clip_features = extract_clip_features_with_cls(
                                images, clip_processor, clip_model, device, include_cls=include_cls
                            )
                            
                            # Immediate memory cleanup
                            cleanup_memory()
                            
                            eva_features = extract_eva_features_with_cls(
                                images, eva_processor, eva_model, device, include_cls=include_cls
                            )
                            
                            # Another memory cleanup
                            cleanup_memory()
                            
                        except RuntimeError as e:
                            if "out of memory" in str(e).lower():
                                oom_count += 1
                                print(f"   üí• OOM in batch {batch_idx}, reducing batch size")
                                
                                # Reduce batch size for next attempt
                                batch_size = max(batch_size // 2, 1)
                                
                                # Aggressive memory cleanup
                                cleanup_memory()
                                
                                if batch_size == 1:
                                    print(f"   ‚ùå OOM even with batch_size=1, skipping this batch")
                                    error_count += 1
                                    continue
                                else:
                                    # Try processing with smaller batch size
                                    # This would require breaking the batch into smaller pieces
                                    print(f"   üîÑ Breaking batch into smaller pieces...")
                                    continue
                            else:
                                raise e
                        
                        # Validate shapes match target tokens
                        assert clip_features.shape[1] == target_tokens, f"CLIP tokens: {clip_features.shape[1]} vs {target_tokens}"
                        assert eva_features.shape[1] == target_tokens, f"EVA tokens: {eva_features.shape[1]} vs {target_tokens}"
                        
                        # Move to CPU and store (already on CPU from extraction functions)
                        shard_clip_embeddings.append(clip_features)
                        shard_eva_embeddings.append(eva_features)
                        shard_captions.extend(captions)
                        shard_keys.extend(keys)
                        
                        total_samples += len(images)
                        
                        # Clear intermediate variables and cleanup
                        del clip_features, eva_features, images, captions, keys
                        cleanup_memory()
                        
                        # Progress update every 10 batches
                        if batch_idx % 10 == 0:
                            elapsed = time.time() - start_time
                            samples_per_sec = total_samples / elapsed if elapsed > 0 else 0
                            post_batch_memory = get_gpu_memory_info(device.index if hasattr(device, 'index') else None)
                            print(f"   Batch {batch_idx}: {total_samples} samples, {samples_per_sec:.1f} samples/sec, "
                                  f"{post_batch_memory.get('free_gb', 0):.1f} GB free")
                    
                    except Exception as e:
                        error_count += 1
                        print(f"   ‚ö†Ô∏è  Error processing batch {batch_idx}: {e}")
                        if error_count > batch_count * 0.5:  # If >50% of batches fail
                            raise Exception(f"Too many batch errors: {error_count}/{batch_count}")
                        continue
                
                print(f"   ‚úÖ Processed {batch_count} batches, {error_count} errors, {oom_count} OOM events, {total_samples} samples")
                break  # Success, exit retry loop
                
            except Exception as e:
                print(f"   ‚ùå Error iterating through dataloader (attempt {attempt + 1}): {e}")
                if attempt == max_retries - 1:
                    return {
                        'shard_idx': shard_idx,
                        'total_samples': 0,
                        'success': False,
                        'error': f'Dataloader iteration failed after {max_retries} attempts: {e}',
                        'oom_events': oom_count
                    }
                continue
        
        except Exception as e:
            print(f"   ‚ùå Error in processing attempt {attempt + 1}: {e}")
            if attempt == max_retries - 1:
                return {
                    'shard_idx': shard_idx,
                    'total_samples': 0,
                    'success': False,
                    'error': f'All processing attempts failed: {e}'
                }
            continue
    
    # Consolidate embeddings for this shard
    if shard_clip_embeddings and total_samples > 0:
        print(f"   üîÑ Consolidating {total_samples} embeddings...")
        
        try:
            final_clip = torch.cat(shard_clip_embeddings, dim=0)
            final_eva = torch.cat(shard_eva_embeddings, dim=0)
            
            # Final validation
            assert final_clip.shape[1] == target_tokens, f"Final CLIP shape: {final_clip.shape}"
            assert final_eva.shape[1] == target_tokens, f"Final EVA shape: {final_eva.shape}"
            assert final_clip.shape[2] == 1024, f"CLIP dim: {final_clip.shape[2]}"
            
            # Create shard data
            shard_data = {
                'clip_blip3o_embeddings': final_clip,
                'eva_blip3o_embeddings': final_eva,
                'captions': shard_captions,
                'keys': shard_keys,
                'total_samples': total_samples,
                'shard_idx': shard_idx,
                'source_tar': tar_file_path,
                'config': {
                    'clip_model': 'openai/clip-vit-large-patch14',
                    'eva_model': 'BAAI/EVA-CLIP-8B',
                    'clip_dim': 1024,
                    'eva_dim': final_eva.shape[2],
                    'tokens': target_tokens,
                    'include_cls': include_cls,
                    'mode': mode_suffix,
                    'extraction_method': 'memory_optimized_cls_patch_extraction_v6',
                    'format_version': f'blip3o_{target_tokens}_tokens_{"cls_" if include_cls else ""}patch_v6_memory_optimized',
                    'extraction_time': time.time() - start_time,
                    'cls_first': include_cls,
                    'patch_order': '16x16_row_major',
                    'distributed': world_size > 1,
                    'rank': rank,
                    'world_size': world_size,
                    'webdataset_fixed': True,
                    'memory_optimized': True,
                    'adaptive_batch_size': True,
                    'oom_events': oom_count,
                    'final_batch_size': batch_size,
                }
            }
            
            # Save shard data
            print(f"   üíæ Saving shard {shard_idx} to persistent storage...")
            
            try:
                with open(shard_path, 'wb') as f:
                    pickle.dump(shard_data, f, protocol=pickle.HIGHEST_PROTOCOL)
                
                file_size_mb = shard_path.stat().st_size / (1024 * 1024)
                
                print(f"   ‚úÖ Shard {shard_idx} completed:")
                print(f"      File: {shard_filename}")
                print(f"      Size: {file_size_mb:.1f} MB")
                print(f"      Samples: {total_samples}")
                print(f"      Mode: {mode_suffix} ({target_tokens} tokens)")
                print(f"      Time: {time.time() - start_time:.1f}s")
                print(f"      OOM events: {oom_count}")
                print(f"      Final batch size: {batch_size}")
                if world_size > 1:
                    print(f"      Distributed: rank {rank}/{world_size}")
                
                # Clear memory
                del shard_clip_embeddings, shard_eva_embeddings, final_clip, final_eva
                cleanup_memory()
                
                return {
                    'shard_idx': shard_idx,
                    'total_samples': total_samples,
                    'file_size_mb': file_size_mb,
                    'processing_time': time.time() - start_time,
                    'output_path': str(shard_path),
                    'success': True,
                    'mode': mode_suffix,
                    'tokens': target_tokens,
                    'include_cls': include_cls,
                    'rank': rank,
                    'world_size': world_size,
                    'webdataset_fixed': True,
                    'memory_optimized': True,
                    'oom_events': oom_count,
                    'final_batch_size': batch_size,
                }
            
            except Exception as e:
                print(f"   ‚ùå Failed to save shard {shard_idx}: {e}")
                return {
                    'shard_idx': shard_idx,
                    'total_samples': total_samples,
                    'success': False,
                    'error': f'File save failed: {e}',
                    'oom_events': oom_count
                }
        
        except Exception as e:
            print(f"   ‚ùå Failed to consolidate embeddings for shard {shard_idx}: {e}")
            return {
                'shard_idx': shard_idx,
                'total_samples': 0,
                'success': False,
                'error': f'Consolidation failed: {e}',
                'oom_events': oom_count
            }
    
    else:
        print(f"   ‚ùå No embeddings extracted from shard {shard_idx}")
        return {
            'shard_idx': shard_idx,
            'total_samples': 0,
            'success': False,
            'error': 'No embeddings extracted - empty or corrupted TAR file',
            'oom_events': oom_count
        }

def main():
    """Main extraction function with memory optimization."""
    parser = argparse.ArgumentParser(description="MEMORY-OPTIMIZED BLIP3-o Embedding Extraction")
    parser.add_argument("--include_cls", action="store_true", default=False,
                       help="Include CLS token (257 tokens) or patches only (256 tokens)")
    parser.add_argument("--max_shards", type=int, default=None,
                       help="Maximum number of shards to process (for testing)")
    parser.add_argument("--batch_size", type=int, default=12,  # Reduced default
                       help="Initial batch size for processing (will be adapted)")
    
    args = parser.parse_args()
    
    # Setup
    target_tokens = 257 if args.include_cls else 256
    mode_name = "CLS+Patches" if args.include_cls else "Patches only"
    
    print("üöÄ MEMORY-OPTIMIZED BLIP3-o Embedding Extraction")
    print("=" * 70)
    print(f"Mode: {mode_name} ({target_tokens} tokens)")
    print(f"CLS token: {'First token [0]' if args.include_cls else 'Not included'}")
    print(f"Patches: {'Tokens [1:257]' if args.include_cls else 'Tokens [0:256]'} (16x16 grid)")
    print(f"Initial batch size: {args.batch_size} (adaptive)")
    print(f"Max shards: {args.max_shards if args.max_shards else 'All'}")
    print(f"MEMORY OPTIMIZATION: Adaptive batch sizing, enhanced cleanup, OOM detection")
    print("=" * 70)
    
    if not torch.cuda.is_available():
        raise RuntimeError("CUDA required for this script")
    
    device = torch.device('cuda')
    project_root = setup_paths()
    
    # Print GPU information
    gpu_info = get_gpu_memory_info(device.index)
    print(f"üéÆ Using GPU: {torch.cuda.get_device_name(0)}")
    print(f"üß† GPU Memory: {gpu_info.get('total_gb', 0):.1f} GB total, {gpu_info.get('free_gb', 0):.1f} GB free")
    
    # Check WebDataset status
    wds_info = check_webdataset_version()
    
    # Setup temp manager
    temp_manager = setup_temp_manager()
    
    if temp_manager:
        mode_suffix = "cls_patch" if args.include_cls else "patch_only"
        embeddings_dir = temp_manager.create_embeddings_subdirectory(f"{mode_suffix}_{target_tokens}_tokens")
        working_dir = temp_manager.get_working_dir()
        temp_manager.setup_model_cache()
        
        print(f"‚úÖ Using structured temp management")
        print(f"üìÅ Embeddings dir: {embeddings_dir}")
        print(f"üìÅ Working dir: {working_dir}")
    else:
        # Fallback
        if "TMPDIR" in os.environ:
            base_temp = Path(os.environ["TMPDIR"])
        else:
            base_temp = Path("./temp")
        
        mode_suffix = "cls_patch" if args.include_cls else "patch_only"
        embeddings_dir = base_temp / f"embeddings_{mode_suffix}"
        working_dir = base_temp / "working"
        
        embeddings_dir.mkdir(parents=True, exist_ok=True)
        working_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"‚ö†Ô∏è  Using fallback temp management")
        print(f"üìÅ Embeddings dir: {embeddings_dir}")
    
    # Load models with memory optimization
    try:
        clip_processor, clip_model, eva_processor, eva_model = load_models(device)
    except Exception as e:
        print(f"‚ùå Failed to load models: {e}")
        return 1
    
    # Find TAR files
    try:
        tar_files = find_data_files(temp_manager, max_shards=args.max_shards)
    except Exception as e:
        print(f"‚ùå {e}")
        return 1
    
    print(f"üì§ Output directory: {embeddings_dir}")
    
    # Process each TAR file with memory optimization
    print(f"\nüîÑ Processing {len(tar_files)} TAR files with MEMORY OPTIMIZATION...")
    
    processing_results = []
    total_samples_all = 0
    failed_shards = []
    oom_shards = []
    
    for shard_idx, tar_file in enumerate(tar_files):
        print(f"\n" + "="*60)
        print(f"PROCESSING SHARD {shard_idx + 1}/{len(tar_files)}")
        print(f"="*60)
        
        result = process_single_tar(
            tar_file_path=tar_file,
            shard_idx=shard_idx,
            clip_processor=clip_processor,
            clip_model=clip_model,
            eva_processor=eva_processor,
            eva_model=eva_model,
            device=device,
            output_dir=embeddings_dir,
            working_dir=working_dir,
            batch_size=args.batch_size,
            include_cls=args.include_cls,
            target_tokens=target_tokens,
            world_size=1,  # Single GPU mode
            rank=0
        )
        
        if result and result['success']:
            processing_results.append(result)
            total_samples_all += result['total_samples']
            print(f"‚úÖ Shard {shard_idx} successful: {result['total_samples']} samples")
            
            # Track OOM events
            if result.get('oom_events', 0) > 0:
                print(f"   ‚ö†Ô∏è Had {result['oom_events']} OOM events but recovered")
                
        else:
            failed_shards.append(shard_idx)
            if result and result.get('oom_events', 0) > 0:
                oom_shards.append(shard_idx)
                print(f"üí• Shard {shard_idx} failed due to OOM: {result.get('error', 'Unknown error')}")
            else:
                print(f"‚ùå Shard {shard_idx} failed: {result.get('error', 'Unknown error') if result else 'No result'}")
    
    # Create manifest with memory optimization info
    manifest_data = {
        'total_shards': len(processing_results),
        'total_samples': total_samples_all,
        'extraction_mode': mode_name,
        'tokens_per_sample': target_tokens,
        'include_cls': args.include_cls,
        'cls_token_position': 0 if args.include_cls else None,
        'patch_tokens_range': [1, 257] if args.include_cls else [0, 256],
        'extraction_timestamp': time.time(),
        'shards': processing_results,
        'failed_shards': failed_shards,
        'oom_shards': oom_shards,
        'format_version': f'blip3o_{target_tokens}_tokens_{"cls_" if args.include_cls else ""}patch_v6_memory_optimized',
        'webdataset_info': wds_info,
        'memory_optimization': {
            'adaptive_batch_sizing': True,
            'enhanced_memory_cleanup': True,
            'oom_detection_and_recovery': True,
            'memory_monitoring': True,
            'optimized_model_loading': True,
        },
        'fixes_applied': [
            'WebDataset version compatibility checks',
            'Multiple fallback approaches for dataset creation',
            'MEMORY: Adaptive batch sizing based on GPU memory',
            'MEMORY: Enhanced memory cleanup and monitoring',
            'MEMORY: OOM detection and recovery mechanisms',
            'MEMORY: Progressive batch size reduction on pressure',
            'MEMORY: Optimized model loading for efficiency',
            'Better error handling and diagnostics',
            'Direct TAR processing fallback when WebDataset fails',
            'Robust single-GPU processing support'
        ],
        'token_layout': {
            'cls_token': {'included': args.include_cls, 'position': 0 if args.include_cls else None},
            'patches': {
                'count': 256,
                'positions': [1, 257] if args.include_cls else [0, 256],
                'layout': '16x16 grid in row-major order'
            }
        },
        'usage': {
            'training_command_cls_patch': f'python train_blip3o_enhanced.py --chunked_embeddings_dir {embeddings_dir} --training_mode cls_patch',
            'training_command_patch_only': f'python train_blip3o_enhanced.py --chunked_embeddings_dir {embeddings_dir} --training_mode patch_only',
        }
    }
    
    manifest_path = embeddings_dir / "embeddings_manifest.json"
    with open(manifest_path, 'w') as f:
        json.dump(manifest_data, f, indent=2)
    
    # Final status
    print("\n" + "=" * 80)
    print("‚úÖ MEMORY-OPTIMIZED EMBEDDING EXTRACTION COMPLETED!")
    print("=" * 80)
    print(f"üìä SUMMARY:")
    print(f"   Mode: {mode_name} ({target_tokens} tokens)")
    print(f"   CLS token: {'Included at position [0]' if args.include_cls else 'Not included'}")
    print(f"   Patches: {'Positions [1:257]' if args.include_cls else 'Positions [0:256]'} (16x16)")
    print(f"   TAR files processed: {len(tar_files)}")
    print(f"   Successful shards: {len(processing_results)}")
    print(f"   Failed shards: {len(failed_shards)}")
    print(f"   OOM-related failures: {len(oom_shards)}")
    print(f"   Total samples: {total_samples_all:,}")
    print(f"   Embeddings location: {embeddings_dir}")
    print(f"   Manifest file: {manifest_path}")
    print(f"   Memory optimization: ‚úÖ ENABLED")
    
    if wds_info:
        print(f"\nüì¶ WebDataset Status:")
        print(f"   Version: {wds_info['version']}")
        print(f"   Compatibility: {'‚úÖ FIXED' if processing_results else '‚ùå ISSUES'}")
    
    if oom_shards:
        print(f"\nüí• OOM-related failures: {oom_shards}")
        print(f"   These shards failed due to out-of-memory issues")
        print(f"   Consider further reducing batch size or processing individually")
    
    if failed_shards:
        print(f"\n‚ö†Ô∏è Other failed shards: {[s for s in failed_shards if s not in oom_shards]}")
        print(f"   Success rate: {len(processing_results)/(len(processing_results)+len(failed_shards))*100:.1f}%")
        
        if len(processing_results) > 0:
            print(f"‚úÖ Partial success - continuing with {len(processing_results)} shards")
        else:
            print(f"‚ùå All shards failed - check memory configuration and TAR files")
            return 1
    
    if len(processing_results) > 0:
        print(f"\nüéâ SUCCESS! {len(processing_results)} shards processed successfully with memory optimization!")
        print("Ready for BLIP3-o training!")
        print(f"\nUsage commands:")
        print(f"CLS+Patch mode (257 tokens):")
        print(f"  python train_blip3o_enhanced.py --chunked_embeddings_dir {embeddings_dir} --training_mode cls_patch")
        print(f"Patch-only mode (256 tokens):")
        print(f"  python train_blip3o_enhanced.py --chunked_embeddings_dir {embeddings_dir} --training_mode patch_only")
    
    print("=" * 80)
    print("üîß MEMORY OPTIMIZATION FIXES APPLIED:")
    print("  ‚úÖ Adaptive batch sizing prevents OOM crashes")
    print("  ‚úÖ Enhanced memory cleanup improves stability")
    print("  ‚úÖ OOM detection enables graceful recovery")
    print("  ‚úÖ Memory monitoring provides visibility")
    print("  ‚úÖ Optimized model loading reduces memory footprint")
    print("  ‚úÖ Progressive batch size reduction handles pressure")
    print("  ‚úÖ Better error handling prevents complete failures")
    
    return 0 if len(processing_results) > 0 else 1

if __name__ == "__main__":
    try:
        exit_code = main()
        sys.exit(exit_code)
    except Exception as e:
        print(f"‚ùå Critical error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)