#!/usr/bin/env python3
"""
Unified BLIP3-o Embedding Extraction with Memory Optimization
src/modules/extract_embeddings_unified.py

UNIFIED EXTRACTION: Handles both single-GPU and multi-GPU extraction automatically
MEMORY OPTIMIZATION FIXES:
1. Enhanced memory monitoring and cleanup
2. Adaptive batch processing based on available memory
3. Better model loading with memory efficiency
4. OOM detection and recovery mechanisms
5. Progressive memory management throughout processing
"""

import sys
import os
import torch
import torch.nn.functional as F
import torch.distributed as dist
import torch.multiprocessing as mp
from transformers import CLIPProcessor, CLIPModel, AutoModel, CLIPImageProcessor
import pickle
from tqdm import tqdm
import numpy as np
from pathlib import Path
import gc
import psutil
import time
import glob
import json
import shutil
import argparse
from datetime import timedelta  # FIXED: Added missing import
from typing import List, Optional, Dict, Any
import logging

def setup_paths():
    """Setup paths for project structure"""
    script_dir = Path(__file__).parent
    project_root = script_dir.parent.parent
    
    # Add import paths
    sys.path.insert(0, str(project_root))
    sys.path.insert(0, str(project_root / "src"))
    sys.path.insert(0, str(project_root / "src" / "data_hand"))
    sys.path.insert(0, str(project_root / "src" / "utils"))
    
    return project_root

def setup_temp_manager():
    """Setup temp manager for structured directory management."""
    try:
        from src.modules.utils.temp_manager import setup_snellius_environment
        manager = setup_snellius_environment("blip3o_workspace")
        return manager
    except ImportError:
        print("âš ï¸  Temp manager not available, using fallback directories")
        return None

def get_memory_usage():
    """Get current memory usage in GB"""
    try:
        process = psutil.Process(os.getpid())
        memory_mb = process.memory_info().rss / 1024 / 1024
        return memory_mb / 1024
    except:
        return 0.0

def get_gpu_memory_info(device_id: int = None) -> dict:
    """Get GPU memory information"""
    try:
        if device_id is not None:
            torch.cuda.set_device(device_id)
        else:
            device_id = torch.cuda.current_device()
            
        total_memory = torch.cuda.get_device_properties(device_id).total_memory / 1e9
        allocated = torch.cuda.memory_allocated(device_id) / 1e9
        cached = torch.cuda.memory_reserved(device_id) / 1e9
        free = total_memory - cached
        
        return {
            'device_id': device_id,
            'total_gb': total_memory,
            'allocated_gb': allocated,
            'cached_gb': cached,
            'free_gb': free,
            'utilization_pct': (cached / total_memory) * 100
        }
        
    except Exception as e:
        return {'error': str(e), 'device_id': device_id}

def cleanup_memory():
    """Enhanced memory cleanup with monitoring"""
    initial_memory = get_memory_usage()
    initial_gpu_memory = get_gpu_memory_info() if torch.cuda.is_available() else {}
    
    # Python garbage collection
    collected = gc.collect()
    
    # PyTorch CUDA cleanup
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    
    # Force another garbage collection
    gc.collect()
    
    final_memory = get_memory_usage()
    final_gpu_memory = get_gpu_memory_info() if torch.cuda.is_available() else {}
    
    # Calculate cleanup effectiveness
    system_freed = initial_memory - final_memory
    gpu_freed = (initial_gpu_memory.get('cached_gb', 0) - 
                final_gpu_memory.get('cached_gb', 0)) if torch.cuda.is_available() else 0
    
    return {
        'objects_collected': collected,
        'system_memory_freed_gb': system_freed,
        'gpu_memory_freed_gb': gpu_freed,
        'final_gpu_free_gb': final_gpu_memory.get('free_gb', 0)
    }

def adaptive_batch_size_selection(device, initial_batch_size: int = 16, min_free_memory_gb: float = 5.0) -> int:
    """Adaptively select batch size based on available GPU memory"""
    try:
        if not torch.cuda.is_available():
            return min(initial_batch_size, 8)  # Conservative for CPU
        
        memory_info = get_gpu_memory_info(device.index if hasattr(device, 'index') else None)
        free_memory_gb = memory_info.get('free_gb', 0)
        
        print(f"GPU memory: {free_memory_gb:.1f} GB free")
        
        # Adaptive batch size selection based on available memory
        if free_memory_gb > 60:  # Abundant memory (H100)
            recommended_batch_size = min(initial_batch_size, 24)
        elif free_memory_gb > 40:  # Good memory availability
            recommended_batch_size = min(initial_batch_size, 16)
        elif free_memory_gb > 20:  # Moderate memory
            recommended_batch_size = min(initial_batch_size, 12)
        elif free_memory_gb > 10:  # Limited memory
            recommended_batch_size = min(initial_batch_size, 8)
        elif free_memory_gb > min_free_memory_gb:  # Minimal memory
            recommended_batch_size = min(initial_batch_size, 4)
        else:  # Critical memory situation
            recommended_batch_size = 2
            print(f"âš ï¸ Critical memory situation: {free_memory_gb:.1f} GB free, using batch_size=2")
        
        if recommended_batch_size != initial_batch_size:
            print(f"ðŸ“Š Adjusted batch size from {initial_batch_size} to {recommended_batch_size} based on memory")
        
        return recommended_batch_size
        
    except Exception as e:
        print(f"âš ï¸ Could not determine adaptive batch size: {e}")
        return min(initial_batch_size, 8)  # Conservative fallback

def load_models(device):
    """Load CLIP and EVA-CLIP models with memory optimization"""
    print("ðŸ“¦ Loading models with memory optimization...")
    
    # Get initial memory state
    initial_memory = get_gpu_memory_info(device.index if hasattr(device, 'index') else None)
    print(f"   Initial GPU memory: {initial_memory.get('free_gb', 0):.1f} GB free")
    
    # Load CLIP ViT-L/14 with memory optimization
    print("   Loading CLIP ViT-L/14...")
    clip_processor = CLIPProcessor.from_pretrained(
        "openai/clip-vit-large-patch14",
        cache_dir=os.environ.get('TRANSFORMERS_CACHE')
    )
    
    clip_model = CLIPModel.from_pretrained(
        "openai/clip-vit-large-patch14",
        torch_dtype=torch.float16,
        device_map=None,  # Manual device placement
        cache_dir=os.environ.get('TRANSFORMERS_CACHE')
    ).to(device)
    clip_model.eval()
    
    # Memory cleanup after CLIP loading
    cleanup_result = cleanup_memory()
    print(f"   After CLIP: {cleanup_result['final_gpu_free_gb']:.1f} GB free (freed {cleanup_result['gpu_memory_freed_gb']:.1f} GB)")
    
    # Load EVA-CLIP-8B with memory optimization
    print("   Loading EVA-CLIP-8B...")
    eva_model = AutoModel.from_pretrained(
        "BAAI/EVA-CLIP-8B", 
        trust_remote_code=True,
        torch_dtype=torch.float16,
        device_map=None,  # Manual device placement
        cache_dir=os.environ.get('TRANSFORMERS_CACHE')
    ).to(device)
    
    eva_processor = CLIPImageProcessor.from_pretrained(
        "openai/clip-vit-large-patch14",
        cache_dir=os.environ.get('TRANSFORMERS_CACHE')
    )
    eva_model.eval()
    
    # Final memory cleanup
    cleanup_result = cleanup_memory()
    final_memory = get_gpu_memory_info(device.index if hasattr(device, 'index') else None)
    
    print("âœ… Models loaded successfully with memory optimization")
    print(f"ðŸ’¾ Final GPU memory: {final_memory.get('free_gb', 0):.1f} GB free")
    print(f"ðŸ’¾ Total memory used by models: {initial_memory.get('free_gb', 0) - final_memory.get('free_gb', 0):.1f} GB")
    
    return clip_processor, clip_model, eva_processor, eva_model

def extract_clip_features_with_cls(images, processor, model, device, include_cls=True):
    """Extract CLIP ViT-L/14 features with CLS token + patches (memory optimized)"""
    features = []
    
    for img in images:
        try:
            inputs = processor(images=img, return_tensors="pt")
            inputs = {k: v.to(device).half() if v.dtype == torch.float32 else v.to(device) 
                     for k, v in inputs.items()}
            
            with torch.no_grad():
                vision_outputs = model.vision_model(
                    pixel_values=inputs['pixel_values'],
                    output_hidden_states=True,
                    return_dict=True
                )
                
                # Get all hidden states (CLS + patches)
                if include_cls:
                    all_embeddings = vision_outputs.last_hidden_state  # [1, 257, 1024]
                    expected_tokens = 257
                else:
                    all_embeddings = vision_outputs.last_hidden_state[:, 1:, :]  # [1, 256, 1024]
                    expected_tokens = 256
                
                batch_size, num_tokens, hidden_dim = all_embeddings.shape
                
                # Validate dimensions
                assert hidden_dim == 1024, f"Expected CLIP 1024-dim, got {hidden_dim}"
                assert num_tokens == expected_tokens, f"Expected {expected_tokens} tokens, got {num_tokens}"
                
                # Convert to float32 and move to CPU immediately to save GPU memory
                features.append(all_embeddings.squeeze().cpu().float())
                
                # Clear GPU memory immediately
                del vision_outputs, all_embeddings
                
        except Exception as e:
            print(f"âš ï¸ Error extracting CLIP features for image: {e}")
            # Create a zero tensor as fallback
            expected_tokens = 257 if include_cls else 256
            fallback_tensor = torch.zeros(expected_tokens, 1024)
            features.append(fallback_tensor)
    
    return torch.stack(features)

def extract_eva_features_with_cls(images, processor, model, device, include_cls=True):
    """Extract EVA-CLIP-8B features with CLS token + patches (memory optimized)"""
    features = []
    
    for img in images:
        try:
            inputs = processor(images=img, return_tensors="pt")
            pixel_values = inputs['pixel_values'].to(device).half()
            
            with torch.no_grad():
                vision_outputs = model.vision_model(
                    pixel_values=pixel_values,
                    output_hidden_states=True,
                    return_dict=True
                )
                
                # Get all hidden states (CLS + patches)
                if include_cls:
                    all_embeddings = vision_outputs.last_hidden_state  # [1, 257, hidden_dim]
                    expected_tokens = 257
                else:
                    all_embeddings = vision_outputs.last_hidden_state[:, 1:, :]  # [1, 256, hidden_dim]
                    expected_tokens = 256
                
                batch_size, num_tokens, hidden_dim = all_embeddings.shape
                
                # Validate dimensions
                assert num_tokens == expected_tokens, f"Expected {expected_tokens} tokens, got {num_tokens}"
                
                # Convert to float32 and move to CPU immediately to save GPU memory
                features.append(all_embeddings.squeeze().cpu().float())
                
                # Clear GPU memory immediately
                del vision_outputs, all_embeddings, pixel_values
                
        except Exception as e:
            print(f"âš ï¸ Error extracting EVA features for image: {e}")
            # Create a zero tensor as fallback (need to determine EVA hidden_dim)
            expected_tokens = 257 if include_cls else 256
            # EVA-CLIP-8B typically has 4096 dim
            fallback_tensor = torch.zeros(expected_tokens, 4096)
            features.append(fallback_tensor)
    
    return torch.stack(features)

def find_data_files(temp_manager, max_shards=None):
    """Find downloaded tar files using temp manager."""
    if temp_manager:
        datasets_dir = temp_manager.get_datasets_dir()
        print(f"ðŸ” Searching for dataset shards in: {datasets_dir}")
    else:
        # Fallback to old method
        print("ðŸ” Searching for dataset shards (fallback method)...")
        if "TMPDIR" in os.environ:
            datasets_dir = Path(os.environ["TMPDIR"]) / "blip3o_data"
        elif "SCRATCH_SHARED" in os.environ:
            user = os.environ.get("USER", "user")
            datasets_dir = Path(os.environ["SCRATCH_SHARED"]) / user / "blip3o_data"
        else:
            datasets_dir = Path(__file__).parent.parent.parent / "data"
    
    # Look for tar files
    tar_files = list(datasets_dir.glob("*.tar"))
    if tar_files:
        tar_files.sort()  # Sort numerically
        tar_files = [str(f) for f in tar_files]
        
        # Limit number of shards if specified
        if max_shards is not None:
            tar_files = tar_files[:max_shards]
            print(f"ðŸ“Š Limited to {max_shards} shards")
        
        print(f"   âœ… Found {len(tar_files)} tar files")
        
        # Validate files exist and show details
        valid_files = []
        total_size_gb = 0
        
        print(f"\nðŸ“Š Validating found files...")
        for tar_file in tar_files:
            tar_path = Path(tar_file)
            if tar_path.exists():
                size_gb = tar_path.stat().st_size / (1024**3)
                total_size_gb += size_gb
                valid_files.append(tar_file)
                print(f"   âœ… {tar_path.name}: {size_gb:.2f} GB")
            else:
                print(f"   âŒ Missing: {tar_file}")
        
        print(f"\nðŸŽ¯ Using {len(valid_files)} tar files for extraction")
        print(f"ðŸ“Š Total dataset size: {total_size_gb:.2f} GB")
        
        # Estimate samples
        estimated_samples = int(total_size_gb * 400000 / 1.0)  # Rough estimate
        print(f"ðŸ“Š Estimated samples: ~{estimated_samples:,}")
        
        return valid_files
    
    raise FileNotFoundError(
        f"No TAR files found in {datasets_dir}!\n"
        "Please download dataset shards first:\n"
        "  python src/data_hand/download_data.py --shards 0 1 2 3 4 5 6 7 8 9\n"
    )

def check_webdataset_version():
    """Check WebDataset version and capabilities"""
    try:
        import webdataset as wds
        version = getattr(wds, '__version__', 'unknown')
        
        # Check for pipe method
        has_pipe = hasattr(wds.WebDataset([]), 'pipe')
        
        # Check for split_by_node
        has_split_by_node = hasattr(wds, 'split_by_node')
        
        print(f"ðŸ“¦ WebDataset version: {version}")
        print(f"   Has .pipe() method: {'âœ…' if has_pipe else 'âŒ'}")
        print(f"   Has split_by_node: {'âœ…' if has_split_by_node else 'âŒ'}")
        
        return {
            'version': version,
            'has_pipe': has_pipe,
            'has_split_by_node': has_split_by_node
        }
    except ImportError:
        print("âŒ WebDataset not available")
        return None

def create_webdataset_with_fallback(tar_file_path: str, world_size: int = 1, rank: int = 0):
    """Create WebDataset with fallback mechanisms"""
    print(f"ðŸ”§ Creating WebDataset (world_size={world_size}, rank={rank})")
    
    # Check WebDataset capabilities
    wds_info = check_webdataset_version()
    if not wds_info:
        print("âŒ WebDataset not available, using fallback")
        return create_fallback_tar_processor(tar_file_path, world_size, rank)
    
    try:
        import webdataset as wds
        from PIL import Image
        import io
        
        def decode_sample(sample):
            """Decode a sample from WebDataset with memory optimization"""
            try:
                # Get image
                for ext in ['jpg', 'jpeg', 'png', 'webp']:
                    if ext in sample:
                        image_data = sample[ext]
                        break
                else:
                    return None
                
                # Load image and immediately convert to RGB to save memory
                image = Image.open(io.BytesIO(image_data)).convert('RGB')
                
                # Get caption
                caption = ""
                for caption_key in ['txt', 'caption', 'text']:
                    if caption_key in sample:
                        caption_data = sample[caption_key]
                        if isinstance(caption_data, bytes):
                            caption = caption_data.decode('utf-8').strip()
                        else:
                            caption = str(caption_data).strip()
                        break
                
                key = sample.get('__key__', 'unknown')
                
                # Clear sample data to free memory
                sample.clear()
                
                return {
                    'image': image,
                    'caption': caption,
                    'key': key,
                }
            except Exception as e:
                return None
        
        # Try different WebDataset approaches
        dataset = None
        
        # APPROACH 1: Try modern WebDataset with .pipe() method
        if wds_info['has_pipe'] and wds_info['has_split_by_node'] and world_size > 1:
            print("   Attempting modern WebDataset with .pipe() method...")
            try:
                dataset = (
                    wds.WebDataset([tar_file_path], empty_check=False, shardshuffle=False)
                    .pipe(wds.split_by_node, group_size=None)
                    .pipe(wds.split_by_worker)
                    .map(decode_sample)
                    .select(lambda x: x is not None)
                )
                print("   âœ… Modern WebDataset created successfully")
                return dataset
            except Exception as e:
                print(f"   âŒ Modern WebDataset failed: {e}")
        
        # APPROACH 2: Try WebDataset with manual distributed logic
        if world_size > 1:
            print("   Attempting WebDataset with manual distributed processing...")
            try:
                # Create dataset with manual rank-based filtering
                class RankFilteredDataset:
                    def __init__(self, base_dataset, rank, world_size):
                        self.base_dataset = base_dataset
                        self.rank = rank
                        self.world_size = world_size
                        self.counter = 0
                    
                    def __iter__(self):
                        self.counter = 0
                        for item in self.base_dataset:
                            if self.counter % self.world_size == self.rank:
                                yield item
                            self.counter += 1
                
                base_dataset = (
                    wds.WebDataset([tar_file_path], empty_check=False, shardshuffle=False)
                    .map(decode_sample)
                    .select(lambda x: x is not None)
                )
                
                dataset = RankFilteredDataset(base_dataset, rank, world_size)
                print("   âœ… Manual distributed WebDataset created successfully")
                return dataset
            except Exception as e:
                print(f"   âŒ Manual distributed WebDataset failed: {e}")
        
        # APPROACH 3: Simple WebDataset (fallback)
        print("   Using simple WebDataset (single-GPU mode or fallback)...")
        try:
            dataset = (
                wds.WebDataset([tar_file_path], empty_check=False, shardshuffle=False)
                .map(decode_sample)
                .select(lambda x: x is not None)
            )
            print("   âœ… Simple WebDataset created successfully")
            return dataset
        except Exception as e:
            print(f"   âŒ Simple WebDataset failed: {e}")
        
        print("âŒ All WebDataset approaches failed")
        return None
        
    except ImportError as e:
        print(f"âŒ WebDataset import failed: {e}")
        return None
    except Exception as e:
        print(f"âŒ Unexpected error creating WebDataset: {e}")
        return None

def create_fallback_tar_processor(tar_file_path: str, world_size: int = 1, rank: int = 0):
    """Fallback TAR processor using Python tarfile when WebDataset fails"""
    print(f"ðŸ”„ Creating fallback TAR processor for {Path(tar_file_path).name}")
    
    try:
        import tarfile
        from PIL import Image
        import io
        
        class FallbackTarDataset:
            def __init__(self, tar_path, rank=0, world_size=1):
                self.tar_path = tar_path
                self.rank = rank
                self.world_size = world_size
                
            def __iter__(self):
                try:
                    with tarfile.open(self.tar_path, 'r') as tar:
                        members = tar.getmembers()
                        
                        # Filter members for this rank if distributed
                        if self.world_size > 1:
                            members = [m for i, m in enumerate(members) if i % self.world_size == self.rank]
                        
                        for member in members:
                            if member.isfile():
                                try:
                                    # Extract the file
                                    file_obj = tar.extractfile(member)
                                    if file_obj is None:
                                        continue
                                    
                                    # Try to decode as image
                                    if any(ext in member.name.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                                        try:
                                            # Read and immediately convert image to save memory
                                            image_data = file_obj.read()
                                            image = Image.open(io.BytesIO(image_data)).convert('RGB')
                                            
                                            # Clear image data to free memory
                                            del image_data
                                            
                                            # Create a simple caption (or try to find associated text file)
                                            key = Path(member.name).stem
                                            caption = f"Image {key}"
                                            
                                            yield {
                                                'image': image,
                                                'caption': caption,
                                                'key': key,
                                            }
                                        except Exception as e:
                                            continue
                                    
                                except Exception as e:
                                    continue
                except Exception as e:
                    print(f"âŒ Error processing TAR file: {e}")
                    return
        
        dataset = FallbackTarDataset(tar_file_path, rank, world_size)
        print("   âœ… Fallback TAR processor created successfully")
        return dataset
        
    except Exception as e:
        print(f"âŒ Fallback TAR processor failed: {e}")
        return None

def setup_distributed(rank: int, world_size: int, master_port: str = "12355"):
    """Initialize distributed training with better error handling"""
    try:
        os.environ['MASTER_ADDR'] = 'localhost'
        os.environ['MASTER_PORT'] = master_port
        os.environ['RANK'] = str(rank)
        os.environ['LOCAL_RANK'] = str(rank)
        os.environ['WORLD_SIZE'] = str(world_size)
        
        # Initialize the process group
        dist.init_process_group(
            backend='nccl',
            init_method=f'env://',
            world_size=world_size,
            rank=rank,
            timeout=timedelta(minutes=30)  # FIXED: Added missing import
        )
        
        # Set CUDA device
        torch.cuda.set_device(rank)
        device = torch.device(f'cuda:{rank}')
        
        if rank == 0:
            print(f"âœ… Distributed initialized: {world_size} GPUs")
        
        return device
        
    except Exception as e:
        print(f"Failed to setup distributed on rank {rank}: {e}")
        raise

def cleanup_distributed():
    """Clean up distributed training"""
    try:
        if dist.is_initialized():
            dist.destroy_process_group()
    except Exception as e:
        print(f"Error during distributed cleanup: {e}")

def distribute_tar_files(tar_files: List[str], world_size: int, rank: int) -> List[str]:
    """Distribute TAR files across GPUs with load balancing"""
    assigned_files = []
    for i, tar_file in enumerate(tar_files):
        if i % world_size == rank:
            assigned_files.append(tar_file)
    
    print(f"Rank {rank}: Assigned {len(assigned_files)}/{len(tar_files)} TAR files")
    return assigned_files

def get_gpu_specific_output_path(base_path: Path, rank: int, shard_idx: int, mode_suffix: str) -> Path:
    """Generate GPU-specific output paths to prevent conflicts"""
    return base_path / f"embeddings_shard_{shard_idx:05d}_{mode_suffix}_gpu{rank}.pkl"

def process_single_tar(
    tar_file_path: str,
    shard_idx: int,
    clip_processor, clip_model, eva_processor, eva_model,
    device: torch.device,
    output_dir: Path,
    working_dir: Path,
    batch_size: int = 16,
    include_cls: bool = True,
    target_tokens: int = 257,
    world_size: int = 1,
    rank: int = 0,
    max_retries: int = 3
) -> dict:
    """Process a single TAR file with enhanced memory management"""
    
    print(f"\nðŸ”„ Processing shard {shard_idx}: {Path(tar_file_path).name}")
    print(f"   Mode: {'CLS+Patches' if include_cls else 'Patches only'} ({target_tokens} tokens)")
    if world_size > 1:
        print(f"   Distributed: rank {rank}/{world_size}")
    
    # Get initial memory state
    initial_memory = get_gpu_memory_info(device.index if hasattr(device, 'index') else None)
    print(f"   Initial GPU memory: {initial_memory.get('free_gb', 0):.1f} GB free")
    
    # Adaptive batch size selection
    adaptive_batch_size = adaptive_batch_size_selection(device, batch_size)
    if adaptive_batch_size != batch_size:
        print(f"   Adjusted batch size from {batch_size} to {adaptive_batch_size}")
        batch_size = adaptive_batch_size
    
    # Expected output file path
    mode_suffix = "cls_patch" if include_cls else "patch_only"
    if world_size > 1:
        # Multi-GPU: use GPU-specific naming
        shard_filename = f"embeddings_shard_{shard_idx:05d}_{mode_suffix}_gpu{rank}.pkl"
    else:
        # Single-GPU: use standard naming
        shard_filename = f"embeddings_shard_{shard_idx:05d}_{mode_suffix}.pkl"
    
    shard_path = output_dir / shard_filename
    
    # Check if this shard already exists
    if shard_path.exists():
        print(f"   âœ… Shard {shard_idx} already exists: {shard_path}")
        try:
            with open(shard_path, 'rb') as f:
                existing_data = pickle.load(f)
            sample_count = len(existing_data.get('captions', []))
            return {
                'shard_idx': shard_idx,
                'total_samples': sample_count,
                'success': True,
                'skipped': True,
                'output_path': str(shard_path)
            }
        except:
            print(f"   âš ï¸  Could not read existing file, will reprocess...")
            shard_path.unlink()
    
    # Try processing with retries and memory optimization
    for attempt in range(max_retries):
        try:
            print(f"   ðŸ”„ Processing attempt {attempt + 1}/{max_retries} (batch_size={batch_size})")
            
            # Pre-processing memory cleanup
            cleanup_result = cleanup_memory()
            print(f"   Memory cleanup: freed {cleanup_result['gpu_memory_freed_gb']:.1f} GB GPU memory")
            
            # Create dataset
            dataset = create_webdataset_with_fallback(tar_file_path, world_size, rank)
            
            if dataset is None:
                print(f"   âŒ All dataset creation methods failed")
                if attempt == max_retries - 1:
                    return {
                        'shard_idx': shard_idx,
                        'total_samples': 0,
                        'success': False,
                        'error': 'Failed to create any dataset processor'
                    }
                continue
            
            # Create dataloader with memory-optimized settings
            def simple_collate(batch):
                valid_batch = [item for item in batch if item is not None]
                if not valid_batch:
                    return None
                    
                images = [item['image'] for item in valid_batch]
                captions = [item['caption'] for item in valid_batch]
                keys = [item['key'] for item in valid_batch]
                return {
                    'image': images,
                    'caption': captions,
                    'key': keys
                }
            
            from torch.utils.data import DataLoader
            dataloader = DataLoader(
                dataset, 
                batch_size=batch_size, 
                collate_fn=simple_collate,
                num_workers=0,
                drop_last=False,
                pin_memory=False
            )
            
            print(f"   âœ… Dataloader created successfully with batch_size={batch_size}")
            
            # Storage for this shard's embeddings
            shard_clip_embeddings = []
            shard_eva_embeddings = []
            shard_captions = []
            shard_keys = []
            
            total_samples = 0
            start_time = time.time()
            batch_count = 0
            error_count = 0
            oom_count = 0
            
            print(f"   ðŸ“Š Processing batches with memory monitoring...")
            
            # Process all batches in this TAR file
            try:
                for batch_idx, batch in enumerate(tqdm(dataloader, desc=f"Shard {shard_idx}", unit="batch")):
                    if batch is None:
                        continue
                        
                    batch_count += 1
                    
                    try:
                        images = batch['image']
                        captions = batch['caption']
                        keys = batch['key']
                        
                        if not images:  # Skip empty batches
                            continue
                        
                        # Extract features with memory monitoring
                        try:
                            clip_features = extract_clip_features_with_cls(
                                images, clip_processor, clip_model, device, include_cls=include_cls
                            )
                            
                            # Immediate memory cleanup
                            cleanup_memory()
                            
                            eva_features = extract_eva_features_with_cls(
                                images, eva_processor, eva_model, device, include_cls=include_cls
                            )
                            
                            # Another memory cleanup
                            cleanup_memory()
                            
                        except RuntimeError as e:
                            if "out of memory" in str(e).lower():
                                oom_count += 1
                                print(f"   ðŸ’¥ OOM in batch {batch_idx}, reducing batch size")
                                
                                # Reduce batch size for next attempt
                                batch_size = max(batch_size // 2, 1)
                                
                                # Aggressive memory cleanup
                                cleanup_memory()
                                
                                if batch_size == 1:
                                    print(f"   âŒ OOM even with batch_size=1, skipping this batch")
                                    error_count += 1
                                    continue
                                else:
                                    continue
                            else:
                                raise e
                        
                        # Validate shapes match target tokens
                        assert clip_features.shape[1] == target_tokens, f"CLIP tokens: {clip_features.shape[1]} vs {target_tokens}"
                        assert eva_features.shape[1] == target_tokens, f"EVA tokens: {eva_features.shape[1]} vs {target_tokens}"
                        
                        # Store (already on CPU from extraction functions)
                        shard_clip_embeddings.append(clip_features)
                        shard_eva_embeddings.append(eva_features)
                        shard_captions.extend(captions)
                        shard_keys.extend(keys)
                        
                        total_samples += len(images)
                        
                        # Clear intermediate variables and cleanup
                        del clip_features, eva_features, images, captions, keys
                        cleanup_memory()
                        
                        # Progress update every 10 batches
                        if batch_idx % 10 == 0:
                            elapsed = time.time() - start_time
                            samples_per_sec = total_samples / elapsed if elapsed > 0 else 0
                            post_batch_memory = get_gpu_memory_info(device.index if hasattr(device, 'index') else None)
                            print(f"   Batch {batch_idx}: {total_samples} samples, {samples_per_sec:.1f} samples/sec, "
                                  f"{post_batch_memory.get('free_gb', 0):.1f} GB free")
                    
                    except Exception as e:
                        error_count += 1
                        print(f"   âš ï¸  Error processing batch {batch_idx}: {e}")
                        if error_count > batch_count * 0.5:  # If >50% of batches fail
                            raise Exception(f"Too many batch errors: {error_count}/{batch_count}")
                        continue
                
                print(f"   âœ… Processed {batch_count} batches, {error_count} errors, {oom_count} OOM events, {total_samples} samples")
                break  # Success, exit retry loop
                
            except Exception as e:
                print(f"   âŒ Error iterating through dataloader (attempt {attempt + 1}): {e}")
                if attempt == max_retries - 1:
                    return {
                        'shard_idx': shard_idx,
                        'total_samples': 0,
                        'success': False,
                        'error': f'Dataloader iteration failed after {max_retries} attempts: {e}',
                        'oom_events': oom_count
                    }
                continue
        
        except Exception as e:
            print(f"   âŒ Error in processing attempt {attempt + 1}: {e}")
            if attempt == max_retries - 1:
                return {
                    'shard_idx': shard_idx,
                    'total_samples': 0,
                    'success': False,
                    'error': f'All processing attempts failed: {e}'
                }
            continue
    
    # Consolidate embeddings for this shard
    if shard_clip_embeddings and total_samples > 0:
        print(f"   ðŸ”„ Consolidating {total_samples} embeddings...")
        
        try:
            final_clip = torch.cat(shard_clip_embeddings, dim=0)
            final_eva = torch.cat(shard_eva_embeddings, dim=0)
            
            # Final validation
            assert final_clip.shape[1] == target_tokens, f"Final CLIP shape: {final_clip.shape}"
            assert final_eva.shape[1] == target_tokens, f"Final EVA shape: {final_eva.shape}"
            assert final_clip.shape[2] == 1024, f"CLIP dim: {final_clip.shape[2]}"
            
            # Create shard data
            shard_data = {
                'clip_blip3o_embeddings': final_clip,
                'eva_blip3o_embeddings': final_eva,
                'captions': shard_captions,
                'keys': shard_keys,
                'total_samples': total_samples,
                'shard_idx': shard_idx,
                'source_tar': tar_file_path,
                'config': {
                    'clip_model': 'openai/clip-vit-large-patch14',
                    'eva_model': 'BAAI/EVA-CLIP-8B',
                    'clip_dim': 1024,
                    'eva_dim': final_eva.shape[2],
                    'tokens': target_tokens,
                    'include_cls': include_cls,
                    'mode': mode_suffix,
                    'extraction_method': 'unified_memory_optimized_extraction_v1',
                    'format_version': f'blip3o_{target_tokens}_tokens_{"cls_" if include_cls else ""}patch_unified_v1',
                    'extraction_time': time.time() - start_time,
                    'distributed': world_size > 1,
                    'rank': rank,
                    'world_size': world_size,
                    'memory_optimized': True,
                    'oom_events': oom_count,
                    'final_batch_size': batch_size,
                }
            }
            
            # Save shard data
            print(f"   ðŸ’¾ Saving shard {shard_idx} to persistent storage...")
            
            try:
                with open(shard_path, 'wb') as f:
                    pickle.dump(shard_data, f, protocol=pickle.HIGHEST_PROTOCOL)
                
                file_size_mb = shard_path.stat().st_size / (1024 * 1024)
                
                print(f"   âœ… Shard {shard_idx} completed:")
                print(f"      File: {shard_filename}")
                print(f"      Size: {file_size_mb:.1f} MB")
                print(f"      Samples: {total_samples}")
                print(f"      Mode: {mode_suffix} ({target_tokens} tokens)")
                print(f"      Time: {time.time() - start_time:.1f}s")
                print(f"      OOM events: {oom_count}")
                print(f"      Final batch size: {batch_size}")
                if world_size > 1:
                    print(f"      Distributed: rank {rank}/{world_size}")
                
                # Clear memory
                del shard_clip_embeddings, shard_eva_embeddings, final_clip, final_eva
                cleanup_memory()
                
                return {
                    'shard_idx': shard_idx,
                    'total_samples': total_samples,
                    'file_size_mb': file_size_mb,
                    'processing_time': time.time() - start_time,
                    'output_path': str(shard_path),
                    'success': True,
                    'mode': mode_suffix,
                    'tokens': target_tokens,
                    'include_cls': include_cls,
                    'rank': rank,
                    'world_size': world_size,
                    'memory_optimized': True,
                    'oom_events': oom_count,
                    'final_batch_size': batch_size,
                }
            
            except Exception as e:
                print(f"   âŒ Failed to save shard {shard_idx}: {e}")
                return {
                    'shard_idx': shard_idx,
                    'total_samples': total_samples,
                    'success': False,
                    'error': f'File save failed: {e}',
                    'oom_events': oom_count
                }
        
        except Exception as e:
            print(f"   âŒ Failed to consolidate embeddings for shard {shard_idx}: {e}")
            return {
                'shard_idx': shard_idx,
                'total_samples': 0,
                'success': False,
                'error': f'Consolidation failed: {e}',
                'oom_events': oom_count
            }
    
    else:
        print(f"   âŒ No embeddings extracted from shard {shard_idx}")
        return {
            'shard_idx': shard_idx,
            'total_samples': 0,
            'success': False,
            'error': 'No embeddings extracted - empty or corrupted TAR file',
            'oom_events': oom_count
        }

def process_tar_files_single_gpu(
    tar_files: List[str],
    output_dir: Path,
    working_dir: Path,
    batch_size: int = 16,
    include_cls: bool = True,
    target_tokens: int = 257,
):
    """Process TAR files on a single GPU"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # Load models
    clip_processor, clip_model, eva_processor, eva_model = load_models(device)
    
    processing_results = []
    total_samples_all = 0
    failed_shards = []
    oom_shards = []
    
    for shard_idx, tar_file in enumerate(tar_files):
        print(f"\n" + "="*60)
        print(f"PROCESSING SHARD {shard_idx + 1}/{len(tar_files)}")
        print(f"="*60)
        
        result = process_single_tar(
            tar_file_path=tar_file,
            shard_idx=shard_idx,
            clip_processor=clip_processor,
            clip_model=clip_model,
            eva_processor=eva_processor,
            eva_model=eva_model,
            device=device,
            output_dir=output_dir,
            working_dir=working_dir,
            batch_size=batch_size,
            include_cls=include_cls,
            target_tokens=target_tokens,
            world_size=1,
            rank=0
        )
        
        if result and result['success']:
            processing_results.append(result)
            total_samples_all += result['total_samples']
            print(f"âœ… Shard {shard_idx} successful: {result['total_samples']} samples")
        else:
            failed_shards.append(shard_idx)
            if result and result.get('oom_events', 0) > 0:
                oom_shards.append(shard_idx)
                print(f"ðŸ’¥ Shard {shard_idx} failed due to OOM: {result.get('error', 'Unknown error')}")
            else:
                print(f"âŒ Shard {shard_idx} failed: {result.get('error', 'Unknown error') if result else 'No result'}")
    
    return processing_results, total_samples_all, failed_shards, oom_shards

def process_tar_files_on_gpu(
    rank: int,
    world_size: int,
    tar_files: List[str],
    output_dir: Path,
    working_dir: Path,
    batch_size: int = 16,
    include_cls: bool = True,
    target_tokens: int = 257,
    master_port: str = "12355",
    max_retries: int = 3
):
    """Process assigned TAR files on a specific GPU with enhanced memory management"""
    
    # Setup distributed
    device = setup_distributed(rank, world_size, master_port)
    
    # Setup logging for this rank
    logging.basicConfig(
        level=logging.INFO if rank == 0 else logging.WARNING,
        format=f'[GPU {rank}] %(asctime)s - %(levelname)s - %(message)s'
    )
    logger = logging.getLogger(f'rank_{rank}')
    
    try:
        logger.info(f"Starting extraction on GPU {rank}")
        
        # Load models with memory optimization
        clip_processor, clip_model, eva_processor, eva_model = load_models(device)
        
        # Get assigned TAR files
        assigned_files = distribute_tar_files(tar_files, world_size, rank)
        
        if not assigned_files:
            logger.info(f"No files assigned to GPU {rank}")
            return
        
        logger.info(f"GPU {rank} will process {len(assigned_files)} files")
        
        # Process each assigned TAR file
        for local_idx, tar_file in enumerate(assigned_files):
            actual_shard_idx = tar_files.index(tar_file)  # Global shard index
            logger.info(f"Processing TAR file {local_idx + 1}/{len(assigned_files)}: {Path(tar_file).name} (global shard {actual_shard_idx})")
            
            result = process_single_tar(
                tar_file_path=tar_file,
                shard_idx=actual_shard_idx,
                clip_processor=clip_processor,
                clip_model=clip_model,
                eva_processor=eva_processor,
                eva_model=eva_model,
                device=device,
                output_dir=output_dir,
                working_dir=working_dir / f"gpu_{rank}",
                batch_size=batch_size,
                include_cls=include_cls,
                target_tokens=target_tokens,
                world_size=world_size,
                rank=rank,
                max_retries=max_retries
            )
            
            if result and result['success']:
                logger.info(f"âœ… Completed shard {actual_shard_idx}: {result['total_samples']} samples")
            else:
                logger.error(f"âŒ Failed shard {actual_shard_idx}: {result.get('error', 'Unknown error') if result else 'No result returned'}")
        
        # Synchronize all GPUs before consolidation
        if dist.is_initialized():
            dist.barrier()
            logger.info(f"GPU {rank} reached synchronization barrier")
        
    except Exception as e:
        logger.error(f"Critical error on GPU {rank}: {e}")
        import traceback
        traceback.print_exc()
        raise
    
    finally:
        # Cleanup models and memory
        try:
            del clip_model, eva_model, clip_processor, eva_processor
        except:
            pass
        
        cleanup_memory()
        cleanup_distributed()

def consolidate_gpu_outputs(
    output_dir: Path,
    world_size: int,
    mode_suffix: str,
    total_shards: int
) -> Dict[str, Any]:
    """Consolidate outputs from all GPUs"""
    
    print("ðŸ”„ Consolidating GPU outputs...")
    
    consolidation_results = {
        'consolidated_shards': 0,
        'total_samples': 0,
        'consolidation_errors': 0,
        'final_files': [],
        'failed_shards': [],
        'skipped_shards': [],
        'oom_shards': [],
        'unified_extraction': True,
    }
    
    for shard_idx in range(total_shards):
        # Look for GPU-specific files for this shard
        gpu_files = []
        shard_data_parts = []
        
        # Check if any GPU processed this shard
        shard_found = False
        oom_detected = False
        
        for rank in range(world_size):
            gpu_output_path = get_gpu_specific_output_path(
                output_dir, rank, shard_idx, mode_suffix
            )
            
            # Check for failure marker
            failure_marker = output_dir / f"failed_shard_{shard_idx:05d}_{mode_suffix}_gpu{rank}.txt"
            if failure_marker.exists():
                try:
                    with open(failure_marker, 'r') as f:
                        failure_content = f.read()
                        if "OOM events:" in failure_content:
                            oom_detected = True
                    print(f"Found failure marker for shard {shard_idx} from GPU {rank}")
                    if oom_detected:
                        consolidation_results['oom_shards'].append(shard_idx)
                    else:
                        consolidation_results['failed_shards'].append(shard_idx)
                except:
                    consolidation_results['failed_shards'].append(shard_idx)
                continue
            
            if gpu_output_path.exists():
                try:
                    with open(gpu_output_path, 'rb') as f:
                        shard_data = pickle.load(f)
                    shard_data_parts.append(shard_data)
                    gpu_files.append(gpu_output_path)
                    shard_found = True
                    print(f"Found shard {shard_idx} from GPU {rank}: {shard_data.get('total_samples', 0)} samples")
                except Exception as e:
                    print(f"âš ï¸ Error loading {gpu_output_path}: {e}")
                    consolidation_results['consolidation_errors'] += 1
        
        if not shard_found:
            print(f"âš ï¸ No valid data found for shard {shard_idx}, skipping")
            consolidation_results['skipped_shards'].append(shard_idx)
            continue
        
        # Consolidate if we have data from any GPU
        if shard_data_parts:
            try:
                # Use the first part as base
                consolidated_data = shard_data_parts[0].copy()
                
                # If multiple GPUs processed the same shard (shouldn't happen with proper distribution), merge
                if len(shard_data_parts) > 1:
                    print(f"Multiple GPU outputs for shard {shard_idx}, merging {len(shard_data_parts)} parts...")
                    
                    all_clip = [part['clip_blip3o_embeddings'] for part in shard_data_parts]
                    all_eva = [part['eva_blip3o_embeddings'] for part in shard_data_parts]
                    all_captions = []
                    all_keys = []
                    
                    for part in shard_data_parts:
                        all_captions.extend(part['captions'])
                        all_keys.extend(part['keys'])
                    
                    consolidated_data.update({
                        'clip_blip3o_embeddings': torch.cat(all_clip, dim=0),
                        'eva_blip3o_embeddings': torch.cat(all_eva, dim=0),
                        'captions': all_captions,
                        'keys': all_keys,
                        'total_samples': sum(part['total_samples'] for part in shard_data_parts)
                    })
                
                # Mark as using unified extraction
                if 'config' in consolidated_data:
                    consolidated_data['config']['unified_extraction'] = True
                    consolidated_data['config']['memory_optimized'] = True
                
                # Save consolidated shard
                final_output_path = output_dir / f"embeddings_shard_{shard_idx:05d}_{mode_suffix}.pkl"
                with open(final_output_path, 'wb') as f:
                    pickle.dump(consolidated_data, f, protocol=pickle.HIGHEST_PROTOCOL)
                
                consolidation_results['consolidated_shards'] += 1
                consolidation_results['total_samples'] += consolidated_data['total_samples']
                consolidation_results['final_files'].append(str(final_output_path))
                
                print(f"âœ… Consolidated shard {shard_idx}: {consolidated_data['total_samples']} samples")
                
                # Clean up GPU-specific files
                for gpu_file in gpu_files:
                    try:
                        gpu_file.unlink()
                        print(f"Cleaned up: {gpu_file.name}")
                    except Exception as e:
                        print(f"âš ï¸ Could not clean up {gpu_file}: {e}")
                
            except Exception as e:
                print(f"âŒ Error consolidating shard {shard_idx}: {e}")
                consolidation_results['consolidation_errors'] += 1
    
    print(f"âœ… Consolidation completed:")
    print(f"   Consolidated shards: {consolidation_results['consolidated_shards']}")
    print(f"   Failed shards: {len(consolidation_results['failed_shards'])}")
    print(f"   OOM shards: {len(consolidation_results['oom_shards'])}")
    print(f"   Skipped shards: {len(consolidation_results['skipped_shards'])}")
    print(f"   Total samples: {consolidation_results['total_samples']:,}")
    print(f"   Errors: {consolidation_results['consolidation_errors']}")
    
    return consolidation_results

def create_unified_manifest(
    output_dir: Path,
    consolidation_results: Dict[str, Any],
    world_size: int,
    include_cls: bool,
    target_tokens: int,
    processing_time: float,
    is_distributed: bool = False
):
    """Create manifest for unified extraction"""
    
    manifest_data = {
        'extraction_info': {
            'method': 'unified_memory_optimized_extraction_v1',
            'distributed': is_distributed,
            'world_size': world_size,
            'extraction_time_seconds': processing_time,
            'timestamp': time.time(),
            'memory_optimized': True,
            'unified_approach': True,
            'fixes_applied': [
                'Unified single/multi-GPU handling',
                'WebDataset version compatibility checks',
                'Multiple fallback approaches for dataset creation',
                'Better error handling with retry mechanism',
                'Direct TAR processing fallback when WebDataset fails',
                'Skip corrupted shards instead of failing completely',
                'Robust consolidation with failure tracking',
                'MEMORY OPTIMIZATION: Adaptive batch sizing',
                'MEMORY OPTIMIZATION: Enhanced memory cleanup',
                'MEMORY OPTIMIZATION: OOM detection and recovery',
                'MEMORY OPTIMIZATION: Model loading optimization'
            ]
        },
        'memory_optimization': {
            'adaptive_batch_sizing': True,
            'enhanced_memory_cleanup': True,
            'oom_detection': True,
            'model_loading_optimization': True,
            'memory_monitoring': True,
        },
        'consolidation_results': consolidation_results,
        'token_info': {
            'tokens_per_sample': target_tokens,
            'include_cls': include_cls,
            'cls_token_position': 0 if include_cls else None,
            'patch_tokens_range': [1, 257] if include_cls else [0, 256],
        },
        'format_version': f'blip3o_{target_tokens}_tokens_{"cls_" if include_cls else ""}patch_unified_v1',
        'total_shards': consolidation_results['consolidated_shards'],
        'total_samples': consolidation_results['total_samples'],
        'failed_shards': consolidation_results.get('failed_shards', []),
        'oom_shards': consolidation_results.get('oom_shards', []),
        'skipped_shards': consolidation_results.get('skipped_shards', []),
        'success_rate': consolidation_results['consolidated_shards'] / (
            consolidation_results['consolidated_shards'] + 
            len(consolidation_results.get('failed_shards', [])) + 
            len(consolidation_results.get('oom_shards', [])) + 
            len(consolidation_results.get('skipped_shards', []))
        ) if (consolidation_results['consolidated_shards'] + 
              len(consolidation_results.get('failed_shards', [])) + 
              len(consolidation_results.get('oom_shards', [])) + 
              len(consolidation_results.get('skipped_shards', []))) > 0 else 0,
        'compatibility': {
            'unified_single_multi_gpu': True,
            'webdataset_version_issues_fixed': True,
            'fallback_mechanisms_available': True,
            'direct_tar_processing': True,
            'distributed_processing_stable': True,
            'memory_pressure_handling': True,
            'oom_recovery': True,
        },
        'usage': {
            'training_command': f'python train_dit_distributed.py --chunked_embeddings_dir {output_dir} --distributed' if is_distributed else f'python train_dit.py --chunked_embeddings_dir {output_dir}',
        }
    }
    
    manifest_path = output_dir / "embeddings_manifest.json"
    with open(manifest_path, 'w') as f:
        json.dump(manifest_data, f, indent=2)
    
    print(f"âœ… Unified extraction manifest saved: {manifest_path}")
    return manifest_path

def main():
    """Main unified embedding extraction function"""
    
    parser = argparse.ArgumentParser(description="Unified BLIP3-o Embedding Extraction with Memory Optimization")
    parser.add_argument("--include_cls", action="store_true", default=False,
                       help="Include CLS token (257 tokens) or patches only (256 tokens)")
    parser.add_argument("--max_shards", type=int, default=None,
                       help="Maximum number of shards to process")
    parser.add_argument("--batch_size", type=int, default=12,
                       help="Initial batch size for processing (will be adapted)")
    parser.add_argument("--world_size", type=int, default=0,
                       help="Number of GPUs to use (0 = auto-detect, 1 = single GPU)")
    parser.add_argument("--master_port", type=str, default="12355",
                       help="Master port for distributed communication")
    parser.add_argument("--max_retries", type=int, default=3,
                       help="Maximum retries per shard")
    
    args = parser.parse_args()
    
    # Auto-detect GPU configuration
    if args.world_size == 0:
        if torch.cuda.is_available():
            available_gpus = torch.cuda.device_count()
            if available_gpus > 1:
                args.world_size = available_gpus
                print(f"ðŸ” Auto-detected {available_gpus} GPUs, enabling multi-GPU extraction")
            else:
                args.world_size = 1
                print(f"ðŸ” Single GPU detected, using single-GPU extraction")
        else:
            print("âŒ CUDA not available!")
            return 1
    
    # Setup
    target_tokens = 257 if args.include_cls else 256
    mode_name = "CLS+Patches" if args.include_cls else "Patches only"
    is_distributed = args.world_size > 1
    
    print("ðŸš€ UNIFIED BLIP3-o Embedding Extraction with Memory Optimization")
    print("=" * 80)
    print(f"Mode: {mode_name} ({target_tokens} tokens)")
    print(f"GPUs: {args.world_size} ({'Multi-GPU Distributed' if is_distributed else 'Single GPU'})")
    print(f"Initial batch size: {args.batch_size} (adaptive)")
    print(f"Max retries per shard: {args.max_retries}")
    print(f"Max shards: {args.max_shards if args.max_shards else 'All'}")
    print("ðŸ”§ UNIFIED EXTRACTION FEATURES:")
    print("  âœ… Automatic single/multi-GPU detection")
    print("  âœ… Memory optimization with adaptive batch sizing")
    print("  âœ… Enhanced error handling and recovery")
    print("  âœ… WebDataset compatibility with fallbacks")
    print("  âœ… OOM detection and graceful degradation")
    print("=" * 80)
    
    project_root = setup_paths()
    
    # Setup temp manager
    temp_manager = setup_temp_manager()
    
    if temp_manager:
        mode_suffix = "cls_patch" if args.include_cls else "patch_only"
        embeddings_dir = temp_manager.create_embeddings_subdirectory(f"{mode_suffix}_{target_tokens}_tokens")
        working_dir = temp_manager.get_working_dir()
        temp_manager.setup_model_cache()
        
        print(f"âœ… Using structured temp management")
        print(f"ðŸ“ Embeddings dir: {embeddings_dir}")
        print(f"ðŸ“ Working dir: {working_dir}")
    else:
        # Fallback
        mode_suffix = "cls_patch" if args.include_cls else "patch_only"
        embeddings_dir = Path(f"./embeddings_{mode_suffix}")
        working_dir = Path("./working")
        
        embeddings_dir.mkdir(parents=True, exist_ok=True)
        working_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"âš ï¸ Using fallback temp management")
        print(f"ðŸ“ Embeddings dir: {embeddings_dir}")
    
    # Find TAR files
    try:
        tar_files = find_data_files(temp_manager, max_shards=args.max_shards)
    except Exception as e:
        print(f"âŒ {e}")
        return 1
    
    print(f"ðŸ“¤ Output directory: {embeddings_dir}")
    print(f"ðŸ”„ Processing {len(tar_files)} TAR files using {'DISTRIBUTED' if is_distributed else 'SINGLE-GPU'} approach...")
    
    start_time = time.time()
    
    if is_distributed:
        # Multi-GPU distributed processing
        print(f"\nðŸš€ Starting multi-GPU distributed extraction with {args.world_size} GPUs...")
        
        try:
            # Use torch.multiprocessing.spawn for multi-GPU processing
            mp.spawn(
                process_tar_files_on_gpu,
                args=(
                    args.world_size,
                    tar_files,
                    embeddings_dir,
                    working_dir,
                    args.batch_size,
                    args.include_cls,
                    target_tokens,
                    args.master_port,
                    args.max_retries
                ),
                nprocs=args.world_size,
                join=True
            )
            
            print("âœ… All GPU processes completed")
            
            # Consolidate results
            print("\nðŸ”„ Consolidating results from all GPUs...")
            
            consolidation_results = consolidate_gpu_outputs(
                embeddings_dir,
                args.world_size,
                mode_suffix,
                len(tar_files)
            )
            
        except Exception as e:
            print(f"âŒ Distributed processing failed: {e}")
            import traceback
            traceback.print_exc()
            return 1
    
    else:
        # Single-GPU processing
        print(f"\nðŸ”„ Starting single-GPU extraction...")
        
        processing_results, total_samples_all, failed_shards, oom_shards = process_tar_files_single_gpu(
            tar_files,
            embeddings_dir,
            working_dir,
            args.batch_size,
            args.include_cls,
            target_tokens
        )
        
        # Create consolidation results format
        consolidation_results = {
            'consolidated_shards': len(processing_results),
            'total_samples': total_samples_all,
            'consolidation_errors': 0,
            'final_files': [result['output_path'] for result in processing_results],
            'failed_shards': failed_shards,
            'skipped_shards': [],
            'oom_shards': oom_shards,
            'unified_extraction': True,
        }
    
    # Create unified manifest
    processing_time = time.time() - start_time
    manifest_path = create_unified_manifest(
        embeddings_dir,
        consolidation_results,
        args.world_size,
        args.include_cls,
        target_tokens,
        processing_time,
        is_distributed
    )
    
    # Final results
    print("\n" + "=" * 80)
    print("ðŸŽ‰ UNIFIED EXTRACTION COMPLETED!")
    print("=" * 80)
    print(f"ðŸ“Š SUMMARY:")
    print(f"   Approach: {'Multi-GPU Distributed' if is_distributed else 'Single GPU'}")
    print(f"   GPUs used: {args.world_size}")
    print(f"   Mode: {mode_name} ({target_tokens} tokens)")
    print(f"   TAR files processed: {len(tar_files)}")
    print(f"   Successful shards: {consolidation_results['consolidated_shards']}")
    print(f"   Failed shards: {len(consolidation_results.get('failed_shards', []))}")
    print(f"   OOM shards: {len(consolidation_results.get('oom_shards', []))}")
    print(f"   Skipped shards: {len(consolidation_results.get('skipped_shards', []))}")
    print(f"   Total samples: {consolidation_results['total_samples']:,}")
    print(f"   Success rate: {consolidation_results.get('success_rate', 0)*100:.1f}%")
    print(f"   Processing time: {processing_time:.1f}s")
    if is_distributed:
        print(f"   Theoretical speedup: ~{args.world_size:.1f}x")
    print(f"   Embeddings location: {embeddings_dir}")
    print(f"   Manifest: {manifest_path}")
    print(f"   Memory optimization: âœ… ENABLED")
    
    # Show failed and OOM shards if any
    failed_shards = consolidation_results.get('failed_shards', [])
    oom_shards = consolidation_results.get('oom_shards', [])
    
    if oom_shards:
        print(f"\nðŸ’¥ OOM shards: {oom_shards}")
        print(f"   These shards failed due to out-of-memory issues")
        print(f"   Consider reducing batch size or processing individually")
    
    if failed_shards:
        print(f"\nâš ï¸ Failed shards: {failed_shards}")
        print(f"   These shards failed due to other processing errors")
    
    if consolidation_results['consolidated_shards'] > 0:
        print(f"\nðŸŽ‰ SUCCESS! {consolidation_results['consolidated_shards']} shards processed successfully!")
        print("Ready for BLIP3-o training!")
        print(f"\nNext steps:")
        if is_distributed:
            print(f"  Multi-GPU training:")
            print(f"  torchrun --nproc_per_node={args.world_size} train_dit_distributed.py \\")
            print(f"    --chunked_embeddings_dir {embeddings_dir} \\")
            print(f"    --distributed --world_size {args.world_size}")
        print(f"  Single-GPU training:")
        print(f"  python train_dit.py --chunked_embeddings_dir {embeddings_dir}")
    else:
        print(f"\nâŒ No shards processed successfully")
        print(f"Check the error logs and consider:")
        print(f"  â€¢ Reducing batch size further")
        print(f"  â€¢ Using fewer GPUs")
        print(f"  â€¢ Processing TAR files individually")
    
    print("=" * 80)
    print("ðŸ”§ UNIFIED EXTRACTION BENEFITS:")
    print("  âœ… Automatic single/multi-GPU detection and handling")
    print("  âœ… Memory optimization prevents OOM crashes")
    print("  âœ… Enhanced error handling improves stability")
    print("  âœ… WebDataset compatibility with fallback mechanisms")
    print("  âœ… Unified codebase reduces maintenance overhead")
    print("  âœ… Scalable from single-GPU research to multi-GPU production")
    print("  âœ… Ready for both single and distributed training")
    print("=" * 80)
    
    return 0 if consolidation_results['consolidated_shards'] > 0 else 1

if __name__ == "__main__":
    try:
        exit_code = main()
        sys.exit(exit_code)
    except Exception as e:
        print(f"âŒ Critical error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)