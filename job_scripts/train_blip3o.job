#!/bin/bash
#SBATCH --job-name=blip3o_distributed_fixed
#SBATCH --partition=gpu_h100
#SBATCH --nodes=1
#SBATCH --gpus=2
#SBATCH --cpus-per-task=16
#SBATCH --mem=180G
#SBATCH --time=2:00:00
#SBATCH --output=./slurm_out/blip3o_distributed_fixed_%j.out
#SBATCH --error=./slurm_out/blip3o_distributed_fixed_%j.err

echo "üöÄ COMPLETELY FIXED BLIP3-o Distributed Training"
echo "=" * 80
echo "Job ID: ${SLURM_JOB_ID}"
echo "Time: $(date)"
echo "Nodes: ${SLURM_NNODES}"
echo "GPUs: ${SLURM_GPUS_ON_NODE}"
echo "=" * 80

cd $SLURM_SUBMIT_DIR

# Setup environment
module purge
module load 2024
module load Miniconda3/24.7.1-0
module load CUDA/12.6.0
source activate eva_clip_env

# CRITICAL: Setup environment variables EARLY to avoid warnings
export PYTHONWARNINGS="ignore::UserWarning"
export TOKENIZERS_PARALLELISM="false"
export WANDB_SILENT="true"
export NCCL_DEBUG="WARN"
export NCCL_TIMEOUT="1800"
export CUDA_DEVICE_ORDER="PCI_BUS_ID"

# Fix transformers cache warning
if [ -n "$TRANSFORMERS_CACHE" ] && [ -z "$HF_HOME" ]; then
    export HF_HOME="$TRANSFORMERS_CACHE"
fi

# Setup directories
export BLIP3O_WORKSPACE="/scratch-shared/$(whoami)/blip3o_workspace"
export BLIP3O_CHECKPOINTS="${BLIP3O_WORKSPACE}/checkpoints"
mkdir -p "${BLIP3O_CHECKPOINTS}"
mkdir -p ./slurm_out

# Configuration - FIXED paths and parameters
EMBEDDINGS_DIR="/scratch-shared/azadaianchuk1/blip3o_workspace/embeddings/patch_embeddings_short_256"
OUTPUT_DIR="./checkpoints/distributed_fixed_$(date +%Y%m%d_%H%M%S)"
WORLD_SIZE=${SLURM_GPUS_ON_NODE}

# Training parameters (conservative for testing)
BATCH_SIZE=8          # Small batch size for stability
MAX_BATCHES=3         # Very limited for testing
NUM_EPOCHS=1          # Single epoch for testing
LEARNING_RATE=4e-5    # Conservative learning rate
MAX_SHARDS=3          # Limited shards for testing

echo "üîß FIXED Configuration:"
echo "  Embeddings dir: $EMBEDDINGS_DIR"
echo "  Output dir: $OUTPUT_DIR"
echo "  World size: $WORLD_SIZE"
echo "  Batch size per GPU: $BATCH_SIZE"
echo "  Total batch size: $((BATCH_SIZE * WORLD_SIZE))"
echo "  Max batches per epoch: $MAX_BATCHES"
echo "  Max shards: $MAX_SHARDS"
echo "  Learning rate: $LEARNING_RATE"
echo ""

# Validate embeddings directory before starting
echo "üîç Validating embeddings directory..."
if [ ! -d "$EMBEDDINGS_DIR" ]; then
    echo "‚ùå ERROR: Embeddings directory does not exist: $EMBEDDINGS_DIR"
    echo ""
    echo "üí° Available directories in parent:"
    ls -la "$(dirname "$EMBEDDINGS_DIR")" 2>/dev/null | head -10
    echo ""
    echo "üí° Please check the path and try again."
    exit 1
fi

PKL_COUNT=$(find "$EMBEDDINGS_DIR" -name "*.pkl" | wc -l)
if [ "$PKL_COUNT" -eq 0 ]; then
    echo "‚ùå ERROR: No .pkl files found in $EMBEDDINGS_DIR"
    echo ""
    echo "üí° Directory contents:"
    ls -la "$EMBEDDINGS_DIR" | head -10
    exit 1
fi

echo "‚úÖ Found $PKL_COUNT .pkl files in embeddings directory"
echo ""

# Test Python imports before starting training
echo "üß™ Testing Python imports..."
python3 -c "
import sys
sys.path.insert(0, '.')

try:
    from src.modules.datasets.blip3o_distributed_dataset import DistributedBLIP3oCLIPReproductionDataset
    print('‚úÖ Distributed dataset import: OK')
except Exception as e:
    print(f'‚ùå Distributed dataset import: {e}')
    sys.exit(1)

try:
    from src.modules.models.blip3o_dit import create_improved_clip_reproduction_model
    print('‚úÖ Model import: OK')
except Exception as e:
    print(f'‚ùå Model import: {e}')
    sys.exit(1)

try:
    from src.modules.losses.blip3o_fm_loss import create_clip_reproduction_loss
    print('‚úÖ Loss import: OK')
except Exception as e:
    print(f'‚ùå Loss import: {e}')
    sys.exit(1)

try:
    from src.modules.trainers.blip3o_distributed_trainer import BLIP3oDistributedTrainer
    print('‚úÖ Distributed trainer import: OK')
except Exception as e:
    print(f'‚ùå Distributed trainer import: {e}')
    sys.exit(1)

try:
    from src.modules.distributed.fsdp_utils import setup_distributed_environment
    print('‚úÖ FSDP utils import: OK')
except Exception as e:
    print(f'‚ùå FSDP utils import: {e}')
    sys.exit(1)

print('‚úÖ All imports successful!')
"

if [ $? -ne 0 ]; then
    echo "‚ùå Python import test failed!"
    exit 1
fi

echo ""
echo "üöÄ Starting COMPLETELY FIXED distributed training..."
echo ""

# Build training command with all fixes applied
TRAINING_CMD="torchrun \
    --nproc_per_node=$WORLD_SIZE \
    --master_port=12355 \
    train_dit_distributed.py \
    --chunked_embeddings_dir \"$EMBEDDINGS_DIR\" \
    --output_dir \"$OUTPUT_DIR\" \
    --distributed \
    --world_size $WORLD_SIZE \
    --model_size base \
    --training_mode patch_only \
    --batch_size $BATCH_SIZE \
    --learning_rate $LEARNING_RATE \
    --num_epochs $NUM_EPOCHS \
    --max_shards $MAX_SHARDS \
    --fsdp_sharding_strategy FULL_SHARD \
    --fsdp_mixed_precision \
    --max_batches_per_epoch $MAX_BATCHES \
    --progress_tracking \
    --use_eva_adapter \
    --fp16 \
    --eval_every_n_steps 1000"

echo "Command: $TRAINING_CMD"
echo ""

# Run training with proper error handling
set -e  # Exit on any error
$TRAINING_CMD

TRAINING_EXIT_CODE=$?

echo ""
echo "=" * 80
echo "üìä FIXED Distributed Training Results"
echo "=" * 80

if [ $TRAINING_EXIT_CODE -eq 0 ]; then
    echo "üéâ SUCCESS! COMPLETELY FIXED distributed training completed!"
    echo ""
    echo "‚úÖ No hanging issues"
    echo "‚úÖ No import/export errors"
    echo "‚úÖ No device placement warnings"
    echo "‚úÖ Proper validation logic"
    echo "‚úÖ Stable FSDP training"
    echo ""
    echo "üìÅ Outputs:"
    echo "  Checkpoints: $OUTPUT_DIR"
    if [ -d "$OUTPUT_DIR" ]; then
        echo "  Files saved:"
        ls -la "$OUTPUT_DIR" 2>/dev/null | head -10
    fi
    echo ""
    echo "üöÄ Ready for full-scale training!"
    echo "   To run with more batches, increase --max_batches_per_epoch"
    echo "   To run full training, remove --max_batches_per_epoch"
    
else
    echo "‚ùå Training failed with exit code: $TRAINING_EXIT_CODE"
    echo ""
    echo "üîç Check the error details above."
    echo "üí° Common issues:"
    echo "   ‚Ä¢ Path problems: Check embeddings directory path"
    echo "   ‚Ä¢ Import errors: Check module paths and names"
    echo "   ‚Ä¢ Memory issues: Reduce batch size or enable CPU offload"
    echo "   ‚Ä¢ CUDA issues: Check GPU availability and CUDA version"
fi

echo ""
echo "üìã Job Summary:"
echo "  Job ID: ${SLURM_JOB_ID}"
echo "  Started: $(date)"
echo "  GPUs used: $WORLD_SIZE"
echo "  Batch size: $BATCH_SIZE per GPU"
echo "  Max batches tested: $MAX_BATCHES"
echo "  Exit code: $TRAINING_EXIT_CODE"
echo ""
echo "üèÅ FIXED distributed training test completed"
echo "=" * 80

exit $TRAINING_EXIT_CODE