#!/bin/bash
#SBATCH --job-name=blip3o_no_norm
#SBATCH --partition=gpu_h100
#SBATCH --nodes=1
#SBATCH --gpus=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --time=12:00:00
#SBATCH --mem=64G
#SBATCH --output=./slurm_out/blip3o_no_norm_%j.out
#SBATCH --error=./slurm_out/blip3o_no_norm_%j.err

# =============================================================================
# BLIP3-o CLIP Reproduction Training WITHOUT Normalization
# Task: Reproduce CLIP embeddings [B, N, 1024] from EVA embeddings [B, N, 4096]
# Method: Rectified Flow Matching with BLIP3-o DiT (NO NORMALIZATION)
# =============================================================================

echo "ðŸš€ BLIP3-o CLIP Reproduction Training (NO NORMALIZATION)"
echo "=========================================="
echo "Job ID: ${SLURM_JOB_ID}"
echo "Node: $(hostname)"
echo "Time: $(date)"
echo "GPUs: $(nvidia-smi --query-gpu=name --format=csv,noheader,nounits | tr '\n' ', ')"
echo "=========================================="

cd $SLURM_SUBMIT_DIR

# Setup environment
module purge
module load 2024
module load Miniconda3/24.7.1-0
module load CUDA/12.6.0
source activate eva_clip_env

# âœ… Set WandB API key
export WANDB_API_KEY="0d9895af249ee18e4fa141e8a2350e0f4adb920f"
export WANDB_MODE="online"
export WANDB_CACHE_DIR="${TMPDIR}/wandb_cache"
mkdir -p "${WANDB_CACHE_DIR}"

# Verify WandB login
echo "ðŸ” Verifying WandB authentication..."
python -c "import wandb; wandb.login(key='$WANDB_API_KEY'); print('âœ… WandB login successful')"

# Configuration
EMBEDDINGS_DIR="/scratch-shared/azadaianchuk1/blip3o_workspace/embeddings/patch_only_256_tokens"
OUTPUT_DIR="./checkpoints/blip3o_no_norm_$(date +%Y%m%d_%H%M%S)"
TRAINING_MODE="patch_only"
MODEL_SIZE="base"

# Training hyperparameters
NUM_EPOCHS=15
BATCH_SIZE=128
LEARNING_RATE=1e-5
WEIGHT_DECAY=0.01
WARMUP_STEPS=100
MAX_GRAD_NORM=1.0

# Loss weights (same as before)
VELOCITY_WEIGHT=1.0
SEMANTIC_WEIGHT=0.5
COSINE_WEIGHT=0.2
CONSISTENCY_WEIGHT=0.3

# Simple scaling factor (data-independent)
SIMPLE_SCALE_FACTOR=1.0  # Can be adjusted if needed

# Evaluation parameters
EVAL_EVERY_N_STEPS=50
EVAL_NUM_SAMPLES=100
EVAL_INFERENCE_STEPS=50

# Data configuration
MAX_SHARDS=3

# Create output directory
mkdir -p "${OUTPUT_DIR}"
mkdir -p ./slurm_out

echo ""
echo "âš™ï¸ BLIP3-o Configuration (NO NORMALIZATION):"
echo "============================="
echo "Task: Reproduce CLIP embeddings from EVA embeddings"
echo "Method: BLIP3-o DiT WITHOUT CLIP normalization"
echo "Target: CLIP embeddings [B, N, 1024] (RAW)"
echo "Conditioning: EVA embeddings [B, N, 4096]"
echo ""
echo "Embeddings: $EMBEDDINGS_DIR"
echo "Output: $OUTPUT_DIR"
echo "Training mode: $TRAINING_MODE"
echo "Model size: $MODEL_SIZE"
echo "Max shards: $MAX_SHARDS"
echo "Simple scale factor: $SIMPLE_SCALE_FACTOR"
echo ""
echo "ðŸ“Š Training Hyperparameters:"
echo "  Epochs: $NUM_EPOCHS"
echo "  Batch size: $BATCH_SIZE"
echo "  Learning rate: $LEARNING_RATE"
echo "  Weight decay: $WEIGHT_DECAY"
echo "  Warmup steps: $WARMUP_STEPS"
echo "  Max grad norm: $MAX_GRAD_NORM"
echo ""
echo "ðŸŽ¯ Loss Weights:"
echo "  Velocity: $VELOCITY_WEIGHT"
echo "  Semantic: $SEMANTIC_WEIGHT"
echo "  Cosine: $COSINE_WEIGHT"
echo "  Consistency: $CONSISTENCY_WEIGHT"
echo ""
echo "ðŸ” Evaluation Configuration:"
echo "  Eval every: $EVAL_EVERY_N_STEPS steps"
echo "  Eval samples: $EVAL_NUM_SAMPLES"
echo "  Inference steps: $EVAL_INFERENCE_STEPS"
echo ""
echo "ðŸ—ï¸ Architecture Features:"
echo "  â€¢ 3D Rotary Position Embedding"
echo "  â€¢ Sandwich Normalization (RMSNorm)"
echo "  â€¢ Grouped-Query Attention"
echo "  â€¢ Rectified Flow Matching"
echo "  â€¢ Simple Linear Timestep Schedule"
echo "  â€¢ Heun's Integration Method"
echo "  â€¢ EVA-CLIP Adapter Layers"
echo ""
echo "ðŸ”‘ KEY CHANGE: NO CLIP NORMALIZATION"
echo "  â€¢ Works directly with raw CLIP embeddings"
echo "  â€¢ No dependency on training data statistics"
echo "  â€¢ Simplified training and evaluation pipeline"
echo "  â€¢ Optional simple data-independent scaling"
echo ""

# Verify embeddings exist
if [ ! -d "$EMBEDDINGS_DIR" ]; then
    echo "âŒ Embeddings directory not found: $EMBEDDINGS_DIR"
    echo "Available embeddings:"
    ls -la "/scratch-shared/azadaianchuk1/blip3o_workspace/embeddings/" 2>/dev/null || echo "No embeddings found"
    exit 1
fi

echo "âœ… Embeddings verified: $EMBEDDINGS_DIR"

# Check available shards
SHARD_COUNT=$(find "$EMBEDDINGS_DIR" -name "*.pkl" | wc -l)
echo "âœ… Found $SHARD_COUNT embedding shards"

if [ $SHARD_COUNT -eq 0 ]; then
    echo "âŒ No embedding shards found!"
    exit 1
fi

if [ $SHARD_COUNT -lt $MAX_SHARDS ]; then
    echo "âš ï¸ Only $SHARD_COUNT shards available (requested $MAX_SHARDS)"
    MAX_SHARDS=$SHARD_COUNT
    echo "   Adjusted to use $MAX_SHARDS shards"
fi

echo "âœ… Training script ready"

echo ""
echo "ðŸš€ Starting BLIP3-o Training (NO NORMALIZATION)..."
echo "===================================="
echo "ðŸŽ¯ Expected Behavior:"
echo "  âœ… No normalization-related crashes"
echo "  âœ… Direct work with raw CLIP embeddings"
echo "  âœ… Simplified training pipeline"
echo "  âœ… Non-zero gradients from first step"
echo "  âœ… Decreasing loss within first few epochs"
echo "  âœ… CLIP similarity during evaluation"
echo "  âœ… No dependency on training data statistics"
echo ""
echo "ðŸ—ï¸ Architecture:"
echo "  â€¢ BLIP3-o DiT with 3D RoPE and Sandwich Normalization"
echo "  â€¢ Rectified Flow Matching"
echo "  â€¢ Simple Linear Timestep Schedule"
echo "  â€¢ Heun's Integration Method"
echo "  â€¢ EVA [4096] â†’ CLIP [1024] mapping (NO NORMALIZATION)"
echo ""

# Launch BLIP3-o training WITHOUT normalization
python train_dit.py \
    --chunked_embeddings_dir "$EMBEDDINGS_DIR" \
    --output_dir "$OUTPUT_DIR" \
    --model_size "$MODEL_SIZE" \
    --training_mode "$TRAINING_MODE" \
    --learning_rate $LEARNING_RATE \
    --batch_size $BATCH_SIZE \
    --num_epochs $NUM_EPOCHS \
    --warmup_steps $WARMUP_STEPS \
    --weight_decay $WEIGHT_DECAY \
    --max_grad_norm $MAX_GRAD_NORM \
    --velocity_weight $VELOCITY_WEIGHT \
    --semantic_weight $SEMANTIC_WEIGHT \
    --cosine_weight $COSINE_WEIGHT \
    --consistency_weight $CONSISTENCY_WEIGHT \
    --simple_scale_factor $SIMPLE_SCALE_FACTOR \
    --eval_every_n_steps $EVAL_EVERY_N_STEPS \
    --eval_num_samples $EVAL_NUM_SAMPLES \
    --eval_inference_steps $EVAL_INFERENCE_STEPS \
    --max_shards $MAX_SHARDS \
    --use_eva_adapter \
    --use_heun_inference \
    --use_timestep_weighting \
    --fp16 \
    --use_wandb \
    --wandb_project "blip3o-clip-no-norm"

TRAINING_EXIT_CODE=$?

echo ""
echo "======================================"
echo "ðŸ“Š BLIP3-o Training Results (NO NORMALIZATION)"
echo "======================================"

if [ $TRAINING_EXIT_CODE -eq 0 ]; then
    echo "âœ… BLIP3-o training completed successfully (NO NORMALIZATION)!"
    
    echo ""
    echo "ðŸ“‹ Training Summary:"
    echo "=================="
    
    # Check for training results
    SUMMARY_FILE="$OUTPUT_DIR/training_summary.json"
    CONFIG_FILE="$OUTPUT_DIR/experiment_config.json"
    
    if [ -f "$SUMMARY_FILE" ]; then
        echo ""
        echo "ðŸ“Š Training Results:"
        echo "=================="
        
        # Extract key metrics using Python
        python -c "
import json
import sys
try:
    with open('$SUMMARY_FILE', 'r') as f:
        data = json.load(f)
    
    print(f'ðŸŽ¯ Best Loss: {data.get(\"best_loss\", float(\"inf\")):.6f}')
    print(f'ðŸŽ¯ Best CLIP Similarity: {data.get(\"best_eval_similarity\", 0):.4f}')
    print(f'ðŸ“Š Total Steps: {data.get(\"total_steps\", 0):,}')
    print(f'â±ï¸ Training Time: {data.get(\"total_time_seconds\", 0):.1f} seconds')
    
    # Final evaluation
    final_eval = data.get('final_eval', {})
    if final_eval:
        print(f'')
        print(f'ðŸ” Final Evaluation (RAW CLIP space):')
        clip_sim = final_eval.get('eval_clip_similarity', 0)
        high_qual = final_eval.get('eval_high_quality', 0) * 100
        very_high_qual = final_eval.get('eval_very_high_quality', 0) * 100
        
        print(f'   Overall CLIP Similarity: {clip_sim:.4f}')
        print(f'   High Quality (>0.7): {high_qual:.1f}%')
        print(f'   Very High Quality (>0.8): {very_high_qual:.1f}%')
        print(f'   Samples Evaluated: {final_eval.get(\"eval_samples\", 0):,}')
        
        # Assessment
        if clip_sim > 0.8:
            print(f'   ðŸŽ‰ EXCELLENT: Outstanding CLIP reproduction without normalization!')
        elif clip_sim > 0.6:
            print(f'   âœ… VERY GOOD: Strong CLIP reproduction with simplified approach!')
        elif clip_sim > 0.4:
            print(f'   âœ… GOOD: Solid CLIP reproduction without data-dependent stats!')
        elif clip_sim > 0.2:
            print(f'   ðŸ“ˆ FAIR: Decent reproduction, shows learning works!')
        else:
            print(f'   âš ï¸ NEEDS WORK: Try adjusting simple_scale_factor or loss weights')
    
except Exception as e:
    print(f'Could not parse training summary: {e}')
    # Try to show any available checkpoints
    import os
    checkpoints = [f for f in os.listdir('$OUTPUT_DIR') if f.endswith('.pt')]
    if checkpoints:
        print(f'Found {len(checkpoints)} checkpoint files')
        print(f'Latest: {max(checkpoints)}')
    sys.exit(1)
"
        
        echo ""
        echo "ðŸ“ Training artifacts saved to: $OUTPUT_DIR"
    else
        echo "âš ï¸ No training summary found, checking for any outputs..."
        echo "Directory contents:"
        ls -la "$OUTPUT_DIR" 2>/dev/null || echo "Output directory not found"
    fi
    
    echo ""
    echo "ðŸŽ¯ Next Steps:"
    echo "============"
    echo "1. Review training logs above for success indicators"
    echo "2. If results are good, run with more data:"
    echo "   â€¢ Increase MAX_SHARDS to use more training data" 
    echo "   â€¢ Increase NUM_EPOCHS for longer training"
    echo "   â€¢ Adjust BATCH_SIZE based on memory usage"
    echo "3. Run comprehensive evaluation on test set"
    echo "4. Try different simple_scale_factor values if needed"
    echo ""
    echo "ðŸ” SUCCESS INDICATORS:"
    echo "  âœ… Non-zero gradients throughout training"
    echo "  âœ… Decreasing loss trend"
    echo "  âœ… Increasing similarity metrics"
    echo "  âœ… Final CLIP similarity >0.2 (fair), >0.4 (good), >0.6 (excellent)"
    echo "  âœ… No normalization-related crashes"
    
    echo ""
    echo "âœ… SUCCESS: BLIP3-o training completed (NO NORMALIZATION)!"
    
else
    echo "âŒ FAILED: Training exit code $TRAINING_EXIT_CODE"
    echo ""
    echo "ðŸ’¡ Troubleshooting:"
    echo "  â€¢ Check log files in ./slurm_out/ for detailed error messages"
    echo "  â€¢ Verify all required Python files are present"
    echo "  â€¢ Check embeddings directory structure and file formats"
    echo "  â€¢ Monitor GPU memory usage with nvidia-smi"
    echo "  â€¢ Try reducing batch_size if out-of-memory errors"
    echo "  â€¢ Try different simple_scale_factor values"
    echo ""
    echo "ðŸ”§ Quick Recovery Options:"
    echo "  â€¢ Use smaller model: --model_size small or tiny"
    echo "  â€¢ Reduce batch size: --batch_size 4"
    echo "  â€¢ Use fewer shards: MAX_SHARDS=2"
    echo "  â€¢ Try scaling factor: --simple_scale_factor 0.1"
fi

echo ""
echo "ðŸ“Š GPU Resource Usage Summary:"
nvidia-smi --query-gpu=name,memory.total,memory.used,utilization.gpu --format=csv,noheader,nounits | \
    awk 'BEGIN{print "GPU | Total Memory | Used Memory | Utilization"} {printf "%s | %s MB | %s MB | %s%%\n", $1, $2, $3, $4}'

echo ""
echo "ðŸ Job completed at $(date)"
echo "Total job time: $(echo "scale=2; ($(date +%s) - $SECONDS) / 3600" | bc -l) hours"
echo ""
echo "ðŸ“š BLIP3-o SUMMARY (NO NORMALIZATION):"
echo "This job trains BLIP3-o DiT to reproduce CLIP embeddings from EVA embeddings"
echo "WITHOUT using data-dependent CLIP normalization. This approach:"
echo "â€¢ Works directly with raw CLIP embeddings"
echo "â€¢ Has no dependency on training data statistics"
echo "â€¢ Simplifies training and evaluation pipelines"
echo "â€¢ Eliminates normalization-related crashes"
echo "â€¢ Makes the model easier to deploy and understand"
echo "Success indicates the architecture works well without complex normalization."
echo "=========================================="

exit $TRAINING_EXIT_CODE

# =============================================================================
# EVALUATION JOB SCRIPT (NO NORMALIZATION)
# =============================================================================

# Separate evaluation job script
cat > evaluate_blip3o_no_norm.job << 'EOF'
#!/bin/bash
#SBATCH --job-name=blip3o_eval_no_norm
#SBATCH --partition=gpu_h100
#SBATCH --nodes=1
#SBATCH --gpus=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --time=2:00:00
#SBATCH --mem=32G
#SBATCH --output=./slurm_out/blip3o_eval_no_norm_%j.out
#SBATCH --error=./slurm_out/blip3o_eval_no_norm_%j.err

echo "ðŸ”¬ BLIP3-o MS-COCO Evaluation (NO NORMALIZATION)"
echo "Job ID: ${SLURM_JOB_ID} | Node: $(hostname) | Time: $(date)"

cd $SLURM_SUBMIT_DIR

# Setup environment
module purge
module load 2024
module load Miniconda3/24.7.1-0
module load CUDA/12.6.0
source activate eva_clip_env

# Configuration
STANDARD_EMBEDDINGS_BASE="/scratch-shared/azadaianchuk1/blip3o_workspace/embeddings"
COCO_EMBEDDINGS_FILE="${STANDARD_EMBEDDINGS_BASE}/coco_embeddings/coco_embeddings_consolidated.pkl"
MODEL_PATH="/home/azadaianchuk1/eva-clip-v6_2/checkpoints/blip3o_no_norm_latest"
OUTPUT_DIR="./coco_eval_no_norm_$(date +%Y%m%d_%H%M%S)"

# Parameters
MAX_SAMPLES=1000
BATCH_SIZE=8
NUM_INFERENCE_STEPS=50

# Parse arguments
[ -n "$1" ] && MODEL_PATH="$1"
[ -n "$2" ] && MAX_SAMPLES="$2"
[ -n "$3" ] && BATCH_SIZE="$3"

echo "Model: $MODEL_PATH"
echo "COCO Embeddings: $COCO_EMBEDDINGS_FILE"
echo "Max Samples: $MAX_SAMPLES | Batch Size: $BATCH_SIZE"
echo "ðŸ”‘ Evaluation WITHOUT normalization"

# Verify files exist
if [ ! -d "$MODEL_PATH" ]; then
    echo "âŒ Model not found: $MODEL_PATH"
    exit 1
fi

if [ ! -f "$COCO_EMBEDDINGS_FILE" ]; then
    echo "âŒ COCO embeddings not found: $COCO_EMBEDDINGS_FILE"
    exit 1
fi

# Check evaluation script
EVAL_SCRIPT="eval_blip3o_coco.py"
if [ ! -f "$EVAL_SCRIPT" ]; then
    echo "âŒ Evaluation script not found: $EVAL_SCRIPT"
    echo "Please ensure the modified evaluation script is available"
    exit 1
fi

mkdir -p "${OUTPUT_DIR}" ./slurm_out

echo "ðŸš€ Starting evaluation (NO NORMALIZATION)..."

# Run evaluation using the modified script (NO NORMALIZATION)
python $EVAL_SCRIPT \
    --model_path "$MODEL_PATH" \
    --coco_embeddings_file "$COCO_EMBEDDINGS_FILE" \
    --batch_size $BATCH_SIZE \
    --num_inference_steps $NUM_INFERENCE_STEPS \
    --max_samples $MAX_SAMPLES \
    --use_heun \
    --training_mode patch_only

EVAL_EXIT_CODE=$?

echo ""
echo "ðŸ“Š Evaluation Results (NO NORMALIZATION)"
echo "===================="

if [ $EVAL_EXIT_CODE -eq 0 ]; then
    echo "âœ… Evaluation completed successfully!"
    echo "ðŸ”‘ Evaluated in raw CLIP embedding space"
    echo "ðŸ“Š No normalization/denormalization applied"
    
else
    echo "âŒ Evaluation failed (exit code: $EVAL_EXIT_CODE)"
    echo "Check logs in ./slurm_out/ for details"
fi

echo "ðŸ Job completed at $(date)"
exit $EVAL_EXIT_CODE
EOF

echo "ðŸ“ Created evaluation job script: evaluate_blip3o_no_norm.job"