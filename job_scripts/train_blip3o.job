#!/bin/bash
#SBATCH --job-name=blip3o_fsdp_training
#SBATCH --partition=gpu
#SBATCH --nodes=1
#SBATCH --gpus=4
#SBATCH --cpus-per-task=16
#SBATCH --mem=180G
#SBATCH --time=12:00:00
#SBATCH --output=./slurm_out/blip3o_fsdp_training_%j.out
#SBATCH --error=./slurm_out/blip3o_fsdp_training_%j.err

# =============================================================================
# FIXED BLIP3-o Distributed Training with FSDP
# Multi-GPU training using Fully Sharded Data Parallel for memory efficiency
# FIXES: Resource requirements and compatibility issues
# =============================================================================

echo "üöÄ FIXED BLIP3-o Distributed Training with FSDP"
echo "=============================================="
echo "Job ID: ${SLURM_JOB_ID}"
echo "Node: $(hostname)"
echo "Time: $(date)"
echo "GPUs: ${SLURM_GPUS_ON_NODE}"
echo "CPUs per task: ${SLURM_CPUS_PER_TASK}"
echo "Memory: ${SLURM_MEM_PER_NODE}MB"
echo "=============================================="

cd $SLURM_SUBMIT_DIR

# Setup environment
module purge
module load 2024
module load Miniconda3/24.7.1-0
module load CUDA/12.6.0
source activate eva_clip_env

# Setup BLIP3-o workspace (temp directory management)
export SCRATCH_SHARED="/scratch-shared"
export BLIP3O_USER=$(whoami)
export BLIP3O_JOB_ID=${SLURM_JOB_ID}

# Set up structured directories for large-scale training
export BLIP3O_WORKSPACE="/scratch-shared/${BLIP3O_USER}/blip3o_workspace"
export BLIP3O_EMBEDDINGS="${BLIP3O_WORKSPACE}/embeddings"
export BLIP3O_CHECKPOINTS="${BLIP3O_WORKSPACE}/checkpoints"
export BLIP3O_DATASETS="${BLIP3O_WORKSPACE}/datasets"
export BLIP3O_LOGS="${BLIP3O_WORKSPACE}/logs"

# Job temp directory
export BLIP3O_JOB_TEMP="/scratch-local/${BLIP3O_USER}.${BLIP3O_JOB_ID}/blip3o_job_${BLIP3O_JOB_ID}"
export BLIP3O_CACHE="${BLIP3O_JOB_TEMP}/cache"

# Create directories
mkdir -p "${BLIP3O_WORKSPACE}"/{datasets,embeddings,checkpoints,logs,metadata}
mkdir -p "${BLIP3O_JOB_TEMP}"/{cache,working,temp_checkpoints}

# Model cache (redirected to job temp to avoid home quota)
export TORCH_HOME="${BLIP3O_CACHE}/torch"
export HF_HOME="${BLIP3O_CACHE}/huggingface"
export TRANSFORMERS_CACHE="${BLIP3O_CACHE}/transformers"
export WANDB_DIR="${BLIP3O_LOGS}/wandb"

# Create cache subdirectories
mkdir -p "${TORCH_HOME}" "${HF_HOME}" "${TRANSFORMERS_CACHE}" "${WANDB_DIR}"

echo "üóÇÔ∏è  BLIP3-o workspace ready:"
echo "   Persistent: $BLIP3O_WORKSPACE"
echo "   Job temp:   $BLIP3O_JOB_TEMP"
echo "   Checkpoints: $BLIP3O_CHECKPOINTS"

# Training configuration (can be overridden by command line arguments)
EMBEDDINGS_DIR="${BLIP3O_EMBEDDINGS}/patch_only_256_tokens"
OUTPUT_DIR="$BLIP3O_CHECKPOINTS/blip3o_fsdp_$(date +%Y%m%d_%H%M%S)"
TEMP_CHECKPOINT_DIR="$BLIP3O_CHECKPOINTS"

# Model and training parameters (FIXED: More conservative settings)
MODEL_SIZE="base"
TRAINING_MODE="patch_only"
BATCH_SIZE_PER_GPU=16  # REDUCED from 32 to avoid memory issues
LEARNING_RATE=4e-5
NUM_EPOCHS=8
WEIGHT_DECAY=0.04
MAX_GRAD_NORM=1.0

# FSDP configuration
FSDP_SHARDING_STRATEGY="FULL_SHARD"
FSDP_MIXED_PRECISION=true
FSDP_CPU_OFFLOAD=false

# Evaluation parameters
EVAL_EVERY_N_STEPS=50
EVAL_NUM_SAMPLES=100
EVAL_INFERENCE_STEPS=50

# Data configuration (FIXED: More conservative)
MAX_SHARDS=3  # REDUCED from 5
SIMPLE_SCALE_FACTOR=1.0

# Distributed configuration
WORLD_SIZE=${SLURM_GPUS_ON_NODE}
MASTER_PORT="12355"

# Parse command line arguments (override defaults)
while [[ $# -gt 0 ]]; do
    case $1 in
        --embeddings_dir)
            EMBEDDINGS_DIR="$2"
            shift 2
            ;;
        --model_size)
            MODEL_SIZE="$2"
            shift 2
            ;;
        --batch_size)
            BATCH_SIZE_PER_GPU="$2"
            shift 2
            ;;
        --learning_rate)
            LEARNING_RATE="$2"
            shift 2
            ;;
        --num_epochs)
            NUM_EPOCHS="$2"
            shift 2
            ;;
        --max_shards)
            MAX_SHARDS="$2"
            shift 2
            ;;
        --fsdp_cpu_offload)
            FSDP_CPU_OFFLOAD=true
            shift
            ;;
        *)
            echo "Unknown option: $1"
            shift
            ;;
    esac
done

# Create output directories
mkdir -p "${OUTPUT_DIR}"
mkdir -p ./slurm_out

echo ""
echo "‚öôÔ∏è FIXED Distributed Training Configuration:"
echo "=========================================="
echo "Model: $MODEL_SIZE"
echo "Training mode: $TRAINING_MODE"
echo "Embeddings: $EMBEDDINGS_DIR"
echo "Output: $OUTPUT_DIR"
echo "Temp checkpoints: $TEMP_CHECKPOINT_DIR"
echo ""
echo "üìä Training Parameters:"
echo "  Batch size per GPU: $BATCH_SIZE_PER_GPU (REDUCED for stability)"
echo "  Total batch size: $((BATCH_SIZE_PER_GPU * WORLD_SIZE))"
echo "  Learning rate: $LEARNING_RATE"
echo "  Epochs: $NUM_EPOCHS"
echo "  Max shards: ${MAX_SHARDS} (REDUCED for compatibility)"
echo "  Weight decay: $WEIGHT_DECAY"
echo "  Max grad norm: $MAX_GRAD_NORM"
echo ""
echo "‚ö° FSDP Configuration:"
echo "  World size: $WORLD_SIZE"
echo "  Sharding strategy: $FSDP_SHARDING_STRATEGY"
echo "  Mixed precision: $FSDP_MIXED_PRECISION"
echo "  CPU offload: $FSDP_CPU_OFFLOAD"
echo "  Master port: $MASTER_PORT"
echo ""
echo "üîß FIXES APPLIED:"
echo "  ‚úÖ Removed DistributedSampler from IterableDataset"
echo "  ‚úÖ Added missing import aliases"
echo "  ‚úÖ Reduced batch size and shard count for stability"
echo "  ‚úÖ More conservative resource requirements"
echo ""

# Verify embeddings exist
if [ ! -d "$EMBEDDINGS_DIR" ]; then
    echo "‚ùå Embeddings directory not found: $EMBEDDINGS_DIR"
    echo ""
    echo "Available embeddings directories:"
    ls -la "$BLIP3O_EMBEDDINGS/" 2>/dev/null || echo "No embeddings found"
    echo ""
    echo "üí° To extract embeddings:"
    echo "sbatch job_scripts/extract_embeddings_distributed.job"
    exit 1
fi

echo "‚úÖ Embeddings verified: $EMBEDDINGS_DIR"

# Check available embedding shards
SHARD_COUNT=$(find "$EMBEDDINGS_DIR" -name "*.pkl" | wc -l)
echo "‚úÖ Found $SHARD_COUNT embedding shards"

if [ $SHARD_COUNT -eq 0 ]; then
    echo "‚ùå No embedding shards found!"
    exit 1
fi

if [ $SHARD_COUNT -lt $MAX_SHARDS ]; then
    echo "‚ö†Ô∏è Only $SHARD_COUNT shards available (requested $MAX_SHARDS)"
    MAX_SHARDS=$SHARD_COUNT
    echo "   Adjusted to use $MAX_SHARDS shards"
fi

echo "‚úÖ FIXED distributed training script ready"

echo ""
echo "üöÄ Starting FIXED BLIP3-o Distributed Training (FSDP)..."
echo "======================================================"
echo "üéØ Expected Benefits:"
echo "  ‚úÖ ~3-4x training speedup with $WORLD_SIZE GPUs"
echo "  ‚úÖ Memory efficiency through parameter sharding"
echo "  ‚úÖ Scalable to larger models (up to 8B parameters)"
echo "  ‚úÖ No CLIP normalization complexity"
echo "  ‚úÖ Smart checkpoint management"
echo "  ‚úÖ Automatic gradient synchronization"
echo "  ‚úÖ FIXED: No sampler conflicts with IterableDataset"
echo ""
echo "üèóÔ∏è Architecture:"
echo "  ‚Ä¢ BLIP3-o DiT with FSDP parameter sharding"
echo "  ‚Ä¢ Rectified Flow Matching (no normalization)"
echo "  ‚Ä¢ EVA [4096] ‚Üí CLIP [1024] mapping"
echo "  ‚Ä¢ Multi-GPU distributed training"
echo ""

# Build training command with all parameters
TRAINING_CMD="torchrun \
    --nproc_per_node=$WORLD_SIZE \
    --master_port=$MASTER_PORT \
    train_dit_distributed.py \
    --chunked_embeddings_dir \"$EMBEDDINGS_DIR\" \
    --output_dir \"$OUTPUT_DIR\" \
    --temp_checkpoint_dir \"$TEMP_CHECKPOINT_DIR\" \
    --distributed \
    --world_size $WORLD_SIZE \
    --model_size $MODEL_SIZE \
    --training_mode $TRAINING_MODE \
    --batch_size $BATCH_SIZE_PER_GPU \
    --learning_rate $LEARNING_RATE \
    --num_epochs $NUM_EPOCHS \
    --weight_decay $WEIGHT_DECAY \
    --max_grad_norm $MAX_GRAD_NORM \
    --velocity_weight 1.0 \
    --simple_scale_factor $SIMPLE_SCALE_FACTOR \
    --eval_every_n_steps $EVAL_EVERY_N_STEPS \
    --eval_num_samples $EVAL_NUM_SAMPLES \
    --eval_inference_steps $EVAL_INFERENCE_STEPS \
    --max_shards $MAX_SHARDS \
    --fsdp_sharding_strategy $FSDP_SHARDING_STRATEGY \
    --use_eva_adapter \
    --use_heun_inference \
    --use_timestep_weighting \
    --fp16 \
    --keep_local_checkpoints 3 \
    --save_to_temp_every_n_steps 1000"

# Add FSDP options
if [ "$FSDP_MIXED_PRECISION" = true ]; then
    TRAINING_CMD="$TRAINING_CMD --fsdp_mixed_precision"
fi

if [ "$FSDP_CPU_OFFLOAD" = true ]; then
    TRAINING_CMD="$TRAINING_CMD --fsdp_cpu_offload"
fi

echo "Executing FIXED distributed training command:"
echo "$TRAINING_CMD"
echo ""

# Launch distributed training
eval $TRAINING_CMD

TRAINING_EXIT_CODE=$?

echo ""
echo "=============================================="
echo "üìä FIXED BLIP3-o Distributed Training Results"
echo "=============================================="

if [ $TRAINING_EXIT_CODE -eq 0 ]; then
    echo "‚úÖ BLIP3-o distributed training completed successfully!"
    
    echo ""
    echo "üìã Training Summary:"
    echo "=================="
    
    # Check for training results
    SUMMARY_FILE="$OUTPUT_DIR/training_summary.json"
    CONFIG_FILE="$OUTPUT_DIR/distributed_experiment_config.json"
    
    if [ -f "$SUMMARY_FILE" ]; then
        echo ""
        echo "üìä Training Results:"
        echo "=================="
        
        # Extract key metrics using Python
        python -c "
import json
import sys
try:
    with open('$SUMMARY_FILE', 'r') as f:
        data = json.load(f)
    
    print(f'üéØ Best Loss: {data.get(\"best_loss\", float(\"inf\")):.6f}')
    print(f'üéØ Best CLIP Similarity: {data.get(\"best_eval_similarity\", 0):.4f}')
    print(f'üìä Total Steps: {data.get(\"total_steps\", 0):,}')
    print(f'‚è±Ô∏è Training Time: {data.get(\"total_time_seconds\", 0):.1f} seconds')
    print(f'‚ö° World Size: {data.get(\"world_size\", 1)} GPUs')
    print(f'üîß FSDP Enabled: {data.get(\"fsdp_enabled\", False)}')
    
    # Final evaluation
    final_eval = data.get('final_eval', {})
    if final_eval:
        print(f'')
        print(f'üîç Final Distributed Evaluation:')
        clip_sim = final_eval.get('eval_clip_similarity', 0)
        high_qual = final_eval.get('eval_high_quality', 0) * 100
        very_high_qual = final_eval.get('eval_very_high_quality', 0) * 100
        
        print(f'   Overall CLIP Similarity: {clip_sim:.4f}')
        print(f'   High Quality (>0.7): {high_qual:.1f}%')
        print(f'   Very High Quality (>0.8): {very_high_qual:.1f}%')
        print(f'   Samples Evaluated: {final_eval.get(\"eval_samples\", 0):,}')
        print(f'   Distributed: {final_eval.get(\"eval_distributed\", False)}')
        
        # Assessment
        if clip_sim > 0.8:
            print(f'   üéâ EXCELLENT: Outstanding distributed training results!')
        elif clip_sim > 0.6:
            print(f'   ‚úÖ VERY GOOD: Strong CLIP reproduction with FSDP!')
        elif clip_sim > 0.4:
            print(f'   ‚úÖ GOOD: Solid distributed training performance!')
        elif clip_sim > 0.2:
            print(f'   üìà FAIR: Distributed training shows learning!')
        else:
            print(f'   ‚ö†Ô∏è NEEDS WORK: Check distributed training configuration')
    
except Exception as e:
    print(f'Could not parse training summary: {e}')
    # Try to show any available checkpoints
    import os
    checkpoints = [f for f in os.listdir('$OUTPUT_DIR') if f.endswith('.pt')]
    if checkpoints:
        print(f'Found {len(checkpoints)} checkpoint files')
        print(f'Latest: {max(checkpoints)}')
    sys.exit(1)
"
        
        echo ""
        echo "üìÅ Training artifacts saved to:"
        echo "   Local: $OUTPUT_DIR"
        echo "   Temp:  $TEMP_CHECKPOINT_DIR"
    else
        echo "‚ö†Ô∏è No training summary found, checking for any outputs..."
        echo "Directory contents:"
        ls -la "$OUTPUT_DIR" 2>/dev/null || echo "Output directory not found"
    fi
    
    # Show distributed training benefits achieved
    echo ""
    echo "‚ö° FIXED Distributed Training Benefits Achieved:"
    echo "  ‚Ä¢ Multi-GPU parameter sharding with FSDP"
    echo "  ‚Ä¢ Memory efficiency for larger model scaling"
    echo "  ‚Ä¢ Automatic gradient synchronization"
    echo "  ‚Ä¢ Smart checkpoint management"
    echo "  ‚Ä¢ No CLIP normalization complexity"
    echo "  ‚Ä¢ Scalable training infrastructure"
    echo "  ‚Ä¢ FIXED: Compatible with IterableDataset"
    
else
    echo "‚ùå FAILED: Distributed training exit code $TRAINING_EXIT_CODE"
    echo ""
    echo "üí° Troubleshooting:"
    echo "  ‚Ä¢ Check log files in ./slurm_out/ for detailed error messages"
    echo "  ‚Ä¢ Verify all fixed files are in place"
    echo "  ‚Ä¢ Check embeddings directory structure and file formats"
    echo "  ‚Ä¢ Monitor GPU memory usage across all ranks"
    echo "  ‚Ä¢ All compatibility fixes have been applied"
    echo ""
    echo "üîß Quick Recovery Options:"
    echo "  ‚Ä¢ Use smaller model: --model_size small"
    echo "  ‚Ä¢ Reduce batch size further: --batch_size 8"
    echo "  ‚Ä¢ Enable CPU offload: --fsdp_cpu_offload"
    echo "  ‚Ä¢ Use fewer shards: --max_shards 2"
fi

echo ""
echo "üìä Multi-GPU Resource Usage Summary:"
nvidia-smi --query-gpu=name,memory.total,memory.used,utilization.gpu --format=csv,noheader,nounits | \
    awk 'BEGIN{print "GPU | Total Memory | Used Memory | Utilization"} {printf "%s | %s MB | %s MB | %s%%\n", $1, $2, $3, $4}'

echo ""
echo "üèÅ Job completed at $(date)"
echo "Total job time: $(echo "scale=2; ($(date +%s) - $SECONDS) / 3600" | bc -l) hours"
echo ""
echo "üìö FIXED FSDP DISTRIBUTED TRAINING SUMMARY:"
echo "This job trained BLIP3-o DiT using FSDP (Fully Sharded Data Parallel)"
echo "to reproduce CLIP embeddings from EVA embeddings across multiple GPUs."
echo ""
echo "üîß FIXES APPLIED:"
echo "‚Ä¢ Removed DistributedSampler incompatibility with IterableDataset"
echo "‚Ä¢ Added missing import aliases for distributed compatibility"
echo "‚Ä¢ More conservative resource requirements"
echo "‚Ä¢ Reduced batch size and shard count for stability"
echo "‚Ä¢ Fixed all import and compatibility issues"
echo ""
echo "Key achievements:"
echo "‚Ä¢ Multi-GPU parameter sharding for memory efficiency"
echo "‚Ä¢ No dependency on CLIP normalization"
echo "‚Ä¢ Smart checkpoint management with temp directory support"
echo "‚Ä¢ Scalable training infrastructure for larger models"
echo "‚Ä¢ Automatic gradient synchronization across ranks"
echo "=============================================="

exit $TRAINING_EXIT_CODE