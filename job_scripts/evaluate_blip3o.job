#!/bin/bash
#SBATCH --job-name=blip3o_coco_eval
#SBATCH --partition=gpu_h100
#SBATCH --nodes=1
#SBATCH --gpus=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --time=2:00:00
#SBATCH --mem=32G
#SBATCH --output=./slurm_out/blip3o_coco_eval_%j.out
#SBATCH --error=./slurm_out/blip3o_coco_eval_%j.err

echo "ğŸ”¬ BLIP3-o MS-COCO Evaluation"
echo "Job ID: ${SLURM_JOB_ID} | Node: $(hostname) | Time: $(date)"

cd $SLURM_SUBMIT_DIR

# Setup environment
module purge
module load 2024
module load Miniconda3/24.7.1-0
module load CUDA/12.6.0
source activate eva_clip_env

# Configuration
STANDARD_EMBEDDINGS_BASE="/scratch-shared/azadaianchuk1/blip3o_workspace/embeddings"
COCO_EMBEDDINGS_FILE="${STANDARD_EMBEDDINGS_BASE}/coco_embeddings/coco_embeddings_consolidated.pkl"
MODEL_PATH="/home/azadaianchuk1/eva-clip-v2_3//checkpoints/blip3o_no_norm_20250803_162340"
OUTPUT_DIR="./coco_eval_$(date +%Y%m%d_%H%M%S)"

# Parameters
MAX_SAMPLES=1000
BATCH_SIZE=64
NUM_INFERENCE_STEPS=50

# Parse arguments
[ -n "$1" ] && MODEL_PATH="$1"
[ -n "$2" ] && MAX_SAMPLES="$2"
[ -n "$3" ] && BATCH_SIZE="$3"

echo "Model: $MODEL_PATH"
echo "COCO Embeddings: $COCO_EMBEDDINGS_FILE"
echo "Max Samples: $MAX_SAMPLES | Batch Size: $BATCH_SIZE"

# Verify files exist
if [ ! -d "$MODEL_PATH" ]; then
    echo "âŒ Model not found: $MODEL_PATH"
    exit 1
fi

if [ ! -f "$COCO_EMBEDDINGS_FILE" ]; then
    echo "âŒ COCO embeddings not found: $COCO_EMBEDDINGS_FILE"
    exit 1
fi

# Check evaluation script
EVAL_SCRIPT="eval_blip3o_coco.py"
if [ ! -f "$EVAL_SCRIPT" ]; then
    echo "âŒ Evaluation script not found: $EVAL_SCRIPT"
    echo "Please ensure the fixed evaluation script is available"
    exit 1
fi

mkdir -p "${OUTPUT_DIR}" ./slurm_out

echo "ğŸš€ Starting evaluation..."

# Run evaluation using the FIXED script
python $EVAL_SCRIPT \
    --model_path "$MODEL_PATH" \
    --coco_embeddings_file "$COCO_EMBEDDINGS_FILE" \
    --batch_size $BATCH_SIZE \
    --num_inference_steps $NUM_INFERENCE_STEPS \
    --max_samples $MAX_SAMPLES \
    --use_heun \
    --training_mode patch_only
# -output_dir "$OUTPUT_DIR" \
EVAL_EXIT_CODE=$?

echo ""
echo "ğŸ“Š Evaluation Results"
echo "===================="

if [ $EVAL_EXIT_CODE -eq 0 ]; then
    echo "âœ… Evaluation completed successfully!"
    
    # Show results if available
    RESULTS_FILE="$OUTPUT_DIR/coco_evaluation_results.json"
    if [ -f "$RESULTS_FILE" ]; then
        python -c "
import json
try:
    with open('$RESULTS_FILE', 'r') as f:
        metrics = json.load(f)
    
    similarity = metrics.get('eval_clip_similarity', 0)
    samples = metrics.get('eval_samples', 0)
    time_sec = metrics.get('evaluation_time_seconds', 0)
    
    print(f'CLIP Similarity: {similarity:.4f}')
    print(f'Samples: {samples:,}')
    print(f'Time: {time_sec:.1f}s')
    
    if similarity > 0.8:
        print('ğŸ‰ EXCELLENT performance!')
    elif similarity > 0.6:
        print('âœ… GOOD performance!')
    elif similarity > 0.4:
        print('ğŸ“ˆ FAIR performance')
    else:
        print('âš ï¸ Needs investigation')
        
except Exception as e:
    print(f'Could not parse results: {e}')
"
    fi
    
    echo "Results saved to: $OUTPUT_DIR"
    
else
    echo "âŒ Evaluation failed (exit code: $EVAL_EXIT_CODE)"
    echo "Check logs in ./slurm_out/ for details"
fi

echo "ğŸ Job completed at $(date)"
exit $EVAL_EXIT_CODE