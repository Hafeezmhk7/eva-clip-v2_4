#!/bin/bash
#SBATCH --job-name=blip3o_multi_gpu
#SBATCH --partition=gpu_a100
#SBATCH --nodes=2
#SBATCH --gpus-per-node=4
#SBATCH --ntasks-per-node=4    # 4 tasks per node
#SBATCH --cpus-per-task=8
#SBATCH --mem=300G
#SBATCH --time=2:00:00

echo "üîç SLURM Environment Debug:"
echo "SLURM_PROCID: $SLURM_PROCID"
echo "SLURM_LOCALID: $SLURM_LOCALID"  
echo "SLURM_NTASKS: $SLURM_NTASKS"
echo "SLURM_GPUS_ON_NODE: $SLURM_GPUS_ON_NODE"
echo "SLURM_STEP_NODELIST: $SLURM_STEP_NODELIST"

cd $SLURM_SUBMIT_DIR

# Setup environment
module purge
module load 2024
module load Miniconda3/24.7.1-0
module load CUDA/12.6.0
source activate eva_clip_env

# Use SLURM_PROCID as rank instead of mp.spawn
export RANK=$SLURM_PROCID
export LOCAL_RANK=$SLURM_LOCALID  
export WORLD_SIZE=$SLURM_NTASKS

# Use srun to distribute across nodes
srun --ntasks=8 python -c "
import os
import sys
sys.path.insert(0, '/gpfs/home4/azadaianchuk1/eva-clip-v2_4')

# Import our function
from src.modules.extract_embeddings_unified import process_tar_files_on_gpu, find_data_files, setup_temp_manager
from pathlib import Path

# Get parameters from environment
rank = int(os.environ['SLURM_PROCID'])
world_size = int(os.environ['SLURM_NTASKS'])

# Setup directories
temp_manager = setup_temp_manager()
if temp_manager:
    embeddings_dir = temp_manager.create_embeddings_subdirectory('patch_only_256_tokens_stable')
    temp_manager.setup_model_cache()
else:
    embeddings_dir = Path('./embeddings_patch_only_stable')
    embeddings_dir.mkdir(parents=True, exist_ok=True)

# Find TAR files
tar_files = find_data_files(temp_manager, max_shards=20)

print(f'[Rank {rank}] Starting with {len(tar_files)} TAR files')

# Run single process (no mp.spawn)
process_tar_files_on_gpu(
    rank=rank,
    world_size=world_size, 
    tar_files=tar_files,
    output_dir=embeddings_dir,
    batch_size=128,
    include_cls=False,
    target_tokens=256,
    master_port='12361'
)
"