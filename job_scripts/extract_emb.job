#!/bin/bash
#SBATCH --job-name=blip3o_multi_gpu
#SBATCH --partition=gpu_a100
#SBATCH --nodes=2
#SBATCH --gpus-per-node=4
#SBATCH --ntasks-per-node=4    # 4 tasks per node
#SBATCH --cpus-per-task=8
#SBATCH --mem=300G
#SBATCH --time=2:00:00
#SBATCH --output=./slurm_out/blip3o_multi_nodes_%j.out
#SBATCH --error=./slurm_out/blip3o_multi_nodes_%j.err

echo "üîç SLURM Environment Debug:"
echo "SLURM_PROCID: $SLURM_PROCID"
echo "SLURM_LOCALID: $SLURM_LOCALID"  
echo "SLURM_NTASKS: $SLURM_NTASKS"
echo "SLURM_GPUS_ON_NODE: $SLURM_GPUS_ON_NODE"
echo "SLURM_STEP_NODELIST: $SLURM_STEP_NODELIST"
echo "SLURM_NODEID: $SLURM_NODEID"

cd $SLURM_SUBMIT_DIR

# Setup environment
module purge
module load 2024
module load Miniconda3/24.7.1-0
module load CUDA/12.6.0
source activate eva_clip_env

# Setup workspace directories
export SCRATCH_SHARED="/scratch-shared"
export BLIP3O_USER=$(whoami)
export BLIP3O_JOB_ID=${SLURM_JOB_ID}

export BLIP3O_WORKSPACE="/scratch-shared/${BLIP3O_USER}/blip3o_workspace"
export BLIP3O_EMBEDDINGS="${BLIP3O_WORKSPACE}/embeddings"
export BLIP3O_CHECKPOINTS="${BLIP3O_WORKSPACE}/checkpoints"
export BLIP3O_DATASETS="${BLIP3O_WORKSPACE}/datasets"
export BLIP3O_LOGS="${BLIP3O_WORKSPACE}/logs"

export BLIP3O_JOB_TEMP="/scratch-local/${BLIP3O_USER}.${BLIP3O_JOB_ID}/blip3o_job_${BLIP3O_JOB_ID}"
export BLIP3O_CACHE="${BLIP3O_JOB_TEMP}/cache"

# Create directories (only on rank 0 to avoid race conditions)
if [ "${SLURM_PROCID:-0}" -eq 0 ]; then
    mkdir -p "${BLIP3O_WORKSPACE}"/{datasets,embeddings,checkpoints,logs,metadata}
    mkdir -p "${BLIP3O_JOB_TEMP}"/{cache,working,temp_checkpoints}
    mkdir -p ./slurm_out
fi

# Model cache (redirected to job temp)
export TORCH_HOME="${BLIP3O_CACHE}/torch"
export HF_HOME="${BLIP3O_CACHE}/huggingface"
export TRANSFORMERS_CACHE="${BLIP3O_CACHE}/transformers"
export WANDB_DIR="${BLIP3O_LOGS}/wandb"
export HUGGINGFACE_HUB_CACHE="${BLIP3O_CACHE}/huggingface/hub"
export HF_DATASETS_CACHE="${BLIP3O_CACHE}/datasets"

mkdir -p "${TORCH_HOME}" "${HF_HOME}" "${TRANSFORMERS_CACHE}" "${WANDB_DIR}" "${HUGGINGFACE_HUB_CACHE}" "${HF_DATASETS_CACHE}"

echo "Workspace: $BLIP3O_WORKSPACE"
echo "Job temp: $BLIP3O_JOB_TEMP"

# Configuration defaults
WORLD_SIZE=${SLURM_NTASKS:-8}
MASTER_PORT="12361"
BATCH_SIZE=128
MAX_SHARDS=20
INCLUDE_CLS=false

# Parse command line arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        --include_cls)
            INCLUDE_CLS=true
            shift
            ;;
        --batch_size)
            BATCH_SIZE="$2"
            shift 2
            ;;
        --max_shards)
            MAX_SHARDS="$2"
            shift 2
            ;;
        --master_port)
            MASTER_PORT="$2"
            shift 2
            ;;
        *)
            echo "Unknown option: $1"
            shift
            ;;
    esac
done

# Determine extraction mode
if [ "$INCLUDE_CLS" = true ]; then
    TARGET_TOKENS=257
    MODE_NAME="CLS+Patches"
    MODE_SUFFIX="cls_patch"
else
    TARGET_TOKENS=256
    MODE_NAME="Patches only"
    MODE_SUFFIX="patch_only"
fi

OUTPUT_DIR="${BLIP3O_EMBEDDINGS}/${MODE_SUFFIX}_${TARGET_TOKENS}_tokens"

# Create output directory (only on rank 0)
if [ "${SLURM_PROCID:-0}" -eq 0 ]; then
    mkdir -p "${OUTPUT_DIR}"
    
    echo "Mode: $MODE_NAME ($TARGET_TOKENS tokens)"
    echo "Output: $OUTPUT_DIR"
    echo "GPUs: $WORLD_SIZE"
    echo "Batch size per GPU: $BATCH_SIZE"
    echo "Max shards: $MAX_SHARDS"
    echo "Master port: $MASTER_PORT"
    
    # Validate extraction script
    EXTRACTION_SCRIPT="src/modules/extract_embeddings_unified.py"
    if [ ! -f "$EXTRACTION_SCRIPT" ]; then
        echo "‚ùå Extraction script not found: $EXTRACTION_SCRIPT"
        exit 1
    fi
    
    # Check for dataset files
    if [ ! -d "$BLIP3O_DATASETS" ]; then
        echo "‚ùå Datasets directory not found: $BLIP3O_DATASETS"
        echo "To download datasets:"
        echo "  python src/data_hand/download_data.py --shards 0 1 2 3 4 5 6 7 8 9"
        exit 1
    fi
    
    TAR_COUNT=$(find "$BLIP3O_DATASETS" -name "*.tar" | wc -l)
    echo "Found $TAR_COUNT TAR files in datasets directory"
    
    if [ $TAR_COUNT -eq 0 ]; then
        echo "‚ùå No TAR files found in datasets directory!"
        exit 1
    fi
    
    if [ $TAR_COUNT -lt $MAX_SHARDS ]; then
        MAX_SHARDS=$TAR_COUNT
        echo "Adjusted to use $MAX_SHARDS shards"
    fi
    
    # Display GPU information
    echo "GPU Information:"
    nvidia-smi --query-gpu=index,name,memory.total,memory.free --format=csv,noheader
    
    echo "Starting extraction at: $(date)"
fi

# Synchronize all processes before starting
sleep 2

# Export environment variables for srun
export MASTER_PORT
export MAX_SHARDS
export BATCH_SIZE
export INCLUDE_CLS
export TARGET_TOKENS
export OUTPUT_DIR

# Use srun to distribute across nodes with proper environment setup
srun --ntasks=${WORLD_SIZE} bash -c '
# Set up distributed environment variables from SLURM
export RANK=${SLURM_PROCID}
export LOCAL_RANK=${SLURM_LOCALID}
export WORLD_SIZE=${SLURM_NTASKS}

# Get master node address
MASTER_NODE=$(scontrol show hostnames ${SLURM_JOB_NODELIST} | head -n 1)
export MASTER_ADDR=${MASTER_NODE}

echo "[Rank ${RANK}] Node: $(hostname), Local Rank: ${LOCAL_RANK}, GPU Assignment: ${LOCAL_RANK}"
echo "[Rank ${RANK}] Master: ${MASTER_ADDR}:${MASTER_PORT}, World Size: ${WORLD_SIZE}"

# Run the Python extraction with FIXED parameters
python -c "
import os
import sys
import torch
sys.path.insert(0, '\''/gpfs/home4/azadaianchuk1/eva-clip-v2_4'\'')

# Import our function
from src.modules.extract_embeddings_unified import process_tar_files_on_gpu, find_data_files, setup_temp_manager
from pathlib import Path

# Get parameters from environment
rank = int(os.environ['\''RANK'\''])
local_rank = int(os.environ['\''LOCAL_RANK'\''])
world_size = int(os.environ['\''WORLD_SIZE'\''])
master_port = os.environ['\''MASTER_PORT'\'']
batch_size = int(os.environ['\''BATCH_SIZE'\''])
max_shards = int(os.environ['\''MAX_SHARDS'\''])
include_cls = os.environ['\''INCLUDE_CLS'\''] == '\''true'\''
target_tokens = int(os.environ['\''TARGET_TOKENS'\''])
output_dir = Path(os.environ['\''OUTPUT_DIR'\''])

print(f'\''[Rank {rank}] Starting extraction with local GPU {local_rank}...'\'')

# CRITICAL: Set CUDA device BEFORE any model loading
torch.cuda.set_device(local_rank)
print(f'\''[Rank {rank}] Set CUDA device to {local_rank}'\'')

# Setup directories
temp_manager = setup_temp_manager()
if temp_manager:
    embeddings_dir = temp_manager.create_embeddings_subdirectory(f'\''patch_only_{target_tokens}_tokens_stable'\'')
    temp_manager.setup_model_cache()
else:
    embeddings_dir = output_dir
    if rank == 0:
        embeddings_dir.mkdir(parents=True, exist_ok=True)

# Find TAR files
tar_files = find_data_files(temp_manager, max_shards=max_shards)

print(f'\''[Rank {rank}] Found {len(tar_files)} TAR files, using GPU {local_rank}'\'')

# Run distributed extraction with ONLY supported parameters
try:
    process_tar_files_on_gpu(
        rank=rank,
        world_size=world_size, 
        tar_files=tar_files,
        output_dir=embeddings_dir,
        batch_size=batch_size,
        include_cls=include_cls,
        target_tokens=target_tokens,
        master_port=master_port
    )
    print(f'\''[Rank {rank}] Extraction completed successfully'\'')
except Exception as e:
    print(f'\''[Rank {rank}] Extraction failed: {e}'\'')
    import traceback
    traceback.print_exc()
    sys.exit(1)
"
'

EXTRACTION_EXIT_CODE=$?

echo "Completed at: $(date)"

# Post-processing and reporting (only on rank 0)
if [ "${SLURM_PROCID:-0}" -eq 0 ]; then
    if [ $EXTRACTION_EXIT_CODE -eq 0 ]; then
        echo "‚úÖ Multi-GPU embedding extraction completed successfully!"
        
        # Check for output files
        CONSOLIDATED_FILES=$(find "$OUTPUT_DIR" -name "embeddings_shard_*_${MODE_SUFFIX}.pkl" | wc -l)
        if [ $CONSOLIDATED_FILES -gt 0 ]; then
            echo "‚úÖ Found $CONSOLIDATED_FILES consolidated embedding files"
            
            TOTAL_SIZE=$(du -sh "$OUTPUT_DIR" 2>/dev/null | cut -f1)
            echo "‚úÖ Total size: $TOTAL_SIZE"
            
            SUCCESS_RATE=$(echo "scale=1; $CONSOLIDATED_FILES * 100 / $MAX_SHARDS" | bc -l 2>/dev/null || echo "N/A")
            echo "‚úÖ Success rate: $SUCCESS_RATE% ($CONSOLIDATED_FILES/$MAX_SHARDS shards)"
            
            # Check manifest
            MANIFEST_FILE="${OUTPUT_DIR}/embeddings_manifest.json"
            if [ -f "$MANIFEST_FILE" ]; then
                echo "‚úÖ Manifest file: $MANIFEST_FILE"
            fi
            
            echo "üéâ EXTRACTION SUCCESS!"
            echo "Next steps:"
            echo "torchrun --nproc_per_node=4 --nnodes=2 train_dit_distributed.py \\"
            echo "  --chunked_embeddings_dir $OUTPUT_DIR \\"
            echo "  --distributed --world_size $WORLD_SIZE"
            
        else
            echo "‚ö†Ô∏è No consolidated files found"
            echo "Files in output directory:"
            ls -la "$OUTPUT_DIR" 2>/dev/null | head -10
        fi
        
    else
        echo "‚ùå Extraction failed with exit code $EXTRACTION_EXIT_CODE"
        
        if [ -f "${OUTPUT_DIR}/error_log.txt" ]; then
            echo "Error log (last 20 lines):"
            tail -20 "${OUTPUT_DIR}/error_log.txt"
        fi
        
        echo "Troubleshooting:"
        echo "1. Check TAR files integrity"
        echo "2. Try with smaller batch size: --batch_size 64"
        echo "3. Try with fewer GPUs/nodes"
        echo "4. Check GPU memory usage during extraction"
    fi
    
    echo "Final GPU Memory Status:"
    nvidia-smi --query-gpu=index,name,memory.used,memory.free --format=csv,noheader
    
    echo "Job completed at $(date)"
    TOTAL_SECONDS=$SECONDS
    TOTAL_HOURS=$(echo "scale=2; $TOTAL_SECONDS / 3600" | bc -l)
    echo "Total job time: ${TOTAL_HOURS} hours"
fi

exit $EXTRACTION_EXIT_CODE