#!/bin/bash
#SBATCH --job-name=blip3o_memory_optimized_extraction
#SBATCH --partition=gpu_h100
#SBATCH --nodes=1
#SBATCH --gpus=3
#SBATCH --ntasks=3
#SBATCH --cpus-per-task=8
#SBATCH --mem=120G
#SBATCH --time=8:00:00
#SBATCH --output=./slurm_out/blip3o_memory_optimized_extraction_%j.out
#SBATCH --error=./slurm_out/blip3o_memory_optimized_extraction_%j.err

# =============================================================================
# MEMORY-OPTIMIZED BLIP3-o Multi-GPU Embedding Extraction
# Fixes OOM issues with adaptive batch sizing and enhanced memory management
# =============================================================================

echo "üöÄ MEMORY-OPTIMIZED BLIP3-o Multi-GPU Embedding Extraction"
echo "=========================================="
echo "Job ID: ${SLURM_JOB_ID}"
echo "Node: $(hostname)"
echo "Time: $(date)"
echo "GPUs: ${SLURM_GPUS_ON_NODE}"
echo "CPUs per task: ${SLURM_CPUS_PER_TASK}"
echo "Memory: ${SLURM_MEM_PER_NODE}MB"
echo "=========================================="

cd $SLURM_SUBMIT_DIR

# Setup environment
module purge
module load 2024
module load Miniconda3/24.7.1-0
module load CUDA/12.6.0
source activate eva_clip_env

# Setup BLIP3-o workspace
export SCRATCH_SHARED="/scratch-shared"
export BLIP3O_USER=$(whoami)
export BLIP3O_JOB_ID=${SLURM_JOB_ID}

# Set up structured directories
export BLIP3O_WORKSPACE="/scratch-shared/${BLIP3O_USER}/blip3o_workspace"
export BLIP3O_EMBEDDINGS="${BLIP3O_WORKSPACE}/embeddings"
export BLIP3O_CHECKPOINTS="${BLIP3O_WORKSPACE}/checkpoints"
export BLIP3O_DATASETS="${BLIP3O_WORKSPACE}/datasets"
export BLIP3O_LOGS="${BLIP3O_WORKSPACE}/logs"

# Job temp directory
export BLIP3O_JOB_TEMP="/scratch-local/${BLIP3O_USER}.${BLIP3O_JOB_ID}/blip3o_job_${BLIP3O_JOB_ID}"
export BLIP3O_CACHE="${BLIP3O_JOB_TEMP}/cache"

# Create directories
mkdir -p "${BLIP3O_WORKSPACE}"/{datasets,embeddings,checkpoints,logs,metadata}
mkdir -p "${BLIP3O_JOB_TEMP}"/{cache,working,temp_checkpoints}

# Model cache (redirected to job temp to avoid home quota)
export TORCH_HOME="${BLIP3O_CACHE}/torch"
export HF_HOME="${BLIP3O_CACHE}/huggingface"
export TRANSFORMERS_CACHE="${BLIP3O_CACHE}/transformers"

# Create cache subdirectories
mkdir -p "${TORCH_HOME}" "${HF_HOME}" "${TRANSFORMERS_CACHE}"

echo "üóÇÔ∏è  BLIP3-o workspace ready:"
echo "   Persistent: $BLIP3O_WORKSPACE"
echo "   Job temp:   $BLIP3O_JOB_TEMP"
echo "   Datasets:   $BLIP3O_DATASETS"
echo "   Embeddings: $BLIP3O_EMBEDDINGS"

# Memory optimization configuration
WORLD_SIZE=${SLURM_GPUS_ON_NODE}
MASTER_PORT="12356"
INITIAL_BATCH_SIZE=8  # Reduced from 32 to avoid OOM
INCLUDE_CLS=false
MAX_SHARDS=50
MAX_RETRIES=3

# Parse command line arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        --include_cls)
            INCLUDE_CLS=true
            shift
            ;;
        --batch_size)
            INITIAL_BATCH_SIZE="$2"
            shift 2
            ;;
        --max_shards)
            MAX_SHARDS="$2"
            shift 2
            ;;
        --world_size)
            WORLD_SIZE="$2"
            shift 2
            ;;
        --max_retries)
            MAX_RETRIES="$2"
            shift 2
            ;;
        *)
            echo "Unknown option: $1"
            shift
            ;;
    esac
done

# Determine extraction mode
if [ "$INCLUDE_CLS" = true ]; then
    TARGET_TOKENS=257
    MODE_NAME="CLS+Patches"
    MODE_SUFFIX="cls_patch"
else
    TARGET_TOKENS=256
    MODE_NAME="Patches only"
    MODE_SUFFIX="patch_only"
fi

# Set output directory
OUTPUT_DIR="${BLIP3O_EMBEDDINGS}/${MODE_SUFFIX}_${TARGET_TOKENS}_tokens"

# Create output directories
mkdir -p "${OUTPUT_DIR}"
mkdir -p ./slurm_out

echo ""
echo "‚öôÔ∏è MEMORY-OPTIMIZED Multi-GPU Extraction Configuration:"
echo "====================================="
echo "Mode: $MODE_NAME ($TARGET_TOKENS tokens)"
echo "Output: $OUTPUT_DIR"
echo "GPUs: $WORLD_SIZE"
echo "Initial batch size per GPU: $INITIAL_BATCH_SIZE (adaptive)"
echo "Max shards: $MAX_SHARDS"
echo "Max retries per shard: $MAX_RETRIES"
echo "Master port: $MASTER_PORT"
echo ""
echo "üîß MEMORY OPTIMIZATION FEATURES:"
echo "  ‚Ä¢ Adaptive batch sizing based on GPU memory availability"
echo "  ‚Ä¢ Enhanced memory cleanup and monitoring"
echo "  ‚Ä¢ OOM detection and recovery mechanisms"
echo "  ‚Ä¢ Progressive batch size reduction on memory pressure"
echo "  ‚Ä¢ Optimized model loading for memory efficiency"
echo "  ‚Ä¢ Better error handling for memory-related issues"
echo ""
echo "üéØ Expected Benefits:"
echo "  ‚Ä¢ Eliminates OOM crashes through adaptive processing"
echo "  ‚Ä¢ Better GPU memory utilization"
echo "  ‚Ä¢ Graceful handling of memory pressure"
echo "  ‚Ä¢ Robust processing even with limited GPU memory"
echo "  ‚Ä¢ ~${WORLD_SIZE}x extraction speedup with stability"
echo ""

# Check for dataset files
if [ ! -d "$BLIP3O_DATASETS" ]; then
    echo "‚ùå Datasets directory not found: $BLIP3O_DATASETS"
    echo ""
    echo "üí° To download datasets:"
    echo "  python src/data_hand/download_data.py --shards 0 1 2 3 4 5 6 7 8 9"
    echo "  # Make sure to place downloaded TAR files in: $BLIP3O_DATASETS"
    exit 1
fi

# Count available TAR files
TAR_COUNT=$(find "$BLIP3O_DATASETS" -name "*.tar" | wc -l)
echo "‚úÖ Found $TAR_COUNT TAR files in datasets directory"

if [ $TAR_COUNT -eq 0 ]; then
    echo "‚ùå No TAR files found in datasets directory!"
    echo "Available files:"
    ls -la "$BLIP3O_DATASETS/" 2>/dev/null || echo "Directory empty"
    exit 1
fi

if [ $TAR_COUNT -lt $MAX_SHARDS ]; then
    echo "‚ö†Ô∏è Only $TAR_COUNT TAR files available (requested $MAX_SHARDS)"
    MAX_SHARDS=$TAR_COUNT
    echo "   Adjusted to use $MAX_SHARDS shards"
fi

# Display GPU information
echo ""
echo "üñ•Ô∏è GPU Information:"
nvidia-smi --query-gpu=index,name,memory.total,memory.free --format=csv,noheader,nounits | \
    awk '{printf "  GPU %s: %s (%s MB total, %s MB free)\n", $1, $2, $3, $4}'

echo ""
echo "üöÄ Starting MEMORY-OPTIMIZED Multi-GPU Embedding Extraction..."
echo "=============================================="
echo "üéØ Memory Optimization Strategy:"
echo "  ‚Ä¢ Start with conservative batch size: $INITIAL_BATCH_SIZE"
echo "  ‚Ä¢ Monitor GPU memory usage during processing"
echo "  ‚Ä¢ Reduce batch size automatically if OOM detected"
echo "  ‚Ä¢ Enhanced memory cleanup between TAR files"
echo "  ‚Ä¢ Graceful degradation instead of crashes"
echo ""
echo "üìä Processing Details:"
echo "  ‚Ä¢ TAR files: $TAR_COUNT available, using $MAX_SHARDS"
echo "  ‚Ä¢ Files per GPU: ~$((MAX_SHARDS / WORLD_SIZE))"
echo "  ‚Ä¢ Adaptive batch processing per GPU memory availability"
echo "  ‚Ä¢ Output format: ${MODE_SUFFIX}_${TARGET_TOKENS}_tokens"
echo "  ‚Ä¢ Memory monitoring and OOM recovery enabled"
echo ""

# Build extraction command with memory optimization
EXTRACTION_CMD="python src/modules/extract_embeddings_distributed.py \
    --world_size $WORLD_SIZE \
    --master_port $MASTER_PORT \
    --batch_size $INITIAL_BATCH_SIZE \
    --max_shards $MAX_SHARDS \
    --max_retries $MAX_RETRIES"

# Add CLS token option if specified
if [ "$INCLUDE_CLS" = true ]; then
    EXTRACTION_CMD="$EXTRACTION_CMD --include_cls"
fi

echo "Executing MEMORY-OPTIMIZED multi-GPU extraction command:"
echo "$EXTRACTION_CMD"
echo ""

# Launch memory-optimized distributed extraction
eval $EXTRACTION_CMD

EXTRACTION_EXIT_CODE=$?

echo ""
echo "=============================================="
echo "üìä MEMORY-OPTIMIZED Multi-GPU Extraction Results"
echo "=============================================="

if [ $EXTRACTION_EXIT_CODE -eq 0 ]; then
    echo "‚úÖ MEMORY-OPTIMIZED Multi-GPU embedding extraction completed successfully!"
    
    echo ""
    echo "üìã Extraction Summary:"
    echo "===================="
    
    # Check for output files
    echo ""
    echo "üìã Generated Files:"
    echo "=================="
    
    # Check for consolidated files
    CONSOLIDATED_FILES=$(find "$OUTPUT_DIR" -name "embeddings_shard_*_${MODE_SUFFIX}.pkl" | wc -l)
    if [ $CONSOLIDATED_FILES -gt 0 ]; then
        echo "‚úÖ Found $CONSOLIDATED_FILES consolidated embedding files"
        
        # Calculate total size
        TOTAL_SIZE=$(du -sh "$OUTPUT_DIR" 2>/dev/null | cut -f1)
        echo "‚úÖ Total size: $TOTAL_SIZE"
        
        # Check manifest
        MANIFEST_FILE="${OUTPUT_DIR}/embeddings_manifest.json"
        if [ -f "$MANIFEST_FILE" ]; then
            echo "‚úÖ Manifest file: $MANIFEST_FILE"
            
            # Extract summary from manifest
            python -c "
import json
import sys
try:
    with open('$MANIFEST_FILE', 'r') as f:
        manifest = json.load(f)
    
    extraction_info = manifest.get('extraction_info', {})
    consolidation = manifest.get('consolidation_results', {})
    memory_opt = manifest.get('memory_optimization', {})
    
    print(f'üìä MEMORY-OPTIMIZED Multi-GPU Extraction Summary:')
    print(f'   GPUs used: {extraction_info.get(\"world_size\", \"unknown\")}')
    print(f'   Extraction time: {extraction_info.get(\"extraction_time_seconds\", 0):.1f}s')
    print(f'   Total shards attempted: $(echo '$MAX_SHARDS')')
    print(f'   Successful shards: {consolidation.get(\"consolidated_shards\", \"unknown\")}')
    print(f'   Failed shards: {len(consolidation.get(\"failed_shards\", []))}')
    print(f'   OOM shards: {len(consolidation.get(\"oom_shards\", []))}')
    print(f'   Total samples: {consolidation.get(\"total_samples\", \"unknown\"):,}')
    print(f'   Success rate: {consolidation.get(\"success_rate\", 0)*100:.1f}%')
    print(f'   Mode: $MODE_NAME ($TARGET_TOKENS tokens)')
    print(f'   Memory optimization: {\"‚úÖ ENABLED\" if memory_opt.get(\"memory_optimized\") else \"‚ùå DISABLED\"}')
    
    # Show failed and OOM shards if any
    failed_shards = consolidation.get('failed_shards', [])
    oom_shards = consolidation.get('oom_shards', [])
    
    if oom_shards:
        print(f'   üí• OOM shards: {oom_shards}')
        print(f'      These failed due to memory limitations')
    
    if failed_shards:
        print(f'   ‚ö†Ô∏è Failed shards: {failed_shards}')
        print(f'      These failed due to other processing errors')
    
    if len(oom_shards) == 0 and len(failed_shards) == 0:
        print(f'   üéâ NO MEMORY ISSUES: All shards processed successfully!')
    
    # Calculate speedup
    world_size = extraction_info.get('world_size', 1)
    extraction_time = extraction_info.get('extraction_time_seconds', 0)
    if world_size > 1 and extraction_time > 0:
        theoretical_speedup = world_size
        print(f'   Theoretical speedup: ~{theoretical_speedup:.1f}x')
    
except Exception as e:
    print(f'Could not parse manifest: {e}')
    sys.exit(1)
"
        fi
        
        echo ""
        echo "üéâ SUCCESS: MEMORY-OPTIMIZED Multi-GPU extraction completed!"
        echo ""
        echo "üí° Next step - Start distributed training:"
        echo "sbatch job_scripts/train_blip3o_fsdp.job --embeddings_dir $OUTPUT_DIR"
        echo ""
        echo "Or manually:"
        echo "torchrun --nproc_per_node=$WORLD_SIZE train_dit_distributed.py \\"
        echo "  --chunked_embeddings_dir $OUTPUT_DIR \\"
        echo "  --output_dir ./checkpoints \\"
        echo "  --distributed --world_size $WORLD_SIZE"
        
    else
        echo "‚ö†Ô∏è No consolidated files found - check for partial results"
        echo "Files in output directory:"
        ls -la "$OUTPUT_DIR" 2>/dev/null | head -10
        
        # Check for GPU-specific files that might need manual consolidation
        GPU_FILES=$(find "$OUTPUT_DIR" -name "*_gpu*.pkl" | wc -l)
        if [ $GPU_FILES -gt 0 ]; then
            echo "Found $GPU_FILES GPU-specific files that may need consolidation"
        fi
    fi
    
    # Show memory optimization benefits achieved
    echo ""
    echo "‚ö° MEMORY-OPTIMIZED Multi-GPU Extraction Benefits Achieved:"
    echo "  ‚Ä¢ Eliminated OOM crashes through adaptive batch sizing"
    echo "  ‚Ä¢ Enhanced memory monitoring and cleanup"
    echo "  ‚Ä¢ Graceful handling of memory pressure situations"
    echo "  ‚Ä¢ Progressive batch size reduction when needed"  
    echo "  ‚Ä¢ Optimized model loading for memory efficiency"
    echo "  ‚Ä¢ Better error recovery from memory-related issues"
    echo "  ‚Ä¢ Parallel TAR file processing across $WORLD_SIZE GPUs"
    echo "  ‚Ä¢ ~${WORLD_SIZE}x theoretical speedup with stability"
    echo "  ‚Ä¢ Ready for distributed training"
    
else
    echo "‚ùå FAILED: MEMORY-OPTIMIZED Multi-GPU extraction exit code $EXTRACTION_EXIT_CODE"
    echo ""
    echo "üí° Memory-Optimized Troubleshooting:"
    echo "  ‚Ä¢ Check log files in ./slurm_out/ for detailed error messages"
    echo "  ‚Ä¢ Look for OOM events and memory pressure indicators"
    echo "  ‚Ä¢ Verify TAR files are accessible and not corrupted"
    echo "  ‚Ä¢ Check GPU memory availability: nvidia-smi"
    echo "  ‚Ä¢ Consider further reducing initial batch size"
    echo "  ‚Ä¢ Try processing fewer shards: --max_shards 10"
    echo ""
    echo "üîß Memory-Specific Recovery options:"
    echo "  ‚Ä¢ Use smaller initial batch size: --batch_size 4"
    echo "  ‚Ä¢ Process fewer files at once: --max_shards 5"
    echo "  ‚Ä¢ Use fewer GPUs to increase memory per GPU"
    echo "  ‚Ä¢ Check for memory leaks in logs"
    echo "  ‚Ä¢ Try single-GPU extraction as fallback"
    echo ""
    echo "üîç Memory Diagnostics:"
    echo "  ‚Ä¢ GPU memory: nvidia-smi"
    echo "  ‚Ä¢ System memory: free -h"
    echo "  ‚Ä¢ Disk space: df -h"
    echo "  ‚Ä¢ Process memory: ps aux --sort=-%mem | head"
    
    # Show any intermediate files that might exist
    echo ""
    echo "üîç Checking for intermediate files..."
    INTERMEDIATE_FILES=$(find "$OUTPUT_DIR" -name "*_gpu*.pkl" 2>/dev/null | wc -l)
    if [ $INTERMEDIATE_FILES -gt 0 ]; then
        echo "Found $INTERMEDIATE_FILES intermediate files:"
        find "$OUTPUT_DIR" -name "*_gpu*.pkl" 2>/dev/null | head -3
        echo ""
        echo "üí° You can try to consolidate these files manually or re-run extraction"
    else
        echo "No intermediate GPU-specific files found"
    fi
    
    # Show failure markers with memory information
    FAILURE_MARKERS=$(find "$OUTPUT_DIR" -name "failed_shard_*.txt" 2>/dev/null | wc -l)
    if [ $FAILURE_MARKERS -gt 0 ]; then
        echo "Found $FAILURE_MARKERS failure markers:"
        echo "Checking for OOM-related failures..."
        grep -l "OOM events:" "$OUTPUT_DIR"/failed_shard_*.txt 2>/dev/null | wc -l | awk '{print "  OOM-related failures: " $1}'
        echo "Check these files for detailed failure analysis"
    fi
fi

echo ""
echo "üìä Post-Extraction GPU Memory Status:"
nvidia-smi --query-gpu=index,name,memory.total,memory.used,memory.free,utilization.gpu --format=csv,noheader,nounits | \
    awk 'BEGIN{print "GPU | Total Memory | Used Memory | Free Memory | Utilization"} {printf "%-3s | %-11s MB | %-11s MB | %-11s MB | %s%%\n", $1, $2, $3, $4, $5}'

echo ""
echo "üèÅ Job completed at $(date)"
echo "Total job time: $(echo "scale=2; ($(date +%s) - $SECONDS) / 3600" | bc -l) hours"
echo ""
echo "üìö MEMORY-OPTIMIZED MULTI-GPU EXTRACTION SUMMARY:"
echo "This job extracted CLIP and EVA-CLIP embeddings from TAR files"
echo "using multiple GPUs with advanced memory optimization techniques."
echo ""
echo "Benefits of MEMORY-OPTIMIZED multi-GPU extraction:"
echo "‚Ä¢ Adaptive batch sizing prevents OOM crashes"
echo "‚Ä¢ Enhanced memory monitoring and cleanup"
echo "‚Ä¢ Graceful degradation under memory pressure"
echo "‚Ä¢ Progressive batch size reduction when needed"
echo "‚Ä¢ Optimized model loading for memory efficiency"
echo "‚Ä¢ Better error recovery from memory issues"
echo "‚Ä¢ Parallel processing across $WORLD_SIZE GPUs with stability"
echo "‚Ä¢ Scalable to various GPU memory configurations"
echo "‚Ä¢ Robust handling of large datasets"
echo ""
echo "The extracted embeddings are ready for distributed BLIP3-o training"
echo "using FSDP (Fully Sharded Data Parallel)."
echo "=============================================="

exit $EXTRACTION_EXIT_CODE