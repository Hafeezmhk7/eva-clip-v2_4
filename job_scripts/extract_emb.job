#!/bin/bash
#SBATCH --job-name=blip3o_extraction_8gpu
#SBATCH --partition=gpu_a100
#SBATCH --nodes=2
#SBATCH --gpus-per-node=4
#SBATCH --ntasks-per-node=4
#SBATCH --cpus-per-task=8
#SBATCH --mem=300G
#SBATCH --time=4:00:00
#SBATCH --output=./slurm_out/blip3o_extraction_%j.out
#SBATCH --error=./slurm_out/blip3o_extraction_%j.err

echo "üîç SLURM Environment Debug:"
echo "SLURM_PROCID: $SLURM_PROCID"
echo "SLURM_LOCALID: $SLURM_LOCALID"  
echo "SLURM_NTASKS: $SLURM_NTASKS"
echo "SLURM_GPUS_ON_NODE: $SLURM_GPUS_ON_NODE"
echo "SLURM_STEP_NODELIST: $SLURM_STEP_NODELIST"
echo "SLURM_NODEID: $SLURM_NODEID"

cd $SLURM_SUBMIT_DIR

# Setup environment
module purge
module load 2024
module load Miniconda3/24.7.1-0
module load CUDA/12.6.0
source activate eva_clip_env

# FIXED: Setup workspace directories
export SCRATCH_SHARED="/scratch-shared"
export BLIP3O_USER=$(whoami)
export BLIP3O_JOB_ID=${SLURM_JOB_ID}

export BLIP3O_WORKSPACE="/scratch-shared/${BLIP3O_USER}/blip3o_workspace"
export BLIP3O_EMBEDDINGS="${BLIP3O_WORKSPACE}/embeddings"
export BLIP3O_CHECKPOINTS="${BLIP3O_WORKSPACE}/checkpoints"
export BLIP3O_DATASETS="${BLIP3O_WORKSPACE}/datasets"
export BLIP3O_LOGS="${BLIP3O_WORKSPACE}/logs"

export BLIP3O_JOB_TEMP="/scratch-local/${BLIP3O_USER}.${BLIP3O_JOB_ID}/blip3o_job_${BLIP3O_JOB_ID}"
export BLIP3O_CACHE="${BLIP3O_JOB_TEMP}/cache"

# Create directories (only on rank 0)
if [ "${SLURM_PROCID:-0}" -eq 0 ]; then
    mkdir -p "${BLIP3O_WORKSPACE}"/{datasets,embeddings,checkpoints,logs,metadata}
    mkdir -p "${BLIP3O_JOB_TEMP}"/{cache,working,temp_checkpoints}
    mkdir -p ./slurm_out
    
    # Create specific output directory
    mkdir -p "${BLIP3O_EMBEDDINGS}/patch_embeddings_short_256"
fi

# Model cache (redirected to job temp)
export TORCH_HOME="${BLIP3O_CACHE}/torch"
export HF_HOME="${BLIP3O_CACHE}/huggingface"
export TRANSFORMERS_CACHE="${BLIP3O_CACHE}/transformers"
export WANDB_DIR="${BLIP3O_LOGS}/wandb"
export HUGGINGFACE_HUB_CACHE="${BLIP3O_CACHE}/huggingface/hub"
export HF_DATASETS_CACHE="${BLIP3O_CACHE}/datasets"

mkdir -p "${TORCH_HOME}" "${HF_HOME}" "${TRANSFORMERS_CACHE}" "${WANDB_DIR}" "${HUGGINGFACE_HUB_CACHE}" "${HF_DATASETS_CACHE}"

echo "Workspace: $BLIP3O_WORKSPACE"
echo "Job temp: $BLIP3O_JOB_TEMP"
echo "Output directory: ${BLIP3O_EMBEDDINGS}/patch_embeddings_short_256"

# FIXED: Configuration for proper 8-GPU usage
WORLD_SIZE=${SLURM_NTASKS:-8}
MASTER_PORT="12361"
BATCH_SIZE=32  # Conservative for stability
MAX_SHARDS=5   # Test with 5 shards
INCLUDE_CLS=false

# Parse command line arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        --include_cls)
            INCLUDE_CLS=true
            shift
            ;;
        --batch_size)
            BATCH_SIZE="$2"
            shift 2
            ;;
        --max_shards)
            MAX_SHARDS="$2"
            shift 2
            ;;
        --master_port)
            MASTER_PORT="$2"
            shift 2
            ;;
        *)
            echo "Unknown option: $1"
            shift
            ;;
    esac
done

# Determine extraction mode
if [ "$INCLUDE_CLS" = true ]; then
    TARGET_TOKENS=257
    MODE_NAME="CLS+Patches"
    MODE_SUFFIX="cls_patch"
else
    TARGET_TOKENS=256
    MODE_NAME="Patches only"
    MODE_SUFFIX="patch_only"
fi

OUTPUT_DIR="${BLIP3O_EMBEDDINGS}/patch_embeddings_short_256"

# Create output directory (only on rank 0)
if [ "${SLURM_PROCID:-0}" -eq 0 ]; then
    mkdir -p "${OUTPUT_DIR}"
    
    echo "Mode: $MODE_NAME ($TARGET_TOKENS tokens)"
    echo "Output: $OUTPUT_DIR"
    echo "GPUs: $WORLD_SIZE"
    echo "Batch size per GPU: $BATCH_SIZE"
    echo "Max shards: $([ $MAX_SHARDS -eq 0 ] && echo 'ALL' || echo $MAX_SHARDS)"
    echo "Master port: $MASTER_PORT"
    
    # Validate extraction script
    EXTRACTION_SCRIPT="src/modules/extract_embeddings_production.py"
    if [ ! -f "$EXTRACTION_SCRIPT" ]; then
        echo "‚ùå Extraction script not found: $EXTRACTION_SCRIPT"
        exit 1
    fi
    
    # Check for dataset files
    if [ ! -d "$BLIP3O_DATASETS" ]; then
        echo "‚ùå Datasets directory not found: $BLIP3O_DATASETS"
        exit 1
    fi
    
    TAR_COUNT=$(find "$BLIP3O_DATASETS" -name "*.tar" | wc -l)
    echo "Found $TAR_COUNT TAR files in datasets directory"
    
    if [ $TAR_COUNT -eq 0 ]; then
        echo "‚ùå No TAR files found in datasets directory!"
        exit 1
    fi
    
    if [ $MAX_SHARDS -gt 0 ] && [ $TAR_COUNT -lt $MAX_SHARDS ]; then
        MAX_SHARDS=$TAR_COUNT
        echo "Adjusted to use $MAX_SHARDS shards"
    elif [ $MAX_SHARDS -eq 0 ]; then
        MAX_SHARDS=$TAR_COUNT
        echo "Processing ALL $MAX_SHARDS shards"
    fi
    
    # Display GPU information
    echo "GPU Information:"
    nvidia-smi --query-gpu=index,name,memory.total,memory.free --format=csv,noheader
    
    echo "Starting extraction at: $(date)"
    echo "Expected processing time: $(( $TAR_COUNT / ($WORLD_SIZE * 2) )) minutes (conservative estimate)"
fi

# FIXED: Synchronize all processes before starting
if command -v srun >/dev/null 2>&1; then
    srun --ntasks=${WORLD_SIZE} /bin/true
fi
sleep 2

# Export environment variables for srun
export MASTER_PORT
export MAX_SHARDS
export BATCH_SIZE
export INCLUDE_CLS
export TARGET_TOKENS
export OUTPUT_DIR

# FIXED: Use srun with proper process management and termination
srun --ntasks=${WORLD_SIZE} --kill-on-bad-exit=1 bash -c '
# Set up distributed environment variables from SLURM
export RANK=${SLURM_PROCID}
export LOCAL_RANK=${SLURM_LOCALID}
export WORLD_SIZE=${SLURM_NTASKS}

# Get master node address
MASTER_NODE=$(scontrol show hostnames ${SLURM_JOB_NODELIST} | head -n 1)
export MASTER_ADDR=${MASTER_NODE}

echo "[Rank ${RANK}] Node: $(hostname), Local Rank: ${LOCAL_RANK}, GPU Assignment: ${LOCAL_RANK}"
echo "[Rank ${RANK}] Master: ${MASTER_ADDR}:${MASTER_PORT}, World Size: ${WORLD_SIZE}"
echo "[Rank ${RANK}] Processing shards from: ${BLIP3O_DATASETS}"
echo "[Rank ${RANK}] Output to: ${OUTPUT_DIR}"

# FIXED: Run the extraction with proper error handling and termination
python -c "
import os
import sys
import torch
import time
import signal

# Add project root to path
sys.path.insert(0, '\''$(pwd)'\'')

def signal_handler(signum, frame):
    print(f'\''[Rank {os.environ[\"RANK\"]}] Received signal {signum}, cleaning up...'\'')
    sys.exit(0)

signal.signal(signal.SIGTERM, signal_handler)
signal.signal(signal.SIGINT, signal_handler)

try:
    # Import our function
    from src.modules.extract_embeddings_production import process_tar_files_on_gpu, find_data_files, setup_temp_manager
    from pathlib import Path

    # Get parameters from environment
    rank = int(os.environ['\''RANK'\''])
    local_rank = int(os.environ['\''LOCAL_RANK'\''])
    world_size = int(os.environ['\''WORLD_SIZE'\''])
    master_port = os.environ['\''MASTER_PORT'\'']
    batch_size = int(os.environ['\''BATCH_SIZE'\''])
    max_shards = int(os.environ['\''MAX_SHARDS'\''])
    include_cls = os.environ['\''INCLUDE_CLS'\''] == '\''true'\''
    target_tokens = int(os.environ['\''TARGET_TOKENS'\''])
    output_dir = Path(os.environ['\''OUTPUT_DIR'\''])

    print(f'\''[Rank {rank}] Starting extraction with {max_shards} shards, batch_size={batch_size}'\'')

    # CRITICAL: Set CUDA device BEFORE any model loading
    torch.cuda.set_device(local_rank)
    print(f'\''[Rank {rank}] Set CUDA device to {local_rank}'\'')

    # Setup directories
    temp_manager = setup_temp_manager()
    if temp_manager:
        temp_manager.setup_model_cache()

    # Find TAR files
    tar_files = find_data_files(temp_manager, max_shards=max_shards if max_shards > 0 else None)

    print(f'\''[Rank {rank}] Found {len(tar_files)} TAR files, processing on GPU {local_rank}'\'')

    # Run extraction
    process_tar_files_on_gpu(
        rank=rank,
        world_size=world_size, 
        tar_files=tar_files,
        output_dir=output_dir,
        batch_size=batch_size,
        include_cls=include_cls,
        target_tokens=target_tokens,
        master_port=master_port
    )
    
    print(f'\''[Rank {rank}] ‚úÖ Extraction completed successfully'\'')

except Exception as e:
    print(f'\''[Rank {rank}] ‚ùå Extraction failed: {e}'\'')
    import traceback
    traceback.print_exc()
    sys.exit(1)
"

# Capture exit code from extraction
EXTRACTION_EXIT_CODE=$?
echo "[Rank ${RANK}] Extraction process finished with exit code: $EXTRACTION_EXIT_CODE"
exit $EXTRACTION_EXIT_CODE
'

# Capture the overall exit code
OVERALL_EXIT_CODE=$?

echo "All processes completed at: $(date)"

# FIXED: Post-processing and consolidation (only on rank 0)
if [ "${SLURM_PROCID:-0}" -eq 0 ]; then
    echo "üîÑ Starting post-processing and consolidation..."
    
    # Wait a moment for all files to be written
    sleep 5
    
    # FIXED: Run consolidation script
    python -c "
import sys
sys.path.insert(0, '$(pwd)')

try:
    from src.modules.extract_embeddings_production import consolidate_gpu_outputs_production
    from pathlib import Path
    
    output_dir = Path('${OUTPUT_DIR}')
    world_size = ${WORLD_SIZE}
    mode_suffix = '${MODE_SUFFIX}'
    max_shards = ${MAX_SHARDS}
    
    print('üîÑ Consolidating GPU outputs...')
    results = consolidate_gpu_outputs_production(
        output_dir=output_dir,
        world_size=world_size,
        mode_suffix=mode_suffix,
        total_shards=max_shards
    )
    
    print(f'‚úÖ Consolidation completed: {results[\"consolidated_shards\"]} shards')
    
except Exception as e:
    print(f'‚ùå Consolidation failed: {e}')
    import traceback
    traceback.print_exc()
"
    
    CONSOLIDATION_EXIT_CODE=$?
    
    if [ $CONSOLIDATION_EXIT_CODE -eq 0 ] && [ $OVERALL_EXIT_CODE -eq 0 ]; then
        echo "‚úÖ Multi-GPU embedding extraction completed successfully!"
        
        # Check for output files
        FINAL_FILES=$(find "$OUTPUT_DIR" -name "embeddings_shard_*_${MODE_SUFFIX}.pkl" | wc -l)
        if [ $FINAL_FILES -gt 0 ]; then
            echo "‚úÖ Found $FINAL_FILES consolidated embedding files"
            
            TOTAL_SIZE=$(du -sh "$OUTPUT_DIR" 2>/dev/null | cut -f1)
            echo "‚úÖ Total size: $TOTAL_SIZE"
            
            echo "üéâ EXTRACTION SUCCESS!"
            echo "üìÅ Embeddings stored in: $OUTPUT_DIR"
            echo "üìä Ready for training with $FINAL_FILES shards"
            
        else
            echo "‚ö†Ô∏è No consolidated files found"
            echo "Files in output directory:"
            ls -la "$OUTPUT_DIR" 2>/dev/null | head -10
        fi
        
    else
        echo "‚ùå Extraction or consolidation failed"
        echo "Exit codes: Overall=$OVERALL_EXIT_CODE, Consolidation=$CONSOLIDATION_EXIT_CODE"
    fi
    
    echo "Final GPU Memory Status:"
    nvidia-smi --query-gpu=index,name,memory.used,memory.free --format=csv,noheader
    
    echo "Job completed at $(date)"
    TOTAL_SECONDS=$SECONDS
    TOTAL_HOURS=$(echo "scale=2; $TOTAL_SECONDS / 3600" | bc -l)
    echo "Total job time: ${TOTAL_HOURS} hours"
fi

# FIXED: Ensure proper exit
exit $OVERALL_EXIT_CODE